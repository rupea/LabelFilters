
OPENMP DISPLAY ENVIRONMENT BEGIN
   _OPENMP='201307'
  [host] KMP_ABORT_DELAY='0'
  [host] KMP_ABORT_IF_NO_IRML='FALSE'
  [host] KMP_ADAPTIVE_LOCK_PROPS='1,1024'
  [host] KMP_ALIGN_ALLOC='64'
  [host] KMP_ALL_THREADPRIVATE='128'
  [host] KMP_ALL_THREADS='2147483647'
  [host] KMP_ASAT_DEC='1'
  [host] KMP_ASAT_FAVOR='0'
  [host] KMP_ASAT_INC='4'
  [host] KMP_ASAT_INTERVAL='5'
  [host] KMP_ASAT_TRIGGER='5000'
  [host] KMP_ATOMIC_MODE='2'
  [host] KMP_BLOCKTIME='200'
  [host] KMP_CPUINFO_FILE: value is not defined
  [host] KMP_DETERMINISTIC_REDUCTION='FALSE'
  [host] KMP_DUPLICATE_LIB_OK='FALSE'
  [host] KMP_FORCE_REDUCTION: value is not defined
  [host] KMP_FOREIGN_THREADS_THREADPRIVATE='TRUE'
  [host] KMP_FORKJOIN_BARRIER='2,2'
  [host] KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'
  [host] KMP_FORKJOIN_FRAMES='TRUE'
  [host] KMP_FORKJOIN_FRAMES_MODE='3'
  [host] KMP_GTID_MODE='3'
  [host] KMP_HANDLE_SIGNALS='FALSE'
  [host] KMP_HOT_TEAMS_MAX_LEVEL='1'
  [host] KMP_HOT_TEAMS_MODE='0'
  [host] KMP_INIT_AT_FORK='TRUE'
  [host] KMP_INIT_WAIT='2048'
  [host] KMP_ITT_PREPARE_DELAY='0'
  [host] KMP_LIBRARY='throughput'
  [host] KMP_LOCK_KIND='queuing'
  [host] KMP_MALLOC_POOL_INCR='1M'
  [host] KMP_MONITOR_STACKSIZE: value is not defined
  [host] KMP_NEXT_WAIT='1024'
  [host] KMP_NUM_LOCKS_IN_BLOCK='1'
  [host] KMP_PLAIN_BARRIER='2,2'
  [host] KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'
  [host] KMP_REDUCTION_BARRIER='1,1'
  [host] KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'
  [host] KMP_SCHEDULE='static,balanced;guided,iterative'
  [host] KMP_SETTINGS='FALSE'
  [host] KMP_STACKOFFSET='64'
  [host] KMP_STACKPAD='0'
  [host] KMP_STACKSIZE='4M'
  [host] KMP_STORAGE_MAP='FALSE'
  [host] KMP_TASKING='2'
  [host] KMP_TASK_STEALING_CONSTRAINT='1'
  [host] KMP_USE_IRML='FALSE'
  [host] KMP_VERSION='FALSE'
  [host] KMP_WARNINGS='TRUE'
  [host] OMP_CANCELLATION='FALSE'
  [host] OMP_DISPLAY_ENV='VERBOSE'
  [host] OMP_DYNAMIC='FALSE'
  [host] OMP_MAX_ACTIVE_LEVELS='2147483647'
  [host] OMP_NESTED='FALSE'
  [host] OMP_NUM_THREADS='8'
  [host] OMP_PLACES: value is not defined
  [host] OMP_PROC_BIND='false'
  [host] OMP_SCHEDULE='static'
  [host] OMP_STACKSIZE='4M'
  [host] OMP_THREAD_LIMIT='2147483647'
  [host] OMP_WAIT_POLICY='PASSIVE'
  [host] KMP_AFFINITY='noverbose,warnings,respect,granularity=core,duplicates,none'
OPENMP DISPLAY ENVIRONMENT END


|| loaded module 'milde_gui'	
 Function:   Solve for Multi-Class Label Filter projection lines
    Determine a number (--proj) of projection lines. Any example whose projection
    lies outside bounds for that class is 'filtered'.  With many projecting lines,
    potentially many classes can be filtered out, leaving few potential classes
    for each example.
 Usage:
    <mcsolve> --xfile=... --yfile=... [--solnfile=...] [--output=...] [ARGS...]
 where ARGS guide the optimization procedure
 - Without solnfile[.soln], use random initial conditions.
 - xfile is a plain eigen DenseM or SparseM (always stored as float)
 - yfile is an Eigen SparseMb matrix of bool storing only the true values,
         read/written via 'eigen_io_binbool'
Allowed options:

mcsolve options:
  -x [ --xfile ] arg                    x data (row-wise nExamples x dim)
  -y [ --yfile ] arg                    y data (if absent, try reading as 
                                        libsvm format)
  -s [ --solnfile ] arg                 solnfile[.soln] starting solver state
  -o [ --output ] arg (=mc)             output[.soln] file base name
  -B [=arg(=1)] (=1)                    B|T output BINARY
  -T [=arg(=1)] (=0)                    B|T output TEXT
  -S [=arg(=1)] (=1)                    S|L output SHORT
  -L [=arg(=1)] (=0)                    S|L output LONG
  --xnorm [=arg(=1)] (=0)               col-normalize x dimensions 
                                        (mean=stdev=1)
                                        (forces Dense x)
  --xunit [=arg(=1)] (=0)               row-normalize x examples
  --xscale arg (=1)                     scale each x example.  xnorm, xunit, 
                                        xscal applied in order, during read.
  -v [ --verbose ] [=arg(=1)] (=0)      --verbosity=-1 may reduce output

solver args:
  -h [ --help ] 
  --proj arg (=5)                       # of projections
  --C1 arg (=10)                        ~ label in correct [l,u]
  --C2 arg (=1)                         ~ label outside other [l,u]
  --maxiter arg (=1000000)              max iterations per projection
  -b [ --batchsize ] arg (=100)         batch size
  -u [ --update ] arg (=MINIBATCH_SGD)  BATCH | SAFE : gradient update type
  --eps arg (=0.0001)                   unused cvgce threshold
  --eta0 arg (=0.10000000000000001)     initial learning rate
  --etatype arg (=ETA_LIN)              CONST | SQRT | LIN | 3_4 : learning 
                                        rate schedule
  --etamin arg (=0)                     learning rate limit
  --optlu arg (=10000)                  expensive exact {l,u} soln period
  --treorder arg (=1000)                reorder iteration period
  --reorder arg (=REORDER_AVG_PROJ_MEANS)
                                        Permutation re-ordering: AVG projected 
                                        means | PROJ projected means | MID 
                                        range midpoints. If --avg=0, default is
                                        PROJ
  --treport arg (=1000)                 period for reports about latest iter
  --avg arg (=0)                        averaging start iteration
  --tavg arg (=0)                       period for reports about avg, expensive
  --reweight arg (=REWEIGHT_LAMBDA)     NONE | LAMBDA | ALL lambda reweighting 
                                        method
  --wt_by_nclasses [=arg(=1)] (=0)      ?
  --wt_class_by_nclasses [=arg(=1)] (=0)
                                        ?
  --sample arg (=0)                     # -ve classes used for each [chunked] 
                                        gradient estimate, 0 ~ all classes
  --remove_constraints [=arg(=1)] (=0)  after each projection, remove 
                                        constraints
  --remove_class [=arg(=1)] (=0)        after each projection, remove 
                                        already-separated classes(?)
  --threads arg (=0)                    # threads, 0 ~ use OMP_NUM_THREADS
  --seed arg (=0)                       random number seed
  --tgrad arg (=0)                      iter period for finite difference 
                                        gradient test
  --ngrad arg (=1000)                   directions per gradient test
  --grad arg (=0.0001)                  step size per gradient test
  --resume [=arg(=1)] (=0)              resume an existing soln?
  --reoptlu [=arg(=1)] (=0)             reoptimize {l,u} bounds of existing 
                                        soln?

lua constructors:
    - <mcsolve>.new("--xfile=... --yfile=... --solnfile=... etc")
    - <mcsolve>.new(<explicit_defaults:xparm>, "--xfile=... etc")
 Program default settings can come from:
   1. lua <explicit_defaults:xparm>
   2. else parameters from previous --solnfile
   3. else library default settings
	
 parse( argc=15, argv, ... )
    argv[0] = foo
    argv[1] = --xfile=../../data/mnist.svm
    argv[2] = --output=mnist2d.soln
    argv[3] = -B
    argv[4] = -S
    argv[5] = --C1=0.1
    argv[6] = --C2=0.01
    argv[7] = --maxiter=10000000
    argv[8] = --optlu=5000
    argv[9] = --treport=100000
    argv[10] = --proj=30
    argv[11] = --update=SAFE
    argv[12] = -b1
    argv[13] = --threads=8
    argv[14] = --eta0=0.1
msolve args...
opt::extract...
 reparse cmdline, this time with .soln parms as defaults
 parse( argc=15, argv, ... )
    argv[0] = foo
    argv[1] = --xfile=../../data/mnist.svm
    argv[2] = --output=mnist2d.soln
    argv[3] = -B
    argv[4] = -S
    argv[5] = --C1=0.1
    argv[6] = --C2=0.01
    argv[7] = --maxiter=10000000
    argv[8] = --optlu=5000
    argv[9] = --treport=100000
    argv[10] = --proj=30
    argv[11] = --update=SAFE
    argv[12] = -b1
    argv[13] = --threads=8
    argv[14] = --eta0=0.1
msolve args...
opt::extract...
 +MCsolveProgram --xfile=../../data/mnist.svm --yFile= --solnFile=.soln --output=mnist2d.soln -B -S
MCsolveProgram::tryRead()
 GOOD libsvm-like text input, minClass=0 maxClass=9 minXidx=13 maxXidx=784 row=60000
	 y.setFromTriplets...
Assuming 1-based x indices: minXidx = 13, not zero
	 x.setFromTriplets...
MCsolveProgram::trySolve() sparse
 mcsolver.hh, solve_optimization: nProj: 30
size x: 60000 rows and 784 columns.
size y: 60000 rows and 10 columns.
 +MCpermState(nClass=10)
 solve_ with _OPENMP and params.num_threads set to 8, nThreads is 8, and omp_max_threads is now 8
 initial lambda=100 C1=0.1 C2=0.01
  ... begin with weights[0x0] weights_avg[0x0] lower_bounds_avg[0x0] upper_bounds_avg[0x0]
  ... starting with     weights[0x0]:

  ... starting with weights_avg[0x0]:

  ... beginning at projection_dim=0 reuse_dim=0
  ... sc_chunks=2 MCTHREADS=1
 init_w : weights.cols()=30 projection_dim=0 random + vector-between-2-classes orthogonalized
 start projection 0 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 0, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         47072.4 0.0596733
objective_val[200000]:         18172.8 0.0286731
objective_val[300000]:         9884.24 0.0163135
objective_val[400000]:         7199.44 0.010527
objective_val[500000]:         6344.34 0.00805129
objective_val[600000]:         6175.19 0.00731305
objective_val[700000]:         6130.54 0.00688228
objective_val[800000]:         6121.09 0.00663906
objective_val[900000]:         6094.05 0.00649423
objective_val[1000000]:         5998.61 0.00650419
objective_val[1100000]:         5991.16 0.00648396
objective_val[1200000]:         5985.91 0.00642315
objective_val[1300000]:         5982.01 0.00642456
objective_val[1400000]:         5980.21 0.00639942
objective_val[1500000]:         5978.34 0.00639715
objective_val[1600000]:         5977.66 0.0063949
objective_val[1700000]:         5976.93 0.00638943
objective_val[1800000]:          5976.5 0.0063824
objective_val[1900000]:         5975.07 0.00638196
objective_val[2000000]:         5974.02 0.00639885
objective_val[2100000]:         5974.14 0.00638728
objective_val[2200000]:         5974.84 0.0063646
objective_val[2300000]:         5973.91 0.00638127
objective_val[2400000]:         5975.21 0.00638326
objective_val[2500000]:         5973.28 0.00638127
objective_val[2600000]:         5972.99 0.00637545
objective_val[2700000]:         5973.59 0.00636005
objective_val[2800000]:         5974.35 0.00634525
objective_val[2900000]:         5973.75 0.00634243
objective_val[3000000]:         5976.68 0.00631745
objective_val[3100000]:         5974.04 0.00633154
objective_val[3200000]:         5974.22 0.00634125
objective_val[3300000]:         5974.23 0.00632527
objective_val[3400000]:         5973.57 0.00633545
objective_val[3500000]:         5972.96 0.00634524
objective_val[3600000]:          5972.6 0.0063529
objective_val[3700000]:         5973.17 0.00635837
objective_val[3800000]:         5972.58 0.00635709
objective_val[3900000]:         5972.36 0.00635239
objective_val[4000000]:         5972.87 0.0063376
objective_val[4100000]:         5973.05 0.00634297
objective_val[4200000]:         5972.82 0.00633826
objective_val[4300000]:         5972.18 0.00635255
objective_val[4400000]:         5971.89 0.00636078
objective_val[4500000]:         5972.13 0.00635679
objective_val[4600000]:         5971.81 0.00635757
objective_val[4700000]:         5971.88 0.00635909
objective_val[4800000]:         5971.54 0.00637086
objective_val[4900000]:         5971.44 0.00636329
objective_val[5000000]:         5971.48 0.00636295
objective_val[5100000]:         5971.65 0.00635988
objective_val[5200000]:         5971.68 0.00635692
objective_val[5300000]:         5971.94 0.00636356
objective_val[5400000]:         5971.68 0.00635796
objective_val[5500000]:         5971.57 0.00635491
objective_val[5600000]:         5972.24 0.00634327
objective_val[5700000]:         5971.72 0.00635106
objective_val[5800000]:         5971.61 0.0063538
objective_val[5900000]:          5971.7 0.00636263
objective_val[6000000]:         5971.44 0.00636166
objective_val[6100000]:         5971.04 0.00637016
objective_val[6200000]:         5971.66 0.00636038
objective_val[6300000]:         5972.15 0.0063547
objective_val[6400000]:         5971.35 0.00635829
objective_val[6500000]:         5971.53 0.00635725
objective_val[6600000]:         5971.91 0.00635101
objective_val[6700000]:         5971.89 0.00634833
objective_val[6800000]:         5972.28 0.0063483
objective_val[6900000]:         5971.48 0.00635324
objective_val[7000000]:         5971.65 0.00634893
objective_val[7100000]:         5971.87 0.00634376
objective_val[7200000]:         5972.03 0.00633902
objective_val[7300000]:         5971.83 0.00634524
objective_val[7400000]:         5971.94 0.0063455
objective_val[7500000]:         5971.86 0.00634307
objective_val[7600000]:         5971.65 0.00634961
objective_val[7700000]:         5971.66 0.00635015
objective_val[7800000]:         5971.69 0.00635164
objective_val[7900000]:         5971.49 0.0063557
objective_val[8000000]:         5971.84 0.0063518
objective_val[8100000]:          5972.1 0.00634434
objective_val[8200000]:         5971.97 0.00634717
objective_val[8300000]:         5971.74 0.00634788
objective_val[8400000]:          5971.7 0.0063449
objective_val[8500000]:          5971.8 0.00635129
objective_val[8600000]:          5971.7 0.00635058
objective_val[8700000]:          5972.5 0.00635784
objective_val[8800000]:         5971.63 0.00635771
objective_val[8900000]:          5971.5 0.0063579
objective_val[9000000]:          5971.4 0.00635814
objective_val[9100000]:         5971.21 0.00635826
objective_val[9200000]:         5971.16 0.00635673
objective_val[9300000]:         5971.21 0.00636023
objective_val[9400000]:         5971.28 0.00635393
objective_val[9500000]:         5971.23 0.00635515
objective_val[9600000]:         5971.15 0.00636089
objective_val[9700000]:         5971.16 0.00635909
objective_val[9800000]:         5971.27 0.00635669
objective_val[9900000]:         5971.34 0.00635406
objective_val[10000000]:         5971.23 0.00635438
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 5971.23 0.00635438 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=1 random + vector-between-2-classes orthogonalized
 start projection 1 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 1, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         58150.1 0.058027
objective_val[200000]:         23285.2 0.0275184
objective_val[300000]:         12278.6 0.0155433
objective_val[400000]:         8460.17 0.00978487
objective_val[500000]:         6903.84 0.00774079
objective_val[600000]:         6634.75 0.0070507
objective_val[700000]:         6582.97 0.00664354
objective_val[800000]:         6563.54 0.00646252
objective_val[900000]:         6554.92 0.0063444
objective_val[1000000]:         6551.86 0.00625038
objective_val[1100000]:         6547.35 0.00621178
objective_val[1200000]:         6544.12 0.00618663
objective_val[1300000]:         6545.18 0.00615547
objective_val[1400000]:         6544.67 0.00614199
objective_val[1500000]:         6539.61 0.00615221
objective_val[1600000]:         6539.97 0.00613232
objective_val[1700000]:         6539.21 0.00612794
objective_val[1800000]:         6538.76 0.00614238
objective_val[1900000]:          6538.2 0.00615094
objective_val[2000000]:         6537.77 0.0061179
objective_val[2100000]:         6537.86 0.00612446
objective_val[2200000]:         6536.62 0.00613376
objective_val[2300000]:         6536.96 0.0061238
objective_val[2400000]:         6536.02 0.00612332
objective_val[2500000]:         6536.36 0.0061265
objective_val[2600000]:         6536.11 0.00611653
objective_val[2700000]:         6535.22 0.00613089
objective_val[2800000]:         6535.93 0.00611237
objective_val[2900000]:         6536.59 0.00611039
objective_val[3000000]:         6535.41 0.00612473
objective_val[3100000]:         6535.46 0.00611785
objective_val[3200000]:         6534.82 0.00612825
objective_val[3300000]:         6534.74 0.0061222
objective_val[3400000]:         6535.41 0.00611192
objective_val[3500000]:         6534.32 0.00613693
objective_val[3600000]:         6534.68 0.00611616
objective_val[3700000]:         6534.97 0.00611969
objective_val[3800000]:         6535.22 0.0061039
objective_val[3900000]:         6534.66 0.00611802
objective_val[4000000]:         6534.51 0.00611941
objective_val[4100000]:          6535.2 0.00612408
objective_val[4200000]:         6533.84 0.00613267
objective_val[4300000]:         6533.37 0.00614168
objective_val[4400000]:         6533.91 0.00613614
objective_val[4500000]:         6535.42 0.00612733
objective_val[4600000]:         6534.18 0.00614129
objective_val[4700000]:         6533.65 0.00613378
objective_val[4800000]:         6533.91 0.00613784
objective_val[4900000]:          6533.4 0.00614189
objective_val[5000000]:         6533.17 0.00615293
objective_val[5100000]:         6532.66 0.00615979
objective_val[5200000]:         6532.57 0.00615979
objective_val[5300000]:         6533.03 0.0061593
objective_val[5400000]:         6532.67 0.00614934
objective_val[5500000]:         6532.85 0.00614796
objective_val[5600000]:         6533.01 0.00614456
objective_val[5700000]:         6532.86 0.00614269
objective_val[5800000]:            6533 0.00613903
objective_val[5900000]:         6533.25 0.00613618
objective_val[6000000]:         6533.08 0.00613844
objective_val[6100000]:         6532.84 0.00613926
objective_val[6200000]:         6532.92 0.00614629
objective_val[6300000]:         6534.26 0.00614043
objective_val[6400000]:         6532.91 0.0061427
objective_val[6500000]:         6532.86 0.00613723
objective_val[6600000]:         6533.23 0.00613552
objective_val[6700000]:         6533.03 0.00614209
objective_val[6800000]:         6532.97 0.00613913
objective_val[6900000]:         6533.37 0.0061315
objective_val[7000000]:         6533.28 0.00613353
objective_val[7100000]:         6532.85 0.00613909
objective_val[7200000]:         6532.68 0.00614128
objective_val[7300000]:          6532.7 0.00614261
objective_val[7400000]:          6532.7 0.0061429
objective_val[7500000]:         6532.52 0.00614544
objective_val[7600000]:         6532.54 0.00614299
objective_val[7700000]:         6532.44 0.00614591
objective_val[7800000]:         6532.66 0.00614175
objective_val[7900000]:         6532.62 0.00614064
objective_val[8000000]:         6532.44 0.00614427
objective_val[8100000]:         6532.62 0.00614278
objective_val[8200000]:         6533.03 0.00614052
objective_val[8300000]:         6533.15 0.00613715
objective_val[8400000]:         6533.92 0.00613434
objective_val[8500000]:         6533.03 0.00614176
objective_val[8600000]:         6533.27 0.00613911
objective_val[8700000]:         6532.74 0.00613929
objective_val[8800000]:         6532.77 0.00613795
objective_val[8900000]:          6532.5 0.00614103
objective_val[9000000]:         6532.51 0.00614883
objective_val[9100000]:         6532.26 0.00614691
objective_val[9200000]:         6532.34 0.00614793
objective_val[9300000]:         6532.36 0.00615014
objective_val[9400000]:         6532.17 0.00614943
objective_val[9500000]:         6532.25 0.00615
objective_val[9600000]:          6532.3 0.00614628
objective_val[9700000]:         6532.39 0.00614463
objective_val[9800000]:         6532.51 0.00614208
objective_val[9900000]:         6532.63 0.00614195
objective_val[10000000]:         6532.62 0.00614413
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6532.62 0.00614413 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=2 random + vector-between-2-classes orthogonalized
 start projection 2 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 2, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         13546.8 0.0197204
objective_val[200000]:         8429.02 0.011732
objective_val[300000]:         6868.38 0.00847207
objective_val[400000]:         6575.98 0.00761536
objective_val[500000]:         6480.83 0.00717202
objective_val[600000]:         6476.31 0.00704195
objective_val[700000]:         6473.96 0.00697503
objective_val[800000]:         6475.09 0.00696097
objective_val[900000]:          6474.2 0.00692212
objective_val[1000000]:         6472.88 0.00691266
objective_val[1100000]:         6471.79 0.00690916
objective_val[1200000]:          6471.5 0.00689079
objective_val[1300000]:         6473.07 0.00688212
objective_val[1400000]:         6470.96 0.00688643
objective_val[1500000]:         6474.19 0.00689151
objective_val[1600000]:         6469.69 0.00691457
objective_val[1700000]:         6470.86 0.00690872
objective_val[1800000]:            6470 0.00691906
objective_val[1900000]:         6469.47 0.00691451
objective_val[2000000]:         6470.55 0.00692032
objective_val[2100000]:         6468.74 0.00691623
objective_val[2200000]:         6470.27 0.00689171
objective_val[2300000]:         6468.96 0.00690056
objective_val[2400000]:         6469.53 0.00688949
objective_val[2500000]:         6470.48 0.00687809
objective_val[2600000]:         6472.39 0.0068695
objective_val[2700000]:         6469.98 0.00687475
objective_val[2800000]:         6469.07 0.00688874
objective_val[2900000]:         6469.61 0.00688059
objective_val[3000000]:         6470.03 0.00687735
objective_val[3100000]:         6470.41 0.00687009
objective_val[3200000]:         6469.76 0.00688321
objective_val[3300000]:         6468.97 0.00689339
objective_val[3400000]:         6468.13 0.00690722
objective_val[3500000]:         6467.78 0.00691816
objective_val[3600000]:         6467.49 0.00692086
objective_val[3700000]:            6468 0.00692736
objective_val[3800000]:         6467.34 0.00692292
objective_val[3900000]:         6467.07 0.00692852
objective_val[4000000]:         6466.82 0.00692972
objective_val[4100000]:         6466.97 0.0069304
objective_val[4200000]:         6467.82 0.00693633
objective_val[4300000]:         6467.77 0.0069267
objective_val[4400000]:         6466.78 0.00692889
objective_val[4500000]:         6466.65 0.00693868
objective_val[4600000]:         6466.76 0.00693506
objective_val[4700000]:         6467.81 0.00693856
objective_val[4800000]:         6466.85 0.0069377
objective_val[4900000]:         6466.69 0.00692938
objective_val[5000000]:         6466.65 0.00692847
objective_val[5100000]:         6466.61 0.00693747
objective_val[5200000]:         6467.13 0.00693153
objective_val[5300000]:         6468.88 0.00693322
objective_val[5400000]:         6467.75 0.00693861
objective_val[5500000]:         6466.99 0.00693828
objective_val[5600000]:         6467.51 0.00693638
objective_val[5700000]:          6466.3 0.00694172
objective_val[5800000]:         6466.39 0.00693745
objective_val[5900000]:         6466.37 0.00693534
objective_val[6000000]:         6466.51 0.00693501
objective_val[6100000]:         6466.58 0.00694453
objective_val[6200000]:         6466.43 0.00693524
objective_val[6300000]:         6466.32 0.0069372
objective_val[6400000]:          6466.2 0.00693742
objective_val[6500000]:         6466.44 0.00694149
objective_val[6600000]:         6466.47 0.00693156
objective_val[6700000]:         6466.56 0.00692981
objective_val[6800000]:         6466.75 0.0069256
objective_val[6900000]:         6466.95 0.00692259
objective_val[7000000]:         6467.16 0.00693059
objective_val[7100000]:         6466.51 0.00693342
objective_val[7200000]:          6466.3 0.00693304
objective_val[7300000]:         6466.14 0.00693862
objective_val[7400000]:         6466.08 0.00693766
objective_val[7500000]:         6466.18 0.00693891
objective_val[7600000]:          6466.3 0.00693866
objective_val[7700000]:         6466.43 0.00693379
objective_val[7800000]:         6466.03 0.00693812
objective_val[7900000]:         6465.99 0.00693976
objective_val[8000000]:         6466.11 0.00693765
objective_val[8100000]:         6466.38 0.00693098
objective_val[8200000]:         6466.22 0.0069359
objective_val[8300000]:         6466.37 0.00693472
objective_val[8400000]:         6466.11 0.00693782
objective_val[8500000]:         6466.41 0.00693174
objective_val[8600000]:         6466.68 0.00692907
objective_val[8700000]:         6466.54 0.00692726
objective_val[8800000]:         6466.66 0.00692991
objective_val[8900000]:         6466.32 0.00693029
objective_val[9000000]:         6467.75 0.00693306
objective_val[9100000]:         6466.89 0.00693835
objective_val[9200000]:         6466.97 0.00693745
objective_val[9300000]:         6466.14 0.00693737
objective_val[9400000]:         6465.94 0.00693962
objective_val[9500000]:         6465.89 0.00693925
objective_val[9600000]:         6466.09 0.00693819
objective_val[9700000]:         6466.07 0.00693553
objective_val[9800000]:         6466.23 0.00693199
objective_val[9900000]:         6466.26 0.0069304
objective_val[10000000]:         6466.34 0.00692908
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6466.34 0.00692908 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=3 random + vector-between-2-classes orthogonalized
 start projection 3 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 3, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         49741.2 0.0565897
objective_val[200000]:         16587.9 0.0265225
objective_val[300000]:         9033.21 0.016191
objective_val[400000]:         7046.07 0.0117772
objective_val[500000]:         6501.16 0.00971101
objective_val[600000]:         6416.37 0.00912673
objective_val[700000]:         6385.72 0.00873296
objective_val[800000]:         6295.93 0.00852534
objective_val[900000]:         6278.68 0.00840246
objective_val[1000000]:         6275.17 0.00825626
objective_val[1100000]:         6273.99 0.00817529
objective_val[1200000]:         6273.16 0.00810856
objective_val[1300000]:         6271.65 0.0080459
objective_val[1400000]:         6271.71 0.0079947
objective_val[1500000]:         6272.22 0.00795934
objective_val[1600000]:         6271.92 0.00792449
objective_val[1700000]:         6273.04 0.00789612
objective_val[1800000]:         6272.25 0.00787377
objective_val[1900000]:         6273.01 0.00784717
objective_val[2000000]:         6272.74 0.00783877
objective_val[2100000]:         6272.59 0.00782594
objective_val[2200000]:         6272.65 0.00780831
objective_val[2300000]:         6272.67 0.00780061
objective_val[2400000]:         6274.21 0.00778752
objective_val[2500000]:         6274.36 0.00777595
objective_val[2600000]:         6273.35 0.00777024
objective_val[2700000]:         6273.67 0.0077622
objective_val[2800000]:         6274.24 0.00774969
objective_val[2900000]:         6274.03 0.0077389
objective_val[3000000]:         6274.02 0.00773449
objective_val[3100000]:         6275.22 0.00771498
objective_val[3200000]:         6274.37 0.00772427
objective_val[3300000]:         6275.02 0.00772329
objective_val[3400000]:         6273.48 0.007731
objective_val[3500000]:         6273.51 0.00773118
objective_val[3600000]:         6273.45 0.00773088
objective_val[3700000]:          6273.7 0.00772127
objective_val[3800000]:         6273.65 0.00772638
objective_val[3900000]:         6274.02 0.00771813
objective_val[4000000]:          6274.5 0.00771076
objective_val[4100000]:         6274.48 0.00770586
objective_val[4200000]:         6275.42 0.00770179
objective_val[4300000]:         6274.08 0.00770842
objective_val[4400000]:          6274.3 0.00769959
objective_val[4500000]:         6274.49 0.00770097
objective_val[4600000]:         6274.29 0.00770043
objective_val[4700000]:         6274.55 0.00769845
objective_val[4800000]:         6273.84 0.0077076
objective_val[4900000]:         6274.74 0.00770156
objective_val[5000000]:         6273.99 0.00770426
objective_val[5100000]:         6274.49 0.00770181
objective_val[5200000]:          6274.4 0.00770099
objective_val[5300000]:         6274.07 0.00770404
objective_val[5400000]:         6274.32 0.00769792
objective_val[5500000]:         6274.06 0.00769633
objective_val[5600000]:         6274.66 0.0076925
objective_val[5700000]:         6274.55 0.00768647
objective_val[5800000]:         6274.59 0.00769105
objective_val[5900000]:         6274.98 0.00768929
objective_val[6000000]:         6274.85 0.00768692
objective_val[6100000]:         6274.27 0.00768811
objective_val[6200000]:         6273.87 0.00769676
objective_val[6300000]:         6273.96 0.00769194
objective_val[6400000]:         6274.18 0.00769038
objective_val[6500000]:          6274.3 0.00768533
objective_val[6600000]:         6274.35 0.00768618
objective_val[6700000]:         6274.52 0.00768165
objective_val[6800000]:         6274.49 0.00767985
objective_val[6900000]:          6274.3 0.00768155
objective_val[7000000]:         6274.09 0.00768662
objective_val[7100000]:         6274.04 0.00768779
objective_val[7200000]:         6274.53 0.00767887
objective_val[7300000]:         6274.35 0.00767788
objective_val[7400000]:         6274.52 0.0076761
objective_val[7500000]:         6274.75 0.00767311
objective_val[7600000]:         6274.73 0.00767106
objective_val[7700000]:         6275.28 0.00766194
objective_val[7800000]:         6275.54 0.0076572
objective_val[7900000]:         6275.84 0.00765632
objective_val[8000000]:         6275.37 0.00765895
objective_val[8100000]:         6275.49 0.00765777
objective_val[8200000]:         6275.78 0.00765421
objective_val[8300000]:         6275.69 0.00765374
objective_val[8400000]:         6275.44 0.00765486
objective_val[8500000]:         6275.08 0.0076593
objective_val[8600000]:         6275.82 0.00765633
objective_val[8700000]:         6275.84 0.00765561
objective_val[8800000]:         6275.23 0.00765999
objective_val[8900000]:         6275.12 0.0076577
objective_val[9000000]:         6275.29 0.00765288
objective_val[9100000]:         6275.83 0.00765338
objective_val[9200000]:         6275.48 0.00765016
objective_val[9300000]:         6275.41 0.0076515
objective_val[9400000]:          6275.1 0.00765559
objective_val[9500000]:         6275.19 0.00765453
objective_val[9600000]:         6275.23 0.00765542
objective_val[9700000]:         6275.15 0.00765521
objective_val[9800000]:          6275.4 0.00765093
objective_val[9900000]:         6275.62 0.00764622
objective_val[10000000]:         6275.48 0.00764694
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6275.48 0.00764694 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=4 random + vector-between-2-classes orthogonalized
 start projection 4 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 4, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         30634.1 0.0397453
objective_val[200000]:         13579.1 0.0215514
objective_val[300000]:         7909.09 0.0135513
objective_val[400000]:         6566.27 0.0100227
objective_val[500000]:         6049.94 0.00886607
objective_val[600000]:         5968.17 0.00847022
objective_val[700000]:         5957.14 0.00828812
objective_val[800000]:         5954.14 0.00814009
objective_val[900000]:         5949.65 0.00805496
objective_val[1000000]:         5946.72 0.00800524
objective_val[1100000]:          5946.2 0.00796078
objective_val[1200000]:         5944.71 0.0079257
objective_val[1300000]:         5944.15 0.00791928
objective_val[1400000]:         5944.46 0.0078858
objective_val[1500000]:         5942.82 0.00789527
objective_val[1600000]:         5942.87 0.0078833
objective_val[1700000]:         5943.08 0.00786208
objective_val[1800000]:         5942.53 0.00786313
objective_val[1900000]:         5942.72 0.00787677
objective_val[2000000]:         5941.95 0.00785808
objective_val[2100000]:         5941.98 0.0078609
objective_val[2200000]:         5942.96 0.00786107
objective_val[2300000]:         5941.03 0.0078627
objective_val[2400000]:         5941.77 0.00784974
objective_val[2500000]:         5941.44 0.00784705
objective_val[2600000]:         5941.44 0.00784125
objective_val[2700000]:         5941.72 0.0078478
objective_val[2800000]:         5941.83 0.00783548
objective_val[2900000]:         5942.32 0.00782969
objective_val[3000000]:         5940.68 0.0078413
objective_val[3100000]:          5940.3 0.00784587
objective_val[3200000]:         5940.29 0.00783931
objective_val[3300000]:         5940.18 0.00784245
objective_val[3400000]:         5940.21 0.00785028
objective_val[3500000]:         5939.81 0.00785185
objective_val[3600000]:         5940.16 0.00784038
objective_val[3700000]:         5939.75 0.00784465
objective_val[3800000]:         5939.73 0.00784585
objective_val[3900000]:         5940.34 0.00783877
objective_val[4000000]:         5940.25 0.00783619
objective_val[4100000]:         5940.17 0.00783855
objective_val[4200000]:         5940.32 0.00784179
objective_val[4300000]:         5939.98 0.00783423
objective_val[4400000]:         5939.57 0.00784288
objective_val[4500000]:         5939.48 0.00784177
objective_val[4600000]:         5939.37 0.00784315
objective_val[4700000]:         5939.43 0.00783917
objective_val[4800000]:         5939.28 0.00784343
objective_val[4900000]:         5939.36 0.00784283
objective_val[5000000]:         5940.17 0.0078316
objective_val[5100000]:         5939.62 0.00783828
objective_val[5200000]:         5939.22 0.0078388
objective_val[5300000]:         5940.11 0.00783138
objective_val[5400000]:         5939.24 0.00783956
objective_val[5500000]:         5939.09 0.00784192
objective_val[5600000]:         5939.23 0.00783965
objective_val[5700000]:         5939.71 0.00783926
objective_val[5800000]:         5939.66 0.00783665
objective_val[5900000]:         5939.18 0.00783713
objective_val[6000000]:         5938.96 0.00784098
objective_val[6100000]:          5938.9 0.00784264
objective_val[6200000]:         5938.64 0.00784891
objective_val[6300000]:         5938.77 0.00785022
objective_val[6400000]:         5939.15 0.00784409
objective_val[6500000]:         5938.96 0.00784366
objective_val[6600000]:         5938.81 0.00784543
objective_val[6700000]:         5938.68 0.00784564
objective_val[6800000]:         5938.58 0.0078482
objective_val[6900000]:         5938.84 0.00785417
objective_val[7000000]:         5938.57 0.00784758
objective_val[7100000]:          5938.9 0.00784754
objective_val[7200000]:         5938.72 0.00784847
objective_val[7300000]:         5938.47 0.00784809
objective_val[7400000]:         5938.61 0.00784721
objective_val[7500000]:         5938.34 0.00785461
objective_val[7600000]:         5938.24 0.00785331
objective_val[7700000]:         5938.33 0.00785397
objective_val[7800000]:         5938.19 0.00785349
objective_val[7900000]:         5938.36 0.00785098
objective_val[8000000]:         5938.53 0.00785
objective_val[8100000]:         5938.67 0.00784796
objective_val[8200000]:         5938.13 0.00785506
objective_val[8300000]:         5938.71 0.00785387
objective_val[8400000]:         5938.03 0.00785458
objective_val[8500000]:         5937.69 0.00786227
objective_val[8600000]:         5937.68 0.00786215
objective_val[8700000]:            5938 0.00785929
objective_val[8800000]:         5937.81 0.00786205
objective_val[8900000]:         5937.56 0.00786506
objective_val[9000000]:         5937.71 0.00786148
objective_val[9100000]:         5937.77 0.00786191
objective_val[9200000]:         5937.61 0.00786443
objective_val[9300000]:         5937.74 0.00786063
objective_val[9400000]:         5937.77 0.00786185
objective_val[9500000]:         5937.79 0.00786361
objective_val[9600000]:          5937.8 0.0078644
objective_val[9700000]:         5937.72 0.00786289
objective_val[9800000]:         5937.82 0.0078619
objective_val[9900000]:         5937.67 0.00786233
objective_val[10000000]:         5937.81 0.00786053
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 5937.81 0.00786053 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=5 random + vector-between-2-classes orthogonalized
 start projection 5 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 5, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         36109.5 0.0412774
objective_val[200000]:         16070.3 0.0210306
objective_val[300000]:         8949.15 0.0118477
objective_val[400000]:         7196.94 0.00824578
objective_val[500000]:         6648.54 0.00703804
objective_val[600000]:         6586.15 0.00657828
objective_val[700000]:         6575.83 0.00641318
objective_val[800000]:         6569.73 0.0062841
objective_val[900000]:         6569.72 0.00620295
objective_val[1000000]:         6570.27 0.00615188
objective_val[1100000]:         6565.91 0.00612406
objective_val[1200000]:         6564.93 0.00609188
objective_val[1300000]:         6563.86 0.00608801
objective_val[1400000]:         6562.98 0.00608387
objective_val[1500000]:         6562.86 0.00606694
objective_val[1600000]:         6563.92 0.00604326
objective_val[1700000]:         6562.68 0.00603983
objective_val[1800000]:          6562.6 0.00603861
objective_val[1900000]:         6563.09 0.0060251
objective_val[2000000]:          6563.7 0.00604429
objective_val[2100000]:         6562.66 0.00600876
objective_val[2200000]:         6562.21 0.0060208
objective_val[2300000]:         6563.67 0.0060085
objective_val[2400000]:         6561.95 0.00602129
objective_val[2500000]:         6562.28 0.00600261
objective_val[2600000]:         6561.92 0.00601154
objective_val[2700000]:         6561.68 0.00601364
objective_val[2800000]:         6562.62 0.00601376
objective_val[2900000]:         6561.91 0.00599893
objective_val[3000000]:         6562.62 0.0059918
objective_val[3100000]:         6562.15 0.00599295
objective_val[3200000]:         6562.62 0.00598463
objective_val[3300000]:         6562.13 0.00598808
objective_val[3400000]:         6561.82 0.00599395
objective_val[3500000]:         6562.49 0.00599059
objective_val[3600000]:         6561.57 0.00600399
objective_val[3700000]:          6561.8 0.00599531
objective_val[3800000]:         6562.04 0.0059866
objective_val[3900000]:         6561.95 0.00598852
objective_val[4000000]:         6561.83 0.0059881
objective_val[4100000]:         6561.78 0.00598372
objective_val[4200000]:         6562.03 0.00597699
objective_val[4300000]:         6562.21 0.00597479
objective_val[4400000]:         6563.14 0.00596987
objective_val[4500000]:         6562.13 0.0059724
objective_val[4600000]:         6562.16 0.00598235
objective_val[4700000]:         6561.86 0.00597856
objective_val[4800000]:         6562.85 0.00597527
objective_val[4900000]:         6562.22 0.0059746
objective_val[5000000]:         6561.86 0.00597345
objective_val[5100000]:         6562.02 0.00597328
objective_val[5200000]:         6562.04 0.00596895
objective_val[5300000]:         6562.04 0.00597271
objective_val[5400000]:         6561.92 0.00597219
objective_val[5500000]:         6562.23 0.00597297
objective_val[5600000]:         6562.07 0.00597134
objective_val[5700000]:         6562.01 0.00597072
objective_val[5800000]:         6562.33 0.00596249
objective_val[5900000]:         6563.42 0.00595745
objective_val[6000000]:          6562.3 0.00596612
objective_val[6100000]:         6562.45 0.00595835
objective_val[6200000]:         6562.36 0.00596059
objective_val[6300000]:         6562.34 0.00596311
objective_val[6400000]:         6562.13 0.00596439
objective_val[6500000]:         6561.91 0.00596882
objective_val[6600000]:         6562.14 0.00596634
objective_val[6700000]:         6562.12 0.00596461
objective_val[6800000]:         6562.28 0.00596484
objective_val[6900000]:         6562.43 0.00595589
objective_val[7000000]:         6562.19 0.00595816
objective_val[7100000]:         6562.46 0.00595486
objective_val[7200000]:         6562.67 0.00595263
objective_val[7300000]:         6562.65 0.00595992
objective_val[7400000]:         6562.08 0.00596277
objective_val[7500000]:         6562.36 0.00595733
objective_val[7600000]:         6562.36 0.00596259
objective_val[7700000]:         6562.47 0.00595601
objective_val[7800000]:         6562.82 0.00595728
objective_val[7900000]:         6562.33 0.00595508
objective_val[8000000]:         6562.73 0.00595432
objective_val[8100000]:         6562.13 0.00596297
objective_val[8200000]:         6562.43 0.00595831
objective_val[8300000]:         6562.22 0.00595891
objective_val[8400000]:         6562.37 0.00595822
objective_val[8500000]:         6562.21 0.00595592
objective_val[8600000]:         6562.14 0.00596059
objective_val[8700000]:         6562.17 0.0059608
objective_val[8800000]:         6562.13 0.00595978
objective_val[8900000]:         6562.22 0.00595569
objective_val[9000000]:          6562.6 0.00594816
objective_val[9100000]:         6562.53 0.00594887
objective_val[9200000]:         6562.94 0.00594727
objective_val[9300000]:         6562.39 0.00594917
objective_val[9400000]:         6562.37 0.00595269
objective_val[9500000]:         6562.53 0.00595099
objective_val[9600000]:         6562.26 0.00595199
objective_val[9700000]:         6562.37 0.00595693
objective_val[9800000]:         6562.28 0.00595212
objective_val[9900000]:         6562.17 0.00595483
objective_val[10000000]:         6562.68 0.00594881
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6562.68 0.00594881 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=6 random + vector-between-2-classes orthogonalized
 start projection 6 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 6, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         25759.4 0.0355542
objective_val[200000]:         12546.5 0.0179069
objective_val[300000]:         8191.36 0.0114674
objective_val[400000]:         6851.32 0.00862179
objective_val[500000]:         6517.55 0.00735295
objective_val[600000]:         6426.78 0.00705809
objective_val[700000]:         6413.37 0.0069817
objective_val[800000]:         6411.39 0.00684475
objective_val[900000]:         6410.46 0.00682767
objective_val[1000000]:         6406.72 0.00679684
objective_val[1100000]:         6404.94 0.00677116
objective_val[1200000]:         6406.18 0.00672129
objective_val[1300000]:         6405.33 0.0067568
objective_val[1400000]:         6405.65 0.00672438
objective_val[1500000]:         6404.14 0.00670738
objective_val[1600000]:         6403.53 0.00672398
objective_val[1700000]:         6403.88 0.00670554
objective_val[1800000]:         6403.31 0.00670783
objective_val[1900000]:         6403.92 0.00670114
objective_val[2000000]:         6402.35 0.00671909
objective_val[2100000]:         6402.37 0.00670683
objective_val[2200000]:         6402.96 0.00668639
objective_val[2300000]:         6402.97 0.00668416
objective_val[2400000]:         6402.71 0.00668828
objective_val[2500000]:          6402.2 0.0066993
objective_val[2600000]:         6402.39 0.0067003
objective_val[2700000]:         6402.12 0.00669712
objective_val[2800000]:         6401.86 0.0067001
objective_val[2900000]:          6401.7 0.006707
objective_val[3000000]:         6402.39 0.00670595
objective_val[3100000]:          6401.3 0.00670563
objective_val[3200000]:         6401.08 0.00671088
objective_val[3300000]:          6401.8 0.00671132
objective_val[3400000]:         6400.78 0.00671514
objective_val[3500000]:         6401.24 0.00671352
objective_val[3600000]:         6401.08 0.00671013
objective_val[3700000]:         6401.07 0.00671402
objective_val[3800000]:         6401.25 0.00670593
objective_val[3900000]:         6400.92 0.00670615
objective_val[4000000]:         6401.79 0.00670041
objective_val[4100000]:         6401.88 0.00669561
objective_val[4200000]:         6401.41 0.00669051
objective_val[4300000]:         6401.51 0.00668631
objective_val[4400000]:         6401.39 0.00669093
objective_val[4500000]:         6400.96 0.00670063
objective_val[4600000]:         6401.32 0.00669691
objective_val[4700000]:         6400.77 0.00670432
objective_val[4800000]:         6401.03 0.0067032
objective_val[4900000]:         6402.41 0.00670294
objective_val[5000000]:         6401.81 0.00668801
objective_val[5100000]:         6401.59 0.00668838
objective_val[5200000]:          6401.8 0.00668207
objective_val[5300000]:         6401.89 0.00668556
objective_val[5400000]:         6401.05 0.00669431
objective_val[5500000]:         6401.17 0.00669524
objective_val[5600000]:         6402.41 0.00668427
objective_val[5700000]:         6401.17 0.0066936
objective_val[5800000]:         6401.24 0.00669497
objective_val[5900000]:         6401.49 0.00668498
objective_val[6000000]:          6401.6 0.0066855
objective_val[6100000]:         6401.32 0.00668453
objective_val[6200000]:         6401.44 0.00668636
objective_val[6300000]:         6401.07 0.00668894
objective_val[6400000]:         6401.08 0.00668866
objective_val[6500000]:          6400.9 0.00669338
objective_val[6600000]:         6400.88 0.00669248
objective_val[6700000]:         6400.77 0.0066968
objective_val[6800000]:         6400.99 0.00669902
objective_val[6900000]:         6400.92 0.00669425
objective_val[7000000]:         6401.27 0.00668969
objective_val[7100000]:         6400.76 0.00669694
objective_val[7200000]:          6400.7 0.00670065
objective_val[7300000]:         6400.59 0.00669874
objective_val[7400000]:         6400.73 0.00669638
objective_val[7500000]:         6400.48 0.00670379
objective_val[7600000]:         6400.35 0.00670535
objective_val[7700000]:         6400.45 0.00670103
objective_val[7800000]:         6400.53 0.00669802
objective_val[7900000]:         6400.61 0.00669559
objective_val[8000000]:         6401.82 0.00668825
objective_val[8100000]:         6401.28 0.00668264
objective_val[8200000]:          6401.2 0.0066863
objective_val[8300000]:            6401 0.00668997
objective_val[8400000]:         6401.01 0.00668574
objective_val[8500000]:         6400.84 0.00668942
objective_val[8600000]:         6401.05 0.00668684
objective_val[8700000]:         6401.06 0.00668981
objective_val[8800000]:         6401.07 0.00669345
objective_val[8900000]:         6401.11 0.00668996
objective_val[9000000]:         6400.82 0.00669037
objective_val[9100000]:         6400.83 0.00668744
objective_val[9200000]:         6401.31 0.00668434
objective_val[9300000]:         6401.15 0.00668402
objective_val[9400000]:         6401.05 0.00668553
objective_val[9500000]:         6401.09 0.00668572
objective_val[9600000]:         6400.97 0.006686
objective_val[9700000]:         6401.51 0.00668442
objective_val[9800000]:         6401.15 0.00668516
objective_val[9900000]:         6401.06 0.00668183
objective_val[10000000]:         6400.97 0.00668356
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6400.97 0.00668356 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=7 random + vector-between-2-classes orthogonalized
 start projection 7 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 7, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         35883.7 0.0424739
objective_val[200000]:         15640.4 0.0214713
objective_val[300000]:         9655.89 0.0132758
objective_val[400000]:         7516.82 0.00961836
objective_val[500000]:         6442.52 0.00832687
objective_val[600000]:         6196.73 0.00800026
objective_val[700000]:         6154.37 0.00784456
objective_val[800000]:         6142.78 0.00768791
objective_val[900000]:         6137.84 0.00763159
objective_val[1000000]:         6132.83 0.00759357
objective_val[1100000]:         6134.15 0.00751714
objective_val[1200000]:         6132.41 0.00749643
objective_val[1300000]:          6128.6 0.00750013
objective_val[1400000]:         6128.69 0.00745943
objective_val[1500000]:         6127.86 0.00745376
objective_val[1600000]:         6127.52 0.00744305
objective_val[1700000]:          6127.2 0.0074265
objective_val[1800000]:         6126.75 0.00743519
objective_val[1900000]:         6127.07 0.00741678
objective_val[2000000]:         6127.22 0.00740765
objective_val[2100000]:         6129.04 0.00739498
objective_val[2200000]:         6126.74 0.00739192
objective_val[2300000]:         6127.69 0.00738472
objective_val[2400000]:         6128.32 0.00738689
objective_val[2500000]:         6126.94 0.00737849
objective_val[2600000]:         6125.99 0.00738528
objective_val[2700000]:         6125.98 0.00738381
objective_val[2800000]:         6125.62 0.00738672
objective_val[2900000]:         6125.76 0.00738471
objective_val[3000000]:         6126.53 0.00737172
objective_val[3100000]:         6126.07 0.00737007
objective_val[3200000]:         6126.51 0.00736408
objective_val[3300000]:         6127.56 0.00734527
objective_val[3400000]:         6127.44 0.00734406
objective_val[3500000]:         6126.64 0.00735407
objective_val[3600000]:         6125.88 0.0073686
objective_val[3700000]:         6126.22 0.00735174
objective_val[3800000]:         6126.14 0.00735519
objective_val[3900000]:         6126.17 0.00735993
objective_val[4000000]:            6126 0.00735506
objective_val[4100000]:         6127.13 0.00733704
objective_val[4200000]:         6127.08 0.00733558
objective_val[4300000]:         6127.12 0.00733107
objective_val[4400000]:          6126.8 0.00733554
objective_val[4500000]:         6127.38 0.0073344
objective_val[4600000]:         6126.75 0.00733368
objective_val[4700000]:         6126.51 0.00733677
objective_val[4800000]:         6126.93 0.0073261
objective_val[4900000]:         6126.32 0.00734072
objective_val[5000000]:         6126.36 0.00734049
objective_val[5100000]:         6125.56 0.00735238
objective_val[5200000]:          6125.8 0.00734893
objective_val[5300000]:         6125.84 0.00734433
objective_val[5400000]:         6125.86 0.00734525
objective_val[5500000]:         6125.68 0.00734862
objective_val[5600000]:         6125.75 0.00734644
objective_val[5700000]:         6125.83 0.00734423
objective_val[5800000]:          6126.2 0.00734206
objective_val[5900000]:         6126.28 0.00733779
objective_val[6000000]:         6125.88 0.00734337
objective_val[6100000]:         6126.25 0.00733536
objective_val[6200000]:         6126.14 0.00733301
objective_val[6300000]:         6126.33 0.00733481
objective_val[6400000]:         6126.38 0.00733541
objective_val[6500000]:         6126.41 0.00732737
objective_val[6600000]:          6126.5 0.00733249
objective_val[6700000]:         6126.44 0.00732797
objective_val[6800000]:         6126.44 0.00732969
objective_val[6900000]:         6126.41 0.00733009
objective_val[7000000]:         6126.35 0.00732583
objective_val[7100000]:         6126.56 0.00732444
objective_val[7200000]:         6127.02 0.00732659
objective_val[7300000]:          6126.4 0.00732657
objective_val[7400000]:         6126.46 0.00732885
objective_val[7500000]:         6126.53 0.00732644
objective_val[7600000]:         6126.44 0.00732651
objective_val[7700000]:         6126.59 0.00732825
objective_val[7800000]:         6126.25 0.00732868
objective_val[7900000]:         6126.66 0.00732438
objective_val[8000000]:          6126.4 0.00733156
objective_val[8100000]:         6126.05 0.00733718
objective_val[8200000]:          6126.3 0.00732793
objective_val[8300000]:          6126.2 0.00732842
objective_val[8400000]:         6126.05 0.0073309
objective_val[8500000]:         6126.19 0.00732931
objective_val[8600000]:         6126.11 0.00733072
objective_val[8700000]:         6126.21 0.00732771
objective_val[8800000]:         6126.83 0.00732372
objective_val[8900000]:         6126.42 0.00732623
objective_val[9000000]:         6127.03 0.00732233
objective_val[9100000]:         6127.28 0.00732142
objective_val[9200000]:         6126.67 0.00732395
objective_val[9300000]:         6126.35 0.00732479
objective_val[9400000]:         6126.18 0.00732602
objective_val[9500000]:          6126.3 0.00732843
objective_val[9600000]:         6126.23 0.00732451
objective_val[9700000]:         6126.22 0.00732548
objective_val[9800000]:         6126.08 0.00732745
objective_val[9900000]:         6126.24 0.00732548
objective_val[10000000]:         6126.14 0.00732655
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6126.14 0.00732655 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=8 random + vector-between-2-classes orthogonalized
 start projection 8 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 8, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:          132804 0.12575
objective_val[200000]:         42711.1 0.0518288
objective_val[300000]:         18324.5 0.0269432
objective_val[400000]:         10553.2 0.0158566
objective_val[500000]:         7787.32 0.0102194
objective_val[600000]:          7039.2 0.00845404
objective_val[700000]:         6786.73 0.00742424
objective_val[800000]:         6622.41 0.0069329
objective_val[900000]:         6562.17 0.00669356
objective_val[1000000]:         6501.35 0.00653737
objective_val[1100000]:         6484.59 0.00644453
objective_val[1200000]:         6473.45 0.00641138
objective_val[1300000]:         6469.13 0.00637351
objective_val[1400000]:         6462.34 0.00636896
objective_val[1500000]:         6458.97 0.00636209
objective_val[1600000]:         6456.02 0.00635912
objective_val[1700000]:         6454.35 0.00634568
objective_val[1800000]:         6452.45 0.00634452
objective_val[1900000]:         6451.04 0.0063455
objective_val[2000000]:         6450.11 0.00633537
objective_val[2100000]:         6449.52 0.00632986
objective_val[2200000]:         6448.05 0.00634634
objective_val[2300000]:          6447.3 0.00635491
objective_val[2400000]:         6449.26 0.00633772
objective_val[2500000]:         6447.37 0.00633125
objective_val[2600000]:         6445.38 0.00636253
objective_val[2700000]:         6444.58 0.00636112
objective_val[2800000]:         6444.54 0.00634831
objective_val[2900000]:         6444.34 0.00634948
objective_val[3000000]:          6443.8 0.00635437
objective_val[3100000]:         6443.53 0.00635289
objective_val[3200000]:          6443.4 0.00635647
objective_val[3300000]:         6443.88 0.00635919
objective_val[3400000]:         6442.83 0.0063711
objective_val[3500000]:         6442.23 0.00637742
objective_val[3600000]:         6442.49 0.00636499
objective_val[3700000]:         6442.47 0.0063625
objective_val[3800000]:            6442 0.00636591
objective_val[3900000]:         6442.09 0.00636961
objective_val[4000000]:          6441.8 0.00637984
objective_val[4100000]:         6440.95 0.00638331
objective_val[4200000]:         6441.47 0.00637111
objective_val[4300000]:          6441.3 0.00636933
objective_val[4400000]:          6440.8 0.00637658
objective_val[4500000]:         6440.63 0.00638211
objective_val[4600000]:         6440.86 0.00638673
objective_val[4700000]:         6440.73 0.00638487
objective_val[4800000]:         6440.41 0.00638542
objective_val[4900000]:         6440.32 0.00638553
objective_val[5000000]:          6440.6 0.00638481
objective_val[5100000]:         6439.88 0.00639643
objective_val[5200000]:         6440.04 0.00639615
objective_val[5300000]:         6439.81 0.00639275
objective_val[5400000]:         6439.54 0.00639373
objective_val[5500000]:         6439.76 0.00639297
objective_val[5600000]:         6439.46 0.00639726
objective_val[5700000]:         6439.14 0.00640001
objective_val[5800000]:         6439.36 0.00639551
objective_val[5900000]:         6439.14 0.00640185
objective_val[6000000]:         6439.41 0.00639091
objective_val[6100000]:          6439.4 0.00639631
objective_val[6200000]:         6439.66 0.00639228
objective_val[6300000]:         6439.04 0.00639667
objective_val[6400000]:         6439.03 0.0063981
objective_val[6500000]:         6439.62 0.00640231
objective_val[6600000]:         6439.16 0.00639656
objective_val[6700000]:         6438.95 0.00639886
objective_val[6800000]:         6439.15 0.0063956
objective_val[6900000]:         6439.04 0.00639648
objective_val[7000000]:         6439.06 0.00640036
objective_val[7100000]:         6438.77 0.00640278
objective_val[7200000]:         6438.45 0.00640806
objective_val[7300000]:          6438.5 0.00641231
objective_val[7400000]:         6438.41 0.00640763
objective_val[7500000]:         6438.16 0.00641301
objective_val[7600000]:          6438.1 0.00641542
objective_val[7700000]:         6438.07 0.00641478
objective_val[7800000]:         6438.25 0.00641536
objective_val[7900000]:         6438.08 0.00641758
objective_val[8000000]:         6437.84 0.00642011
objective_val[8100000]:         6437.74 0.00642551
objective_val[8200000]:         6437.69 0.0064259
objective_val[8300000]:         6437.67 0.00642429
objective_val[8400000]:         6437.98 0.00641632
objective_val[8500000]:         6438.24 0.00641071
objective_val[8600000]:            6439 0.00640582
objective_val[8700000]:         6438.27 0.00640723
objective_val[8800000]:         6438.42 0.00640915
objective_val[8900000]:         6438.14 0.00641318
objective_val[9000000]:          6438.3 0.0064105
objective_val[9100000]:          6438.1 0.00641489
objective_val[9200000]:         6438.15 0.00641461
objective_val[9300000]:         6438.46 0.00641113
objective_val[9400000]:          6438.4 0.00641131
objective_val[9500000]:         6438.23 0.00641038
objective_val[9600000]:         6437.96 0.00641645
objective_val[9700000]:         6437.72 0.00642036
objective_val[9800000]:         6437.72 0.00641816
objective_val[9900000]:         6438.03 0.00641393
objective_val[10000000]:         6438.61 0.00641293
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6438.61 0.00641293 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=9 random + vector-between-2-classes orthogonalized
 start projection 9 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 9, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         71946.5 0.0715953
objective_val[200000]:         25803.6 0.0340912
objective_val[300000]:         12481.2 0.0193792
objective_val[400000]:          8129.4 0.0130119
objective_val[500000]:         6933.07 0.0100691
objective_val[600000]:         6590.31 0.00904101
objective_val[700000]:         6503.31 0.00845998
objective_val[800000]:         6441.92 0.00807737
objective_val[900000]:         6338.47 0.0078704
objective_val[1000000]:         6221.47 0.00771293
objective_val[1100000]:         6182.94 0.00762448
objective_val[1200000]:         6166.36 0.00755818
objective_val[1300000]:         6156.61 0.00751727
objective_val[1400000]:         6152.49 0.00743742
objective_val[1500000]:         6150.95 0.00739329
objective_val[1600000]:         6144.82 0.00739002
objective_val[1700000]:         6144.88 0.00736963
objective_val[1800000]:         6141.39 0.00735535
objective_val[1900000]:         6138.84 0.00736789
objective_val[2000000]:         6139.08 0.00732542
objective_val[2100000]:         6137.78 0.00732456
objective_val[2200000]:         6138.09 0.00730827
objective_val[2300000]:         6136.91 0.00729879
objective_val[2400000]:          6135.8 0.00730433
objective_val[2500000]:         6135.88 0.0072891
objective_val[2600000]:         6135.11 0.00728889
objective_val[2700000]:          6134.3 0.00730032
objective_val[2800000]:         6134.15 0.00729424
objective_val[2900000]:         6133.25 0.00730303
objective_val[3000000]:         6133.64 0.00728475
objective_val[3100000]:         6133.34 0.00727975
objective_val[3200000]:         6133.62 0.00727034
objective_val[3300000]:         6132.88 0.00727266
objective_val[3400000]:         6132.83 0.00727343
objective_val[3500000]:         6132.52 0.00726878
objective_val[3600000]:         6132.45 0.00727627
objective_val[3700000]:          6131.5 0.00728757
objective_val[3800000]:         6132.07 0.00727077
objective_val[3900000]:         6131.67 0.00728162
objective_val[4000000]:         6132.18 0.00726975
objective_val[4100000]:         6132.16 0.00726977
objective_val[4200000]:         6131.41 0.00727497
objective_val[4300000]:         6130.99 0.00727317
objective_val[4400000]:          6130.7 0.0072834
objective_val[4500000]:         6130.36 0.0072844
objective_val[4600000]:          6130.4 0.007287
objective_val[4700000]:          6130.3 0.00728314
objective_val[4800000]:         6130.21 0.00728312
objective_val[4900000]:         6130.45 0.00727694
objective_val[5000000]:          6130.7 0.00727015
objective_val[5100000]:         6131.17 0.00726207
objective_val[5200000]:         6130.56 0.00726919
objective_val[5300000]:         6130.49 0.00727135
objective_val[5400000]:         6130.16 0.00727159
objective_val[5500000]:         6130.44 0.00727864
objective_val[5600000]:         6131.35 0.00727238
objective_val[5700000]:         6130.03 0.00727572
objective_val[5800000]:         6130.09 0.00727651
objective_val[5900000]:         6130.03 0.00727229
objective_val[6000000]:         6129.73 0.00727556
objective_val[6100000]:         6129.95 0.00726887
objective_val[6200000]:         6130.21 0.00727187
objective_val[6300000]:         6129.63 0.00727472
objective_val[6400000]:         6129.76 0.00727273
objective_val[6500000]:         6129.52 0.00727717
objective_val[6600000]:          6129.9 0.00726942
objective_val[6700000]:         6129.76 0.00727
objective_val[6800000]:         6129.64 0.00726947
objective_val[6900000]:         6129.93 0.00726627
objective_val[7000000]:         6129.72 0.00726783
objective_val[7100000]:         6129.61 0.00727342
objective_val[7200000]:         6129.75 0.00726612
objective_val[7300000]:         6129.71 0.00726646
objective_val[7400000]:         6129.64 0.00726599
objective_val[7500000]:         6129.53 0.0072693
objective_val[7600000]:         6129.54 0.00726963
objective_val[7700000]:         6129.29 0.00727374
objective_val[7800000]:         6129.36 0.00727123
objective_val[7900000]:         6129.15 0.00727502
objective_val[8000000]:         6128.88 0.00728224
objective_val[8100000]:         6128.99 0.00728029
objective_val[8200000]:         6128.98 0.00727875
objective_val[8300000]:          6129.2 0.007272
objective_val[8400000]:         6129.09 0.00727905
objective_val[8500000]:         6128.93 0.00727747
objective_val[8600000]:         6128.87 0.00727652
objective_val[8700000]:         6128.67 0.00728182
objective_val[8800000]:         6128.63 0.00728284
objective_val[8900000]:         6128.58 0.0072865
objective_val[9000000]:         6128.69 0.00728473
objective_val[9100000]:          6128.6 0.00728325
objective_val[9200000]:         6128.72 0.0072812
objective_val[9300000]:         6128.84 0.00727871
objective_val[9400000]:         6128.62 0.00728211
objective_val[9500000]:         6128.56 0.00728426
objective_val[9600000]:          6128.4 0.00728569
objective_val[9700000]:          6128.4 0.00729364
objective_val[9800000]:         6128.31 0.00728858
objective_val[9900000]:         6128.28 0.00728811
objective_val[10000000]:          6128.4 0.00728549
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6128.4 0.00728549 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=10 random + vector-between-2-classes orthogonalized
 start projection 10 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 10, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         51052.1 0.0564463
objective_val[200000]:         19359.1 0.0275167
objective_val[300000]:         10434.5 0.0157221
objective_val[400000]:         7367.81 0.0103418
objective_val[500000]:         6509.86 0.00790878
objective_val[600000]:         6263.62 0.0072304
objective_val[700000]:         6147.53 0.00683322
objective_val[800000]:          6123.6 0.00665772
objective_val[900000]:         6114.72 0.00651111
objective_val[1000000]:         6076.23 0.00645074
objective_val[1100000]:          6062.6 0.00644803
objective_val[1200000]:         6054.36 0.00646806
objective_val[1300000]:         6049.93 0.00644596
objective_val[1400000]:         6045.33 0.0064794
objective_val[1500000]:          6042.9 0.00649326
objective_val[1600000]:         6038.53 0.00651115
objective_val[1700000]:            6036 0.00652112
objective_val[1800000]:         6034.95 0.00653188
objective_val[1900000]:         6034.04 0.00652013
objective_val[2000000]:         6031.88 0.00655115
objective_val[2100000]:         6032.52 0.00657073
objective_val[2200000]:         6029.13 0.00656445
objective_val[2300000]:         6028.46 0.0065915
objective_val[2400000]:         6028.39 0.00656293
objective_val[2500000]:         6027.14 0.00657232
objective_val[2600000]:         6026.96 0.00658738
objective_val[2700000]:          6024.7 0.00661131
objective_val[2800000]:         6024.41 0.0066151
objective_val[2900000]:         6023.31 0.00662648
objective_val[3000000]:         6023.31 0.00663189
objective_val[3100000]:         6024.86 0.00662286
objective_val[3200000]:          6023.1 0.00664459
objective_val[3300000]:         6021.73 0.00665646
objective_val[3400000]:         6021.73 0.00665655
objective_val[3500000]:         6020.68 0.00666897
objective_val[3600000]:            6020 0.00668057
objective_val[3700000]:         6019.87 0.00668311
objective_val[3800000]:         6019.52 0.00668245
objective_val[3900000]:         6019.38 0.00668489
objective_val[4000000]:         6019.14 0.00668776
objective_val[4100000]:            6019 0.00668373
objective_val[4200000]:            6019 0.00668847
objective_val[4300000]:         6018.87 0.00668779
objective_val[4400000]:         6018.78 0.00670506
objective_val[4500000]:         6018.22 0.00670741
objective_val[4600000]:         6017.25 0.00672172
objective_val[4700000]:         6016.64 0.00673411
objective_val[4800000]:         6016.79 0.00673522
objective_val[4900000]:         6016.17 0.00673621
objective_val[5000000]:         6016.58 0.00672634
objective_val[5100000]:         6016.62 0.00672954
objective_val[5200000]:         6016.63 0.00672605
objective_val[5300000]:         6016.21 0.00673185
objective_val[5400000]:         6016.63 0.00673999
objective_val[5500000]:         6015.67 0.00673896
objective_val[5600000]:         6015.34 0.00674867
objective_val[5700000]:         6015.03 0.00675383
objective_val[5800000]:            6015 0.00675456
objective_val[5900000]:         6015.34 0.00674501
objective_val[6000000]:         6015.27 0.00674842
objective_val[6100000]:         6015.28 0.0067502
objective_val[6200000]:         6015.18 0.00675054
objective_val[6300000]:         6014.83 0.00675456
objective_val[6400000]:         6014.64 0.00676091
objective_val[6500000]:         6014.68 0.00676443
objective_val[6600000]:         6014.58 0.00675767
objective_val[6700000]:         6014.63 0.00675772
objective_val[6800000]:         6014.21 0.00676547
objective_val[6900000]:          6014.2 0.00676634
objective_val[7000000]:         6013.98 0.00677168
objective_val[7100000]:         6013.75 0.00677552
objective_val[7200000]:         6013.87 0.00677944
objective_val[7300000]:         6013.64 0.00678142
objective_val[7400000]:         6014.04 0.00678545
objective_val[7500000]:         6013.43 0.00678181
objective_val[7600000]:         6013.67 0.00678526
objective_val[7700000]:         6013.67 0.00677897
objective_val[7800000]:          6013.4 0.00678576
objective_val[7900000]:         6013.58 0.00679016
objective_val[8000000]:         6013.14 0.00679073
objective_val[8100000]:         6013.01 0.00678811
objective_val[8200000]:         6013.27 0.0067982
objective_val[8300000]:         6012.72 0.00679455
objective_val[8400000]:         6013.06 0.00679011
objective_val[8500000]:         6013.66 0.00678565
objective_val[8600000]:         6013.12 0.00678402
objective_val[8700000]:         6013.18 0.0067841
objective_val[8800000]:         6012.96 0.00678898
objective_val[8900000]:         6013.01 0.00678585
objective_val[9000000]:         6013.09 0.00678714
objective_val[9100000]:         6012.85 0.0067942
objective_val[9200000]:         6012.66 0.00679435
objective_val[9300000]:         6012.49 0.00679764
objective_val[9400000]:         6012.28 0.00680076
objective_val[9500000]:          6012.2 0.0068029
objective_val[9600000]:         6012.09 0.00680377
objective_val[9700000]:         6012.27 0.00679982
objective_val[9800000]:         6012.28 0.00680357
objective_val[9900000]:         6012.25 0.0068034
objective_val[10000000]:         6012.21 0.0068045
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6012.21 0.0068045 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=11 random + vector-between-2-classes orthogonalized
 start projection 11 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 11, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         90417.7 0.0916124
objective_val[200000]:         31389.6 0.0395328
objective_val[300000]:         14432.4 0.0209704
objective_val[400000]:         8953.83 0.0123278
objective_val[500000]:         7149.05 0.00829869
objective_val[600000]:         6781.04 0.00724218
objective_val[700000]:         6612.38 0.00678475
objective_val[800000]:         6551.73 0.00653056
objective_val[900000]:         6496.69 0.00640905
objective_val[1000000]:         6477.13 0.00639694
objective_val[1100000]:         6468.16 0.00638608
objective_val[1200000]:         6461.92 0.00639868
objective_val[1300000]:         6458.18 0.00639113
objective_val[1400000]:         6454.49 0.00638915
objective_val[1500000]:         6452.39 0.0063765
objective_val[1600000]:         6451.25 0.00636739
objective_val[1700000]:         6450.89 0.00636441
objective_val[1800000]:         6448.77 0.00637364
objective_val[1900000]:         6446.92 0.00637263
objective_val[2000000]:         6446.64 0.00636041
objective_val[2100000]:         6446.58 0.00637477
objective_val[2200000]:         6445.76 0.006356
objective_val[2300000]:         6447.73 0.00633271
objective_val[2400000]:         6445.22 0.0063376
objective_val[2500000]:         6444.34 0.00635751
objective_val[2600000]:         6443.38 0.00636428
objective_val[2700000]:         6442.64 0.00638296
objective_val[2800000]:         6443.93 0.0064041
objective_val[2900000]:         6443.31 0.0064024
objective_val[3000000]:         6441.91 0.00639381
objective_val[3100000]:         6441.85 0.00637884
objective_val[3200000]:         6441.42 0.0063867
objective_val[3300000]:         6441.44 0.00638306
objective_val[3400000]:          6441.9 0.00638944
objective_val[3500000]:         6440.63 0.00639135
objective_val[3600000]:         6440.88 0.00639379
objective_val[3700000]:         6440.84 0.00638918
objective_val[3800000]:         6440.88 0.00638264
objective_val[3900000]:         6440.38 0.00638965
objective_val[4000000]:         6440.03 0.00640054
objective_val[4100000]:         6440.05 0.00639584
objective_val[4200000]:         6439.73 0.006401
objective_val[4300000]:          6440.1 0.00640399
objective_val[4400000]:         6440.17 0.00640638
objective_val[4500000]:         6440.04 0.00639664
objective_val[4600000]:         6439.71 0.00640193
objective_val[4700000]:         6439.64 0.00639785
objective_val[4800000]:         6439.32 0.00640261
objective_val[4900000]:         6439.08 0.00640567
objective_val[5000000]:         6439.44 0.0063961
objective_val[5100000]:         6439.39 0.00639782
objective_val[5200000]:         6439.04 0.00640811
objective_val[5300000]:         6439.44 0.00640894
objective_val[5400000]:         6438.44 0.00642116
objective_val[5500000]:         6438.29 0.00642032
objective_val[5600000]:         6438.34 0.00641741
objective_val[5700000]:          6438.3 0.00641908
objective_val[5800000]:         6438.78 0.00641495
objective_val[5900000]:         6438.43 0.00641377
objective_val[6000000]:         6439.32 0.00640336
objective_val[6100000]:         6440.05 0.00640184
objective_val[6200000]:         6439.15 0.00641119
objective_val[6300000]:         6438.66 0.00641263
objective_val[6400000]:         6438.17 0.00641782
objective_val[6500000]:         6437.99 0.00642374
objective_val[6600000]:         6437.77 0.00642487
objective_val[6700000]:         6437.75 0.00642897
objective_val[6800000]:         6438.24 0.00642545
objective_val[6900000]:         6437.83 0.00642185
objective_val[7000000]:         6437.74 0.00642339
objective_val[7100000]:         6437.82 0.00642456
objective_val[7200000]:         6438.01 0.00642169
objective_val[7300000]:         6438.04 0.0064277
objective_val[7400000]:         6437.75 0.00642802
objective_val[7500000]:         6437.72 0.00642836
objective_val[7600000]:          6437.5 0.00642991
objective_val[7700000]:         6437.26 0.00643276
objective_val[7800000]:         6437.67 0.00642969
objective_val[7900000]:         6437.72 0.00643348
objective_val[8000000]:         6437.09 0.0064389
objective_val[8100000]:         6436.93 0.00644105
objective_val[8200000]:         6437.73 0.00644287
objective_val[8300000]:         6437.54 0.00644009
objective_val[8400000]:         6437.06 0.00643636
objective_val[8500000]:         6437.09 0.00643578
objective_val[8600000]:         6437.26 0.00643279
objective_val[8700000]:         6437.35 0.00643199
objective_val[8800000]:         6437.14 0.00643557
objective_val[8900000]:         6437.38 0.0064294
objective_val[9000000]:         6437.39 0.00642822
objective_val[9100000]:         6437.42 0.00642582
objective_val[9200000]:         6437.28 0.00642918
objective_val[9300000]:         6437.37 0.00642675
objective_val[9400000]:         6437.25 0.00642796
objective_val[9500000]:         6437.23 0.00642911
objective_val[9600000]:         6437.23 0.0064319
objective_val[9700000]:         6437.37 0.00642697
objective_val[9800000]:         6437.43 0.00642491
objective_val[9900000]:          6437.6 0.00642173
objective_val[10000000]:         6437.63 0.00641985
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6437.63 0.00641985 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=12 random + vector-between-2-classes orthogonalized
 start projection 12 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 12, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:           60948 0.0702265
objective_val[200000]:         21879.1 0.0325874
objective_val[300000]:         10650.7 0.0180183
objective_val[400000]:         7244.74 0.0114638
objective_val[500000]:         6365.29 0.00856946
objective_val[600000]:         6153.55 0.00784516
objective_val[700000]:         6094.23 0.00745499
objective_val[800000]:         6083.84 0.00726164
objective_val[900000]:          6074.3 0.00711445
objective_val[1000000]:         6069.69 0.00704792
objective_val[1100000]:         6069.06 0.00698365
objective_val[1200000]:         6066.92 0.00692285
objective_val[1300000]:         6065.42 0.00690447
objective_val[1400000]:         6063.34 0.00691229
objective_val[1500000]:         6061.68 0.00689507
objective_val[1600000]:         6061.35 0.00688346
objective_val[1700000]:         6061.51 0.00687612
objective_val[1800000]:         6060.64 0.00686907
objective_val[1900000]:         6060.98 0.00685975
objective_val[2000000]:         6059.94 0.00686348
objective_val[2100000]:         6059.36 0.0068518
objective_val[2200000]:         6059.78 0.00684728
objective_val[2300000]:         6059.61 0.00682923
objective_val[2400000]:         6060.21 0.00681423
objective_val[2500000]:         6060.39 0.00680067
objective_val[2600000]:         6061.57 0.00678949
objective_val[2700000]:         6060.51 0.00679798
objective_val[2800000]:         6059.83 0.00680541
objective_val[2900000]:         6059.88 0.00679659
objective_val[3000000]:         6059.23 0.00680739
objective_val[3100000]:         6060.05 0.00678926
objective_val[3200000]:         6059.46 0.00680071
objective_val[3300000]:         6058.84 0.00680765
objective_val[3400000]:         6059.18 0.00679296
objective_val[3500000]:          6059.7 0.00678864
objective_val[3600000]:         6059.37 0.00678407
objective_val[3700000]:         6060.13 0.00677175
objective_val[3800000]:          6059.6 0.00678273
objective_val[3900000]:         6058.65 0.00679529
objective_val[4000000]:         6058.51 0.00679083
objective_val[4100000]:         6058.63 0.00679263
objective_val[4200000]:         6058.54 0.00679161
objective_val[4300000]:         6058.26 0.00679666
objective_val[4400000]:         6058.08 0.00679982
objective_val[4500000]:         6057.83 0.00680419
objective_val[4600000]:         6058.95 0.00681136
objective_val[4700000]:          6058.2 0.00681542
objective_val[4800000]:         6057.29 0.00682283
objective_val[4900000]:         6057.27 0.0068212
objective_val[5000000]:         6057.66 0.00680437
objective_val[5100000]:         6057.93 0.00680646
objective_val[5200000]:         6058.44 0.00680387
objective_val[5300000]:         6057.95 0.00680309
objective_val[5400000]:         6058.65 0.00680238
objective_val[5500000]:         6058.16 0.00679227
objective_val[5600000]:          6058.2 0.00679239
objective_val[5700000]:         6058.39 0.00678731
objective_val[5800000]:         6058.36 0.00678379
objective_val[5900000]:         6058.14 0.00678519
objective_val[6000000]:         6058.15 0.00678658
objective_val[6100000]:         6058.19 0.00678308
objective_val[6200000]:         6058.82 0.00678325
objective_val[6300000]:         6058.09 0.0067886
objective_val[6400000]:         6058.06 0.00679437
objective_val[6500000]:         6057.72 0.00679747
objective_val[6600000]:         6057.59 0.00679996
objective_val[6700000]:         6057.47 0.00680035
objective_val[6800000]:         6057.54 0.00680008
objective_val[6900000]:         6057.38 0.0068032
objective_val[7000000]:         6057.68 0.00679963
objective_val[7100000]:         6057.82 0.00679831
objective_val[7200000]:         6057.43 0.00679659
objective_val[7300000]:         6057.78 0.00679124
objective_val[7400000]:         6058.01 0.00678708
objective_val[7500000]:         6058.18 0.00677991
objective_val[7600000]:         6057.98 0.00678512
objective_val[7700000]:         6057.73 0.00678997
objective_val[7800000]:         6058.08 0.00678609
objective_val[7900000]:         6057.51 0.00679286
objective_val[8000000]:         6057.49 0.00679733
objective_val[8100000]:         6057.69 0.00678925
objective_val[8200000]:         6058.05 0.00679288
objective_val[8300000]:         6058.11 0.00679102
objective_val[8400000]:         6058.22 0.00679484
objective_val[8500000]:         6058.14 0.00679503
objective_val[8600000]:         6057.98 0.00679757
objective_val[8700000]:         6057.65 0.00679409
objective_val[8800000]:         6057.76 0.00678761
objective_val[8900000]:         6057.95 0.00678509
objective_val[9000000]:         6057.73 0.00678771
objective_val[9100000]:         6057.76 0.00678739
objective_val[9200000]:         6057.54 0.00679137
objective_val[9300000]:         6057.53 0.00679328
objective_val[9400000]:         6057.56 0.00679239
objective_val[9500000]:         6057.63 0.00678833
objective_val[9600000]:          6057.7 0.00678986
objective_val[9700000]:         6057.61 0.00678818
objective_val[9800000]:         6057.73 0.00678697
objective_val[9900000]:         6057.68 0.00678947
objective_val[10000000]:         6057.58 0.00678832
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6057.58 0.00678832 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=13 random + vector-between-2-classes orthogonalized
 start projection 13 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 13, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:           22867 0.031645
objective_val[200000]:         10346.3 0.0159139
objective_val[300000]:         7572.24 0.0105582
objective_val[400000]:         6564.11 0.00849554
objective_val[500000]:         6207.59 0.00749385
objective_val[600000]:         6066.71 0.0072561
objective_val[700000]:         6053.86 0.00708775
objective_val[800000]:         6052.44 0.00692987
objective_val[900000]:          6048.1 0.00688872
objective_val[1000000]:         6046.86 0.00681428
objective_val[1100000]:         6048.52 0.00671518
objective_val[1200000]:         6046.39 0.00671214
objective_val[1300000]:         6047.74 0.00667331
objective_val[1400000]:         6047.83 0.00662798
objective_val[1500000]:         6049.49 0.006626
objective_val[1600000]:         6046.01 0.00662362
objective_val[1700000]:         6045.24 0.00663909
objective_val[1800000]:         6046.17 0.00661527
objective_val[1900000]:         6045.64 0.00658975
objective_val[2000000]:          6045.9 0.00657508
objective_val[2100000]:         6046.32 0.00656468
objective_val[2200000]:         6046.94 0.00655141
objective_val[2300000]:         6046.35 0.00655495
objective_val[2400000]:         6046.28 0.00655028
objective_val[2500000]:          6046.3 0.00654147
objective_val[2600000]:          6046.1 0.0065423
objective_val[2700000]:         6045.73 0.00654878
objective_val[2800000]:         6045.84 0.00653923
objective_val[2900000]:         6046.26 0.00652822
objective_val[3000000]:         6046.18 0.00652913
objective_val[3100000]:         6046.48 0.00652937
objective_val[3200000]:         6046.04 0.00653083
objective_val[3300000]:         6045.74 0.00652629
objective_val[3400000]:         6045.83 0.00651992
objective_val[3500000]:         6045.68 0.0065254
objective_val[3600000]:         6045.67 0.00652991
objective_val[3700000]:         6046.57 0.00652387
objective_val[3800000]:         6046.43 0.00651574
objective_val[3900000]:          6045.6 0.00651899
objective_val[4000000]:          6045.6 0.0065185
objective_val[4100000]:         6045.98 0.00650611
objective_val[4200000]:         6045.97 0.00650698
objective_val[4300000]:         6045.96 0.00650395
objective_val[4400000]:         6046.07 0.00650697
objective_val[4500000]:         6046.29 0.00650555
objective_val[4600000]:         6045.87 0.00650394
objective_val[4700000]:         6045.88 0.00650235
objective_val[4800000]:         6046.32 0.0064994
objective_val[4900000]:         6046.27 0.00649027
objective_val[5000000]:         6046.45 0.00649012
objective_val[5100000]:         6046.88 0.00648123
objective_val[5200000]:         6047.02 0.00648842
objective_val[5300000]:         6046.41 0.00648448
objective_val[5400000]:         6046.95 0.00648126
objective_val[5500000]:         6046.63 0.00647985
objective_val[5600000]:         6046.68 0.00647566
objective_val[5700000]:         6046.89 0.00648165
objective_val[5800000]:         6046.85 0.00647267
objective_val[5900000]:         6047.49 0.00647258
objective_val[6000000]:         6047.06 0.00646785
objective_val[6100000]:         6046.95 0.00646922
objective_val[6200000]:         6046.94 0.0064724
objective_val[6300000]:         6047.28 0.00647556
objective_val[6400000]:         6047.06 0.00647563
objective_val[6500000]:         6046.63 0.00647494
objective_val[6600000]:          6046.6 0.0064744
objective_val[6700000]:         6046.62 0.0064793
objective_val[6800000]:         6046.57 0.00647917
objective_val[6900000]:         6046.52 0.00647555
objective_val[7000000]:         6046.28 0.00648118
objective_val[7100000]:         6046.39 0.00648267
objective_val[7200000]:         6046.63 0.00647766
objective_val[7300000]:         6046.49 0.00648261
objective_val[7400000]:         6046.33 0.00648153
objective_val[7500000]:         6046.52 0.006475
objective_val[7600000]:         6046.49 0.00647587
objective_val[7700000]:         6046.45 0.00647522
objective_val[7800000]:         6046.56 0.00648295
objective_val[7900000]:         6046.07 0.0064854
objective_val[8000000]:         6045.83 0.0064923
objective_val[8100000]:         6045.95 0.0064941
objective_val[8200000]:         6045.99 0.00648948
objective_val[8300000]:         6045.97 0.00648777
objective_val[8400000]:         6045.97 0.00648627
objective_val[8500000]:         6046.64 0.0064887
objective_val[8600000]:         6046.14 0.00648891
objective_val[8700000]:         6046.17 0.00648272
objective_val[8800000]:         6046.26 0.00647736
objective_val[8900000]:         6046.33 0.00647758
objective_val[9000000]:         6046.31 0.0064764
objective_val[9100000]:         6046.83 0.00647162
objective_val[9200000]:         6046.65 0.00646768
objective_val[9300000]:         6046.46 0.00647371
objective_val[9400000]:         6046.33 0.00647616
objective_val[9500000]:         6046.19 0.00648002
objective_val[9600000]:         6046.41 0.00647837
objective_val[9700000]:         6046.13 0.0064807
objective_val[9800000]:         6046.07 0.00648245
objective_val[9900000]:         6046.28 0.00647738
objective_val[10000000]:         6046.17 0.00647877
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6046.17 0.00647877 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=14 random + vector-between-2-classes orthogonalized
 start projection 14 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 14, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         7152.05 0.00840913
objective_val[200000]:          6643.6 0.00721939
objective_val[300000]:         6450.19 0.00710058
objective_val[400000]:         6419.51 0.00696293
objective_val[500000]:         6413.19 0.00700578
objective_val[600000]:         6412.53 0.00695963
objective_val[700000]:          6411.6 0.00694817
objective_val[800000]:         6409.24 0.00698115
objective_val[900000]:         6409.24 0.00697173
objective_val[1000000]:         6407.41 0.00697432
objective_val[1100000]:          6408.4 0.00695888
objective_val[1200000]:         6412.94 0.00696116
objective_val[1300000]:         6406.84 0.00693742
objective_val[1400000]:         6407.97 0.00695117
objective_val[1500000]:         6406.22 0.00695882
objective_val[1600000]:         6406.14 0.00695822
objective_val[1700000]:         6405.09 0.00697468
objective_val[1800000]:         6405.64 0.00696141
objective_val[1900000]:          6404.8 0.00697555
objective_val[2000000]:         6405.42 0.00697164
objective_val[2100000]:         6405.58 0.00695944
objective_val[2200000]:         6405.65 0.00696357
objective_val[2300000]:         6405.57 0.0069607
objective_val[2400000]:         6405.34 0.00695712
objective_val[2500000]:         6404.98 0.00696589
objective_val[2600000]:         6405.55 0.00696235
objective_val[2700000]:         6404.52 0.00697112
objective_val[2800000]:         6404.61 0.00696652
objective_val[2900000]:         6404.79 0.00696728
objective_val[3000000]:         6405.44 0.00695483
objective_val[3100000]:          6405.3 0.00696277
objective_val[3200000]:         6404.52 0.00697141
objective_val[3300000]:         6404.24 0.00697827
objective_val[3400000]:         6404.43 0.00698043
objective_val[3500000]:         6404.24 0.00698108
objective_val[3600000]:         6404.55 0.00696489
objective_val[3700000]:          6403.8 0.00697958
objective_val[3800000]:         6403.82 0.0069832
objective_val[3900000]:         6403.67 0.00698338
objective_val[4000000]:         6404.32 0.00697292
objective_val[4100000]:         6404.04 0.0069762
objective_val[4200000]:          6403.5 0.00698484
objective_val[4300000]:          6403.6 0.00698902
objective_val[4400000]:         6404.46 0.00698132
objective_val[4500000]:         6404.44 0.0069693
objective_val[4600000]:         6404.56 0.00697214
objective_val[4700000]:         6404.09 0.00697208
objective_val[4800000]:          6404.3 0.00697725
objective_val[4900000]:         6404.11 0.00697293
objective_val[5000000]:         6403.82 0.0069793
objective_val[5100000]:         6404.07 0.00698277
objective_val[5200000]:         6403.75 0.00698564
objective_val[5300000]:         6403.99 0.00697861
objective_val[5400000]:         6403.48 0.00698218
objective_val[5500000]:         6404.11 0.00697551
objective_val[5600000]:         6404.32 0.00697752
objective_val[5700000]:         6404.24 0.00697413
objective_val[5800000]:         6404.52 0.00697766
objective_val[5900000]:         6403.84 0.00698073
objective_val[6000000]:         6403.75 0.00698422
objective_val[6100000]:         6403.85 0.00698299
objective_val[6200000]:         6404.63 0.00697813
objective_val[6300000]:         6403.79 0.00697482
objective_val[6400000]:         6403.85 0.00697511
objective_val[6500000]:         6403.81 0.00698542
objective_val[6600000]:         6403.72 0.00698611
objective_val[6700000]:         6403.32 0.00698944
objective_val[6800000]:         6403.32 0.0069902
objective_val[6900000]:         6403.31 0.00698858
objective_val[7000000]:         6403.31 0.00698787
objective_val[7100000]:         6403.35 0.00699611
objective_val[7200000]:         6403.11 0.0069918
objective_val[7300000]:            6403 0.00699549
objective_val[7400000]:         6403.04 0.00699468
objective_val[7500000]:         6403.04 0.00699574
objective_val[7600000]:         6403.12 0.00698919
objective_val[7700000]:          6403.1 0.00699425
objective_val[7800000]:         6403.14 0.00699038
objective_val[7900000]:         6403.04 0.00699395
objective_val[8000000]:         6403.23 0.0069856
objective_val[8100000]:         6403.47 0.00698305
objective_val[8200000]:         6403.54 0.00698123
objective_val[8300000]:         6403.49 0.00697978
objective_val[8400000]:         6403.47 0.00698279
objective_val[8500000]:         6403.31 0.00698335
objective_val[8600000]:         6403.03 0.00698879
objective_val[8700000]:         6404.02 0.00698791
objective_val[8800000]:         6404.64 0.00698991
objective_val[8900000]:         6403.11 0.00699022
objective_val[9000000]:         6403.03 0.00698936
objective_val[9100000]:         6403.87 0.00698726
objective_val[9200000]:         6403.68 0.00698673
objective_val[9300000]:         6403.35 0.00698285
objective_val[9400000]:          6403.3 0.00698648
objective_val[9500000]:         6403.52 0.00698487
objective_val[9600000]:         6403.12 0.00699172
objective_val[9700000]:         6402.89 0.00699379
objective_val[9800000]:         6402.96 0.00699053
objective_val[9900000]:         6403.01 0.00699027
objective_val[10000000]:         6402.98 0.00698997
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6402.98 0.00698997 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=15 random + vector-between-2-classes orthogonalized
 start projection 15 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 15, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         42939.8 0.0544283
objective_val[200000]:         18720.4 0.0279891
objective_val[300000]:         10222.3 0.0164658
objective_val[400000]:         7486.67 0.0110354
objective_val[500000]:         6555.39 0.00865968
objective_val[600000]:         6374.26 0.00788252
objective_val[700000]:         6282.64 0.00746812
objective_val[800000]:         6165.44 0.00717204
objective_val[900000]:         6147.25 0.00698934
objective_val[1000000]:         6134.02 0.00688781
objective_val[1100000]:         6091.03 0.00681141
objective_val[1200000]:         6066.08 0.00677002
objective_val[1300000]:         6057.26 0.00671816
objective_val[1400000]:         6051.02 0.00671056
objective_val[1500000]:          6045.8 0.00670424
objective_val[1600000]:          6042.9 0.00669841
objective_val[1700000]:         6042.22 0.00670726
objective_val[1800000]:         6036.96 0.00671868
objective_val[1900000]:         6034.92 0.00671777
objective_val[2000000]:         6033.01 0.00672458
objective_val[2100000]:         6031.55 0.00672881
objective_val[2200000]:          6030.8 0.00673504
objective_val[2300000]:         6030.03 0.00674586
objective_val[2400000]:          6027.5 0.0067492
objective_val[2500000]:         6027.58 0.00674138
objective_val[2600000]:         6026.14 0.00676664
objective_val[2700000]:         6024.69 0.00677452
objective_val[2800000]:          6024.7 0.00677828
objective_val[2900000]:         6023.15 0.00678758
objective_val[3000000]:         6022.87 0.00678788
objective_val[3100000]:         6022.18 0.00679353
objective_val[3200000]:         6021.89 0.00679967
objective_val[3300000]:         6021.09 0.00680334
objective_val[3400000]:          6021.2 0.00680012
objective_val[3500000]:         6021.13 0.00679407
objective_val[3600000]:         6020.92 0.00680257
objective_val[3700000]:         6020.51 0.0067959
objective_val[3800000]:         6020.26 0.00680687
objective_val[3900000]:         6020.18 0.00681224
objective_val[4000000]:         6019.49 0.00680949
objective_val[4100000]:         6019.61 0.00680659
objective_val[4200000]:         6018.77 0.00682274
objective_val[4300000]:         6018.92 0.00682042
objective_val[4400000]:         6018.37 0.00682626
objective_val[4500000]:         6018.37 0.00682464
objective_val[4600000]:         6018.17 0.00682683
objective_val[4700000]:         6017.72 0.00683323
objective_val[4800000]:         6017.28 0.00684399
objective_val[4900000]:         6017.08 0.0068416
objective_val[5000000]:         6016.89 0.00684992
objective_val[5100000]:          6016.1 0.00686354
objective_val[5200000]:         6016.27 0.0068579
objective_val[5300000]:         6015.97 0.00686284
objective_val[5400000]:         6016.33 0.00685546
objective_val[5500000]:         6015.98 0.00686155
objective_val[5600000]:         6015.63 0.00686314
objective_val[5700000]:         6016.49 0.00685236
objective_val[5800000]:         6015.46 0.00686284
objective_val[5900000]:          6015.3 0.00686517
objective_val[6000000]:         6015.26 0.00686561
objective_val[6100000]:         6015.39 0.00686642
objective_val[6200000]:          6015.1 0.00687042
objective_val[6300000]:         6014.88 0.00687365
objective_val[6400000]:         6014.61 0.00687624
objective_val[6500000]:          6014.6 0.00688235
objective_val[6600000]:         6014.31 0.0068876
objective_val[6700000]:         6014.21 0.00688366
objective_val[6800000]:         6014.59 0.00687887
objective_val[6900000]:         6014.11 0.00688401
objective_val[7000000]:         6014.09 0.00688448
objective_val[7100000]:         6013.78 0.00689432
objective_val[7200000]:         6013.56 0.00689491
objective_val[7300000]:         6013.83 0.00688957
objective_val[7400000]:         6014.02 0.00688801
objective_val[7500000]:         6013.96 0.00688519
objective_val[7600000]:            6014 0.00688481
objective_val[7700000]:         6013.61 0.00688988
objective_val[7800000]:         6013.33 0.0068964
objective_val[7900000]:         6013.44 0.00689335
objective_val[8000000]:         6013.46 0.00689344
objective_val[8100000]:         6014.16 0.006892
objective_val[8200000]:         6014.17 0.00689098
objective_val[8300000]:         6013.68 0.00689011
objective_val[8400000]:          6013.2 0.00689741
objective_val[8500000]:         6013.05 0.00690102
objective_val[8600000]:         6013.79 0.0068998
objective_val[8700000]:         6013.59 0.00689674
objective_val[8800000]:         6013.81 0.0068989
objective_val[8900000]:         6013.09 0.00690146
objective_val[9000000]:         6012.92 0.00689909
objective_val[9100000]:         6013.23 0.00690391
objective_val[9200000]:         6012.71 0.00690766
objective_val[9300000]:         6012.54 0.00691245
objective_val[9400000]:         6012.51 0.00691229
objective_val[9500000]:         6012.52 0.00691213
objective_val[9600000]:         6012.71 0.00691252
objective_val[9700000]:         6012.59 0.00691024
objective_val[9800000]:         6012.44 0.00690854
objective_val[9900000]:         6012.78 0.00690919
objective_val[10000000]:         6012.32 0.00691115
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6012.32 0.00691115 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=16 random + vector-between-2-classes orthogonalized
 start projection 16 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 16, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         52447.6 0.0602699
objective_val[200000]:         20735.9 0.0300737
objective_val[300000]:         11580.8 0.0183417
objective_val[400000]:          7993.4 0.0122571
objective_val[500000]:          6864.2 0.00935601
objective_val[600000]:         6441.22 0.00836776
objective_val[700000]:         6359.17 0.00776299
objective_val[800000]:         6325.52 0.00739103
objective_val[900000]:         6218.89 0.0071536
objective_val[1000000]:         6151.45 0.00696168
objective_val[1100000]:         6139.16 0.00688326
objective_val[1200000]:         6086.05 0.00684423
objective_val[1300000]:         6068.05 0.00679081
objective_val[1400000]:          6061.6 0.00676707
objective_val[1500000]:         6052.05 0.00675087
objective_val[1600000]:         6047.17 0.00674996
objective_val[1700000]:         6043.88 0.00671631
objective_val[1800000]:         6041.39 0.00672408
objective_val[1900000]:         6040.02 0.00669561
objective_val[2000000]:         6037.31 0.00670705
objective_val[2100000]:         6033.79 0.00672389
objective_val[2200000]:         6032.19 0.00672785
objective_val[2300000]:         6031.02 0.00673196
objective_val[2400000]:         6030.47 0.00674642
objective_val[2500000]:         6028.41 0.00676146
objective_val[2600000]:         6028.44 0.00676349
objective_val[2700000]:         6026.47 0.00676866
objective_val[2800000]:         6026.24 0.0067554
objective_val[2900000]:         6025.48 0.0067636
objective_val[3000000]:         6024.24 0.00677175
objective_val[3100000]:         6024.27 0.0067768
objective_val[3200000]:         6023.56 0.00677912
objective_val[3300000]:         6024.16 0.00675969
objective_val[3400000]:         6022.98 0.00678006
objective_val[3500000]:         6022.36 0.00678426
objective_val[3600000]:          6022.4 0.00678267
objective_val[3700000]:         6021.07 0.00679924
objective_val[3800000]:         6020.74 0.00680379
objective_val[3900000]:         6022.54 0.00680817
objective_val[4000000]:         6019.83 0.00681525
objective_val[4100000]:         6019.31 0.00681706
objective_val[4200000]:         6019.29 0.00682029
objective_val[4300000]:         6019.25 0.00681907
objective_val[4400000]:          6018.5 0.00682549
objective_val[4500000]:         6019.27 0.00683369
objective_val[4600000]:         6017.98 0.00683686
objective_val[4700000]:         6018.09 0.00683285
objective_val[4800000]:         6017.87 0.0068329
objective_val[4900000]:         6018.44 0.00682701
objective_val[5000000]:         6017.82 0.0068342
objective_val[5100000]:         6017.69 0.00683059
objective_val[5200000]:         6017.75 0.00684007
objective_val[5300000]:         6017.27 0.00683261
objective_val[5400000]:         6017.14 0.00683804
objective_val[5500000]:         6016.76 0.00684874
objective_val[5600000]:         6017.36 0.00684831
objective_val[5700000]:         6016.23 0.00685474
objective_val[5800000]:         6016.54 0.00685233
objective_val[5900000]:         6016.02 0.00685133
objective_val[6000000]:         6016.03 0.00685608
objective_val[6100000]:         6016.68 0.0068466
objective_val[6200000]:         6016.47 0.00684392
objective_val[6300000]:         6016.35 0.00684815
objective_val[6400000]:         6015.82 0.00685556
objective_val[6500000]:         6015.53 0.00685723
objective_val[6600000]:          6015.4 0.0068594
objective_val[6700000]:         6015.11 0.00686806
objective_val[6800000]:         6014.85 0.00687084
objective_val[6900000]:         6014.89 0.0068759
objective_val[7000000]:         6014.24 0.00688348
objective_val[7100000]:         6014.37 0.0068826
objective_val[7200000]:         6014.09 0.00688432
objective_val[7300000]:          6014.2 0.00688345
objective_val[7400000]:         6014.13 0.00688366
objective_val[7500000]:            6015 0.00689125
objective_val[7600000]:         6013.79 0.00688979
objective_val[7700000]:         6014.42 0.0068971
objective_val[7800000]:         6014.24 0.00690191
objective_val[7900000]:         6013.43 0.00689688
objective_val[8000000]:         6013.33 0.00689757
objective_val[8100000]:         6013.23 0.00690162
objective_val[8200000]:         6013.35 0.00690322
objective_val[8300000]:         6013.23 0.00690494
objective_val[8400000]:         6012.75 0.00690981
objective_val[8500000]:         6013.06 0.00691333
objective_val[8600000]:         6012.87 0.00691059
objective_val[8700000]:         6012.92 0.00690379
objective_val[8800000]:         6013.05 0.00690124
objective_val[8900000]:         6013.14 0.0068991
objective_val[9000000]:         6012.95 0.00690096
objective_val[9100000]:         6012.73 0.00690673
objective_val[9200000]:         6012.77 0.0069069
objective_val[9300000]:         6012.69 0.00690619
objective_val[9400000]:         6012.74 0.00690597
objective_val[9500000]:          6012.7 0.00690563
objective_val[9600000]:         6012.96 0.00690268
objective_val[9700000]:         6012.79 0.00690289
objective_val[9800000]:          6012.8 0.00690328
objective_val[9900000]:         6012.72 0.00690363
objective_val[10000000]:         6012.88 0.00690328
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6012.88 0.00690328 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=17 random + vector-between-2-classes orthogonalized
 start projection 17 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 17, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         62638.9 0.0658017
objective_val[200000]:         22922.1 0.0298197
objective_val[300000]:         11430.2 0.0160052
objective_val[400000]:         7840.71 0.00997184
objective_val[500000]:         6956.46 0.00725
objective_val[600000]:         6683.29 0.00664785
objective_val[700000]:         6522.65 0.00640306
objective_val[800000]:         6488.01 0.00639627
objective_val[900000]:         6441.85 0.00632222
objective_val[1000000]:         6427.84 0.00627809
objective_val[1100000]:         6425.13 0.00622958
objective_val[1200000]:         6422.44 0.00617919
objective_val[1300000]:         6417.97 0.00618959
objective_val[1400000]:         6416.75 0.00619949
objective_val[1500000]:         6414.64 0.00622257
objective_val[1600000]:         6411.73 0.0062295
objective_val[1700000]:         6412.35 0.00621843
objective_val[1800000]:         6410.01 0.00622213
objective_val[1900000]:         6410.51 0.00622344
objective_val[2000000]:         6409.71 0.00620222
objective_val[2100000]:          6409.5 0.00623185
objective_val[2200000]:         6408.88 0.00623235
objective_val[2300000]:         6408.15 0.0062227
objective_val[2400000]:          6407.5 0.00622301
objective_val[2500000]:         6407.42 0.00622039
objective_val[2600000]:         6406.81 0.00622277
objective_val[2700000]:         6406.62 0.00622783
objective_val[2800000]:         6406.26 0.00622959
objective_val[2900000]:         6405.66 0.00623527
objective_val[3000000]:         6405.16 0.00624573
objective_val[3100000]:         6404.35 0.00625265
objective_val[3200000]:         6404.57 0.00624562
objective_val[3300000]:         6407.32 0.00624863
objective_val[3400000]:         6403.83 0.00625638
objective_val[3500000]:         6404.38 0.00624302
objective_val[3600000]:         6404.69 0.00623955
objective_val[3700000]:         6405.37 0.0062388
objective_val[3800000]:         6404.89 0.00624248
objective_val[3900000]:          6404.5 0.00624604
objective_val[4000000]:         6403.04 0.00626944
objective_val[4100000]:         6403.02 0.0062713
objective_val[4200000]:          6403.3 0.0062702
objective_val[4300000]:         6402.95 0.00627315
objective_val[4400000]:         6402.67 0.00627313
objective_val[4500000]:         6402.46 0.00628271
objective_val[4600000]:         6402.54 0.00628182
objective_val[4700000]:         6402.94 0.0062745
objective_val[4800000]:         6403.02 0.006272
objective_val[4900000]:         6402.68 0.00627781
objective_val[5000000]:         6402.74 0.0062742
objective_val[5100000]:          6403.4 0.00627525
objective_val[5200000]:         6402.75 0.00627972
objective_val[5300000]:         6402.07 0.00628149
objective_val[5400000]:         6402.11 0.0062854
objective_val[5500000]:         6401.87 0.00628675
objective_val[5600000]:         6402.26 0.00627201
objective_val[5700000]:         6402.42 0.00626787
objective_val[5800000]:         6402.26 0.00627212
objective_val[5900000]:         6402.82 0.00627595
objective_val[6000000]:         6402.12 0.00628171
objective_val[6100000]:         6401.72 0.00628993
objective_val[6200000]:         6402.11 0.00628616
objective_val[6300000]:         6402.39 0.00627972
objective_val[6400000]:         6402.03 0.00627915
objective_val[6500000]:         6402.82 0.00627834
objective_val[6600000]:          6401.7 0.00628171
objective_val[6700000]:         6401.69 0.00628251
objective_val[6800000]:         6402.01 0.00628181
objective_val[6900000]:         6401.42 0.00628775
objective_val[7000000]:         6401.18 0.00629589
objective_val[7100000]:          6400.9 0.00629979
objective_val[7200000]:         6400.95 0.00629838
objective_val[7300000]:         6401.39 0.00629604
objective_val[7400000]:         6401.23 0.00629704
objective_val[7500000]:         6401.59 0.00629548
objective_val[7600000]:         6401.39 0.00629199
objective_val[7700000]:         6401.57 0.00628777
objective_val[7800000]:         6401.95 0.00628781
objective_val[7900000]:         6401.64 0.00628556
objective_val[8000000]:         6401.31 0.00628789
objective_val[8100000]:         6401.19 0.00628961
objective_val[8200000]:         6401.31 0.0062871
objective_val[8300000]:         6401.89 0.00628961
objective_val[8400000]:         6401.29 0.00628732
objective_val[8500000]:         6401.51 0.00628417
objective_val[8600000]:         6401.17 0.0062879
objective_val[8700000]:          6401.3 0.00628509
objective_val[8800000]:         6401.25 0.00628384
objective_val[8900000]:         6401.23 0.00628426
objective_val[9000000]:         6401.46 0.00628337
objective_val[9100000]:         6401.18 0.00629097
objective_val[9200000]:         6401.96 0.00629231
objective_val[9300000]:         6401.02 0.00629535
objective_val[9400000]:         6401.01 0.0062923
objective_val[9500000]:         6401.17 0.00629493
objective_val[9600000]:         6401.08 0.00629301
objective_val[9700000]:         6400.92 0.00629415
objective_val[9800000]:         6400.75 0.00629892
objective_val[9900000]:         6400.66 0.00629817
objective_val[10000000]:         6400.64 0.00629814
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6400.64 0.00629814 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=18 random + vector-between-2-classes orthogonalized
 start projection 18 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 18, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:          102212 0.0978745
objective_val[200000]:         36785.6 0.0440698
objective_val[300000]:         16409.5 0.024008
objective_val[400000]:          9793.2 0.0149525
objective_val[500000]:         7326.17 0.0104411
objective_val[600000]:         6769.77 0.00898727
objective_val[700000]:          6636.3 0.00813669
objective_val[800000]:         6523.93 0.00755427
objective_val[900000]:         6365.34 0.00713353
objective_val[1000000]:         6299.75 0.00689264
objective_val[1100000]:         6280.62 0.00671835
objective_val[1200000]:         6158.85 0.00668176
objective_val[1300000]:         6120.22 0.00668135
objective_val[1400000]:         6106.89 0.00664313
objective_val[1500000]:         6097.93 0.00665472
objective_val[1600000]:         6093.24 0.00667029
objective_val[1700000]:         6087.57 0.00667533
objective_val[1800000]:         6085.76 0.00665861
objective_val[1900000]:         6084.48 0.00665148
objective_val[2000000]:         6081.25 0.006641
objective_val[2100000]:         6079.55 0.00664455
objective_val[2200000]:         6078.04 0.00664996
objective_val[2300000]:         6076.89 0.00663802
objective_val[2400000]:          6075.8 0.00664347
objective_val[2500000]:         6074.51 0.00665456
objective_val[2600000]:         6074.23 0.00665602
objective_val[2700000]:         6073.87 0.00663789
objective_val[2800000]:         6073.06 0.00664385
objective_val[2900000]:         6073.04 0.00663585
objective_val[3000000]:         6073.18 0.00664376
objective_val[3100000]:         6071.87 0.00665488
objective_val[3200000]:         6071.51 0.00664565
objective_val[3300000]:         6071.14 0.00664173
objective_val[3400000]:         6070.93 0.00664188
objective_val[3500000]:         6070.15 0.00665657
objective_val[3600000]:         6071.73 0.00663868
objective_val[3700000]:         6070.38 0.0066347
objective_val[3800000]:         6070.64 0.00665534
objective_val[3900000]:         6069.02 0.00666486
objective_val[4000000]:         6069.27 0.0066532
objective_val[4100000]:         6070.01 0.00663829
objective_val[4200000]:         6069.58 0.0066432
objective_val[4300000]:         6069.07 0.00664365
objective_val[4400000]:         6069.03 0.00664373
objective_val[4500000]:         6069.17 0.00665074
objective_val[4600000]:         6069.42 0.0066489
objective_val[4700000]:         6068.68 0.00664717
objective_val[4800000]:         6068.62 0.00664358
objective_val[4900000]:         6068.56 0.0066437
objective_val[5000000]:         6068.67 0.00663873
objective_val[5100000]:         6068.48 0.00664224
objective_val[5200000]:          6068.5 0.00664045
objective_val[5300000]:         6068.23 0.0066504
objective_val[5400000]:         6068.34 0.00665244
objective_val[5500000]:         6068.24 0.00665384
objective_val[5600000]:         6067.88 0.00665347
objective_val[5700000]:         6068.34 0.00664751
objective_val[5800000]:          6067.7 0.00665029
objective_val[5900000]:         6067.76 0.00664627
objective_val[6000000]:         6067.55 0.00664732
objective_val[6100000]:         6067.57 0.00664416
objective_val[6200000]:         6067.41 0.0066478
objective_val[6300000]:         6067.43 0.00664956
objective_val[6400000]:         6067.29 0.00665345
objective_val[6500000]:         6067.29 0.00664943
objective_val[6600000]:         6067.29 0.00665412
objective_val[6700000]:         6067.13 0.00665332
objective_val[6800000]:         6067.48 0.00664718
objective_val[6900000]:         6067.37 0.00664839
objective_val[7000000]:            6067 0.00665059
objective_val[7100000]:         6066.72 0.006656
objective_val[7200000]:         6067.09 0.00665744
objective_val[7300000]:            6067 0.00665594
objective_val[7400000]:         6066.54 0.00665838
objective_val[7500000]:         6066.47 0.00665853
objective_val[7600000]:         6066.38 0.00666043
objective_val[7700000]:         6066.41 0.00666171
objective_val[7800000]:          6066.3 0.00666353
objective_val[7900000]:         6066.17 0.006664
objective_val[8000000]:         6066.14 0.00666408
objective_val[8100000]:         6065.88 0.00667139
objective_val[8200000]:         6066.28 0.00666484
objective_val[8300000]:         6066.27 0.00666779
objective_val[8400000]:         6065.97 0.00666811
objective_val[8500000]:         6066.17 0.00666411
objective_val[8600000]:         6065.92 0.00666653
objective_val[8700000]:         6066.17 0.00666618
objective_val[8800000]:         6066.26 0.00666029
objective_val[8900000]:         6066.22 0.00665717
objective_val[9000000]:         6065.98 0.00666261
objective_val[9100000]:         6065.66 0.0066672
objective_val[9200000]:         6065.78 0.00666601
objective_val[9300000]:         6065.72 0.00666546
objective_val[9400000]:         6065.92 0.00666871
objective_val[9500000]:         6065.96 0.00666482
objective_val[9600000]:         6065.85 0.00666803
objective_val[9700000]:         6065.49 0.00667159
objective_val[9800000]:         6065.55 0.00667379
objective_val[9900000]:         6065.48 0.0066714
objective_val[10000000]:         6065.45 0.00667129
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6065.45 0.00667129 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=19 random + vector-between-2-classes orthogonalized
 start projection 19 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 19, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         28419.9 0.0344552
objective_val[200000]:           12735 0.0182394
objective_val[300000]:         7878.15 0.0113691
objective_val[400000]:         6404.02 0.00877791
objective_val[500000]:         6178.64 0.00793041
objective_val[600000]:         6144.04 0.00769861
objective_val[700000]:         6134.86 0.00762137
objective_val[800000]:          6133.1 0.00752136
objective_val[900000]:         6131.19 0.00748794
objective_val[1000000]:         6129.74 0.00748921
objective_val[1100000]:          6129.1 0.00744693
objective_val[1200000]:         6129.47 0.00743842
objective_val[1300000]:         6127.26 0.00742197
objective_val[1400000]:         6127.73 0.00740965
objective_val[1500000]:         6127.01 0.00739936
objective_val[1600000]:         6127.66 0.00740421
objective_val[1700000]:          6127.8 0.00737172
objective_val[1800000]:         6128.12 0.00739292
objective_val[1900000]:         6126.88 0.00737808
objective_val[2000000]:         6127.69 0.00735772
objective_val[2100000]:         6127.71 0.00735995
objective_val[2200000]:         6128.85 0.00735137
objective_val[2300000]:         6127.72 0.00734734
objective_val[2400000]:         6127.22 0.00735111
objective_val[2500000]:         6127.94 0.00733339
objective_val[2600000]:          6128.1 0.00732221
objective_val[2700000]:         6128.04 0.00732114
objective_val[2800000]:         6128.02 0.00731872
objective_val[2900000]:         6127.51 0.00733272
objective_val[3000000]:         6127.56 0.00732869
objective_val[3100000]:         6127.99 0.00732105
objective_val[3200000]:         6127.59 0.00733255
objective_val[3300000]:         6127.85 0.00732826
objective_val[3400000]:         6128.04 0.00731552
objective_val[3500000]:         6127.59 0.00732229
objective_val[3600000]:         6127.07 0.00732562
objective_val[3700000]:         6127.31 0.00732184
objective_val[3800000]:         6126.83 0.0073252
objective_val[3900000]:         6127.23 0.00731787
objective_val[4000000]:         6127.24 0.00732068
objective_val[4100000]:         6127.14 0.00731964
objective_val[4200000]:         6126.72 0.00732719
objective_val[4300000]:         6127.12 0.0073161
objective_val[4400000]:         6127.56 0.00732005
objective_val[4500000]:            6127 0.00731933
objective_val[4600000]:         6126.79 0.00732176
objective_val[4700000]:         6126.88 0.0073208
objective_val[4800000]:         6126.54 0.00732968
objective_val[4900000]:         6126.72 0.00733392
objective_val[5000000]:         6126.19 0.00733241
objective_val[5100000]:         6126.23 0.00733262
objective_val[5200000]:         6126.63 0.00732521
objective_val[5300000]:         6126.67 0.00732552
objective_val[5400000]:         6126.45 0.0073248
objective_val[5500000]:         6126.78 0.00732357
objective_val[5600000]:          6126.5 0.00732691
objective_val[5700000]:         6126.88 0.0073231
objective_val[5800000]:         6127.35 0.00732462
objective_val[5900000]:         6126.34 0.00733019
objective_val[6000000]:         6127.28 0.00733852
objective_val[6100000]:         6126.19 0.00732931
objective_val[6200000]:         6126.05 0.00733558
objective_val[6300000]:         6126.49 0.00733403
objective_val[6400000]:          6126.4 0.00732401
objective_val[6500000]:         6126.58 0.00732158
objective_val[6600000]:         6126.35 0.00732315
objective_val[6700000]:         6126.51 0.00732157
objective_val[6800000]:         6126.29 0.00732322
objective_val[6900000]:         6126.43 0.00732179
objective_val[7000000]:         6126.55 0.00732109
objective_val[7100000]:         6127.14 0.00731042
objective_val[7200000]:         6127.17 0.00730671
objective_val[7300000]:         6127.12 0.00730615
objective_val[7400000]:         6127.37 0.00730527
objective_val[7500000]:         6127.45 0.00730068
objective_val[7600000]:         6127.99 0.00729754
objective_val[7700000]:         6127.44 0.00729861
objective_val[7800000]:         6127.13 0.00730415
objective_val[7900000]:         6127.31 0.00730149
objective_val[8000000]:         6127.31 0.00730732
objective_val[8100000]:         6126.98 0.00731159
objective_val[8200000]:         6126.93 0.00731045
objective_val[8300000]:         6126.92 0.0073128
objective_val[8400000]:         6126.98 0.00731397
objective_val[8500000]:         6127.11 0.00731092
objective_val[8600000]:         6126.92 0.00731584
objective_val[8700000]:         6126.43 0.00731671
objective_val[8800000]:         6126.71 0.00731227
objective_val[8900000]:         6126.74 0.00731267
objective_val[9000000]:          6126.4 0.00731819
objective_val[9100000]:         6126.64 0.00732028
objective_val[9200000]:         6126.25 0.00732284
objective_val[9300000]:         6126.77 0.00732071
objective_val[9400000]:         6126.04 0.00732574
objective_val[9500000]:         6126.39 0.00732379
objective_val[9600000]:         6126.38 0.00732552
objective_val[9700000]:         6126.02 0.00732691
objective_val[9800000]:         6125.91 0.00732982
objective_val[9900000]:          6126.3 0.0073246
objective_val[10000000]:         6126.36 0.00732035
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6126.36 0.00732035 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=20 random + vector-between-2-classes orthogonalized
 start projection 20 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 20, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         96465.7 0.0970298
objective_val[200000]:         31810.9 0.0415335
objective_val[300000]:         14792.1 0.0221681
objective_val[400000]:         9005.34 0.0131349
objective_val[500000]:         6981.04 0.00904661
objective_val[600000]:         6353.13 0.00800983
objective_val[700000]:         6166.25 0.00743954
objective_val[800000]:         6107.08 0.00713558
objective_val[900000]:         6093.81 0.00701181
objective_val[1000000]:         6082.02 0.00694625
objective_val[1100000]:         6083.37 0.00687415
objective_val[1200000]:         6077.38 0.00681223
objective_val[1300000]:         6072.78 0.00680286
objective_val[1400000]:         6073.24 0.00679594
objective_val[1500000]:         6070.33 0.00677582
objective_val[1600000]:         6068.93 0.00677381
objective_val[1700000]:         6068.77 0.00677327
objective_val[1800000]:         6069.43 0.00677858
objective_val[1900000]:         6066.21 0.00676802
objective_val[2000000]:         6066.56 0.00676643
objective_val[2100000]:         6065.41 0.00676969
objective_val[2200000]:          6064.1 0.00678226
objective_val[2300000]:         6062.25 0.00678805
objective_val[2400000]:         6062.47 0.00679238
objective_val[2500000]:         6062.22 0.0067826
objective_val[2600000]:          6062.1 0.00676728
objective_val[2700000]:         6063.41 0.00675413
objective_val[2800000]:         6061.65 0.0067603
objective_val[2900000]:         6061.65 0.00675592
objective_val[3000000]:         6062.18 0.00676002
objective_val[3100000]:         6061.32 0.00677342
objective_val[3200000]:         6061.91 0.00675355
objective_val[3300000]:         6061.71 0.00675445
objective_val[3400000]:         6060.97 0.00676313
objective_val[3500000]:         6060.26 0.00677615
objective_val[3600000]:         6061.38 0.00676901
objective_val[3700000]:          6060.7 0.00675782
objective_val[3800000]:         6060.68 0.0067563
objective_val[3900000]:         6060.69 0.00676224
objective_val[4000000]:         6061.66 0.00674991
objective_val[4100000]:         6060.91 0.00675205
objective_val[4200000]:         6060.61 0.00675656
objective_val[4300000]:         6061.74 0.00675852
objective_val[4400000]:         6060.09 0.00675739
objective_val[4500000]:         6060.15 0.00676022
objective_val[4600000]:          6060.3 0.00675197
objective_val[4700000]:         6060.12 0.00675523
objective_val[4800000]:         6059.81 0.00675804
objective_val[4900000]:         6060.23 0.00676342
objective_val[5000000]:         6060.13 0.00675637
objective_val[5100000]:         6060.87 0.00675585
objective_val[5200000]:         6059.27 0.00676611
objective_val[5300000]:         6059.66 0.00675869
objective_val[5400000]:         6059.56 0.00676633
objective_val[5500000]:         6059.03 0.00677394
objective_val[5600000]:         6059.54 0.00677397
objective_val[5700000]:         6058.63 0.00677811
objective_val[5800000]:         6058.46 0.00678281
objective_val[5900000]:         6058.46 0.00678428
objective_val[6000000]:         6058.91 0.00678195
objective_val[6100000]:         6058.74 0.00677962
objective_val[6200000]:         6058.61 0.00677828
objective_val[6300000]:          6058.7 0.00677731
objective_val[6400000]:         6058.58 0.00677993
objective_val[6500000]:         6059.15 0.00677977
objective_val[6600000]:         6058.78 0.00677511
objective_val[6700000]:         6059.72 0.00676864
objective_val[6800000]:         6059.78 0.00677335
objective_val[6900000]:         6058.99 0.00677832
objective_val[7000000]:         6058.61 0.00677627
objective_val[7100000]:         6058.32 0.00677833
objective_val[7200000]:         6058.35 0.00677724
objective_val[7300000]:         6058.31 0.00677822
objective_val[7400000]:         6058.51 0.00677357
objective_val[7500000]:         6058.59 0.0067794
objective_val[7600000]:         6058.75 0.00677597
objective_val[7700000]:         6058.92 0.0067749
objective_val[7800000]:         6059.05 0.00678136
objective_val[7900000]:         6058.76 0.00678232
objective_val[8000000]:         6058.52 0.0067775
objective_val[8100000]:         6058.64 0.00676924
objective_val[8200000]:         6058.56 0.00677195
objective_val[8300000]:         6058.52 0.00677448
objective_val[8400000]:         6058.65 0.00677314
objective_val[8500000]:         6058.19 0.0067785
objective_val[8600000]:         6058.33 0.00677551
objective_val[8700000]:         6057.94 0.00678479
objective_val[8800000]:         6057.95 0.00678817
objective_val[8900000]:         6057.92 0.00678474
objective_val[9000000]:         6057.79 0.00678707
objective_val[9100000]:         6058.07 0.00679156
objective_val[9200000]:          6057.9 0.0067873
objective_val[9300000]:         6058.14 0.00678229
objective_val[9400000]:         6058.06 0.00677946
objective_val[9500000]:         6058.07 0.00678004
objective_val[9600000]:          6058.6 0.00677353
objective_val[9700000]:         6058.28 0.0067756
objective_val[9800000]:         6058.23 0.00677616
objective_val[9900000]:         6058.02 0.00678132
objective_val[10000000]:          6058.3 0.00677421
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6058.3 0.00677421 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=21 random + vector-between-2-classes orthogonalized
 start projection 21 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 21, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         23364.7 0.0311627
objective_val[200000]:         9958.05 0.0154911
objective_val[300000]:            6997 0.00975846
objective_val[400000]:         6330.17 0.00782973
objective_val[500000]:         6126.56 0.00713934
objective_val[600000]:         6081.74 0.00699112
objective_val[700000]:         6073.83 0.0069369
objective_val[800000]:          6077.5 0.00686611
objective_val[900000]:         6066.37 0.00684708
objective_val[1000000]:         6068.61 0.00685936
objective_val[1100000]:         6064.95 0.00680268
objective_val[1200000]:         6063.14 0.00680745
objective_val[1300000]:         6063.71 0.00681303
objective_val[1400000]:         6064.32 0.00679469
objective_val[1500000]:         6062.72 0.00680514
objective_val[1600000]:         6061.54 0.00678639
objective_val[1700000]:         6060.53 0.00679463
objective_val[1800000]:          6060.3 0.00680144
objective_val[1900000]:         6062.26 0.00680091
objective_val[2000000]:         6060.25 0.00678441
objective_val[2100000]:         6060.37 0.00678013
objective_val[2200000]:         6060.68 0.00678445
objective_val[2300000]:         6061.35 0.00677307
objective_val[2400000]:         6061.78 0.00676094
objective_val[2500000]:         6060.22 0.0067631
objective_val[2600000]:         6059.82 0.00676794
objective_val[2700000]:         6060.37 0.00676915
objective_val[2800000]:         6059.04 0.00678283
objective_val[2900000]:         6059.45 0.00677758
objective_val[3000000]:         6061.05 0.00677662
objective_val[3100000]:          6059.4 0.00677642
objective_val[3200000]:         6059.23 0.0067782
objective_val[3300000]:         6059.38 0.00677371
objective_val[3400000]:         6058.85 0.00677661
objective_val[3500000]:          6059.7 0.0067783
objective_val[3600000]:         6058.63 0.00677981
objective_val[3700000]:         6058.72 0.00678106
objective_val[3800000]:         6058.74 0.00678712
objective_val[3900000]:         6059.16 0.00677632
objective_val[4000000]:         6059.16 0.00676503
objective_val[4100000]:         6059.05 0.00676855
objective_val[4200000]:         6058.85 0.00677312
objective_val[4300000]:         6058.27 0.00678371
objective_val[4400000]:         6058.68 0.00677718
objective_val[4500000]:         6058.31 0.00678202
objective_val[4600000]:         6058.85 0.00676942
objective_val[4700000]:         6059.23 0.00677018
objective_val[4800000]:         6059.46 0.00676608
objective_val[4900000]:         6059.19 0.00676643
objective_val[5000000]:         6059.04 0.00676803
objective_val[5100000]:         6058.46 0.00677512
objective_val[5200000]:         6058.73 0.0067819
objective_val[5300000]:          6058.7 0.00677379
objective_val[5400000]:         6058.55 0.00677802
objective_val[5500000]:          6058.5 0.00678491
objective_val[5600000]:         6058.35 0.00678302
objective_val[5700000]:          6058.1 0.00678437
objective_val[5800000]:         6058.71 0.00678218
objective_val[5900000]:         6058.12 0.00678256
objective_val[6000000]:         6058.04 0.00678489
objective_val[6100000]:         6058.28 0.00679252
objective_val[6200000]:         6057.88 0.00678839
objective_val[6300000]:         6057.99 0.00678451
objective_val[6400000]:         6057.86 0.00678639
objective_val[6500000]:         6058.08 0.00678275
objective_val[6600000]:         6060.15 0.0067766
objective_val[6700000]:         6058.47 0.00678177
objective_val[6800000]:         6058.61 0.00677481
objective_val[6900000]:         6058.36 0.00677177
objective_val[7000000]:         6058.42 0.00677221
objective_val[7100000]:         6058.16 0.00677756
objective_val[7200000]:         6058.59 0.0067728
objective_val[7300000]:         6058.22 0.00678026
objective_val[7400000]:         6058.28 0.00677804
objective_val[7500000]:         6058.16 0.00677942
objective_val[7600000]:          6058.1 0.0067801
objective_val[7700000]:         6057.85 0.00678538
objective_val[7800000]:         6057.57 0.00678937
objective_val[7900000]:         6057.53 0.00679285
objective_val[8000000]:          6057.6 0.00679205
objective_val[8100000]:         6058.06 0.0067863
objective_val[8200000]:         6057.58 0.00678985
objective_val[8300000]:         6057.86 0.00679163
objective_val[8400000]:         6057.71 0.00678616
objective_val[8500000]:         6057.79 0.00678513
objective_val[8600000]:         6057.56 0.00678777
objective_val[8700000]:          6057.8 0.00678838
objective_val[8800000]:         6058.42 0.00678063
objective_val[8900000]:         6058.06 0.00678366
objective_val[9000000]:         6057.46 0.00679224
objective_val[9100000]:         6057.56 0.00679662
objective_val[9200000]:         6057.83 0.00678378
objective_val[9300000]:          6058.1 0.00678361
objective_val[9400000]:         6057.86 0.00678332
objective_val[9500000]:          6057.9 0.00678095
objective_val[9600000]:         6057.95 0.00678005
objective_val[9700000]:         6057.98 0.00678296
objective_val[9800000]:         6057.83 0.00678219
objective_val[9900000]:         6057.94 0.00677968
objective_val[10000000]:         6057.97 0.00677758
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6057.97 0.00677758 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=22 random + vector-between-2-classes orthogonalized
 start projection 22 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 22, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:          125588 0.120193
objective_val[200000]:         41537.2 0.0507314
objective_val[300000]:         18240.1 0.0267093
objective_val[400000]:         10476.4 0.0157029
objective_val[500000]:         7687.26 0.0101275
objective_val[600000]:         6980.78 0.00836987
objective_val[700000]:         6757.78 0.00741711
objective_val[800000]:         6587.11 0.0069144
objective_val[900000]:         6521.91 0.0066269
objective_val[1000000]:         6490.16 0.00655438
objective_val[1100000]:         6476.39 0.00648421
objective_val[1200000]:         6469.76 0.00642055
objective_val[1300000]:         6463.54 0.00639501
objective_val[1400000]:         6458.66 0.00638302
objective_val[1500000]:         6457.46 0.00637083
objective_val[1600000]:         6454.67 0.00634525
objective_val[1700000]:         6453.04 0.00634202
objective_val[1800000]:         6451.31 0.006362
objective_val[1900000]:         6449.66 0.00638779
objective_val[2000000]:         6449.37 0.00636893
objective_val[2100000]:         6448.03 0.00635001
objective_val[2200000]:         6446.53 0.00637164
objective_val[2300000]:         6445.91 0.00636138
objective_val[2400000]:          6446.1 0.00635744
objective_val[2500000]:         6446.06 0.00635138
objective_val[2600000]:         6444.69 0.00635554
objective_val[2700000]:         6445.89 0.00636443
objective_val[2800000]:         6444.29 0.00635
objective_val[2900000]:         6444.62 0.00634158
objective_val[3000000]:         6443.99 0.00635248
objective_val[3100000]:         6443.53 0.00634491
objective_val[3200000]:         6443.48 0.00635223
objective_val[3300000]:         6442.63 0.00636091
objective_val[3400000]:         6441.87 0.00637163
objective_val[3500000]:         6441.79 0.00637247
objective_val[3600000]:         6442.41 0.00636855
objective_val[3700000]:         6441.43 0.00638016
objective_val[3800000]:         6441.85 0.00637135
objective_val[3900000]:         6441.69 0.00636831
objective_val[4000000]:         6441.84 0.00638012
objective_val[4100000]:         6442.09 0.00637158
objective_val[4200000]:         6441.51 0.00637106
objective_val[4300000]:         6440.97 0.00637327
objective_val[4400000]:          6440.8 0.0063753
objective_val[4500000]:         6440.28 0.00638561
objective_val[4600000]:         6440.88 0.00637984
objective_val[4700000]:         6441.16 0.00636963
objective_val[4800000]:         6441.56 0.00636928
objective_val[4900000]:          6440.7 0.00637387
objective_val[5000000]:         6440.54 0.00637355
objective_val[5100000]:         6439.88 0.0063912
objective_val[5200000]:         6440.17 0.00638455
objective_val[5300000]:         6439.89 0.00638431
objective_val[5400000]:         6439.92 0.00639026
objective_val[5500000]:         6439.67 0.00638828
objective_val[5600000]:         6439.86 0.00638814
objective_val[5700000]:         6439.61 0.00638954
objective_val[5800000]:         6439.72 0.0063925
objective_val[5900000]:         6439.32 0.00639213
objective_val[6000000]:         6439.92 0.00638313
objective_val[6100000]:         6439.93 0.00638551
objective_val[6200000]:         6439.58 0.00638716
objective_val[6300000]:         6439.45 0.00638788
objective_val[6400000]:         6439.08 0.00639522
objective_val[6500000]:         6438.96 0.00640078
objective_val[6600000]:         6438.95 0.00640535
objective_val[6700000]:          6438.5 0.00640905
objective_val[6800000]:         6438.52 0.00640955
objective_val[6900000]:         6438.38 0.00640996
objective_val[7000000]:         6438.33 0.00641397
objective_val[7100000]:         6438.81 0.00640925
objective_val[7200000]:         6438.47 0.00640972
objective_val[7300000]:         6438.56 0.00640751
objective_val[7400000]:         6439.09 0.00640357
objective_val[7500000]:         6438.39 0.00641412
objective_val[7600000]:         6438.37 0.00641018
objective_val[7700000]:         6438.27 0.00641231
objective_val[7800000]:         6438.22 0.00641605
objective_val[7900000]:         6438.22 0.00641363
objective_val[8000000]:         6438.12 0.00641507
objective_val[8100000]:         6437.83 0.00642036
objective_val[8200000]:         6437.88 0.00641801
objective_val[8300000]:         6437.97 0.00641738
objective_val[8400000]:         6437.77 0.00642117
objective_val[8500000]:         6437.72 0.00642037
objective_val[8600000]:         6437.78 0.00641972
objective_val[8700000]:          6437.9 0.00641962
objective_val[8800000]:         6437.71 0.00642233
objective_val[8900000]:         6437.73 0.00642069
objective_val[9000000]:         6437.85 0.00641817
objective_val[9100000]:         6438.06 0.00641987
objective_val[9200000]:         6438.22 0.0064102
objective_val[9300000]:         6438.34 0.00640787
objective_val[9400000]:         6437.86 0.00641713
objective_val[9500000]:         6438.09 0.0064151
objective_val[9600000]:         6437.62 0.00642082
objective_val[9700000]:         6437.52 0.00642125
objective_val[9800000]:         6437.75 0.00641751
objective_val[9900000]:         6437.96 0.00641776
objective_val[10000000]:         6437.55 0.00642094
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6437.55 0.00642094 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=23 random + vector-between-2-classes orthogonalized
 start projection 23 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 23, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         60765.8 0.0702205
objective_val[200000]:         21802.4 0.0328104
objective_val[300000]:         10587.3 0.0177134
objective_val[400000]:         7193.94 0.0113548
objective_val[500000]:         6400.38 0.00860717
objective_val[600000]:         6118.35 0.00786755
objective_val[700000]:         6083.65 0.00750069
objective_val[800000]:         6071.06 0.00728016
objective_val[900000]:         6067.79 0.00715482
objective_val[1000000]:         6064.48 0.00707309
objective_val[1100000]:         6069.55 0.00698071
objective_val[1200000]:         6065.09 0.00693379
objective_val[1300000]:         6063.81 0.0069121
objective_val[1400000]:         6062.13 0.00690518
objective_val[1500000]:          6061.5 0.00688832
objective_val[1600000]:         6060.65 0.00688834
objective_val[1700000]:          6061.8 0.00685855
objective_val[1800000]:         6060.78 0.00683717
objective_val[1900000]:            6061 0.00684892
objective_val[2000000]:         6061.31 0.00681996
objective_val[2100000]:         6059.66 0.00683513
objective_val[2200000]:         6058.85 0.00683827
objective_val[2300000]:         6060.13 0.00681426
objective_val[2400000]:         6062.58 0.00680261
objective_val[2500000]:         6059.16 0.00681184
objective_val[2600000]:         6059.69 0.00679955
objective_val[2700000]:         6059.58 0.00679579
objective_val[2800000]:         6059.09 0.00680476
objective_val[2900000]:          6058.6 0.00682461
objective_val[3000000]:         6059.51 0.00680695
objective_val[3100000]:         6058.88 0.00679839
objective_val[3200000]:         6059.31 0.00679783
objective_val[3300000]:         6059.46 0.0067877
objective_val[3400000]:         6059.42 0.0067852
objective_val[3500000]:         6059.33 0.00678097
objective_val[3600000]:         6059.35 0.00678564
objective_val[3700000]:         6058.84 0.00678432
objective_val[3800000]:         6059.16 0.00678443
objective_val[3900000]:         6058.77 0.00678573
objective_val[4000000]:         6058.69 0.00678932
objective_val[4100000]:          6058.5 0.00679115
objective_val[4200000]:         6058.37 0.00679859
objective_val[4300000]:         6058.31 0.00679578
objective_val[4400000]:          6058.4 0.00679463
objective_val[4500000]:         6058.32 0.00679595
objective_val[4600000]:         6058.64 0.00678284
objective_val[4700000]:         6058.64 0.00678131
objective_val[4800000]:         6058.69 0.00677911
objective_val[4900000]:         6058.49 0.00678238
objective_val[5000000]:          6058.4 0.0067859
objective_val[5100000]:         6058.32 0.00678511
objective_val[5200000]:         6058.46 0.0067847
objective_val[5300000]:         6057.79 0.00679382
objective_val[5400000]:          6058.3 0.00678993
objective_val[5500000]:         6057.92 0.00679346
objective_val[5600000]:         6058.04 0.00679196
objective_val[5700000]:         6058.07 0.00678821
objective_val[5800000]:         6057.74 0.00679488
objective_val[5900000]:         6057.68 0.00679323
objective_val[6000000]:         6058.05 0.00679159
objective_val[6100000]:          6057.7 0.00679656
objective_val[6200000]:         6058.18 0.00679148
objective_val[6300000]:         6057.64 0.00679414
objective_val[6400000]:         6057.66 0.00679316
objective_val[6500000]:         6057.65 0.0067948
objective_val[6600000]:         6057.55 0.0067935
objective_val[6700000]:          6057.4 0.00679661
objective_val[6800000]:         6057.95 0.00678767
objective_val[6900000]:         6058.02 0.00678616
objective_val[7000000]:         6057.95 0.00678734
objective_val[7100000]:         6057.92 0.00678542
objective_val[7200000]:         6058.16 0.00678539
objective_val[7300000]:         6058.33 0.00678559
objective_val[7400000]:         6058.12 0.00678844
objective_val[7500000]:         6057.77 0.0067922
objective_val[7600000]:         6057.59 0.00679372
objective_val[7700000]:         6057.83 0.00678835
objective_val[7800000]:         6057.94 0.00678913
objective_val[7900000]:         6058.67 0.0067909
objective_val[8000000]:         6059.04 0.00678488
objective_val[8100000]:         6058.08 0.00678617
objective_val[8200000]:         6057.87 0.00678439
objective_val[8300000]:          6057.7 0.00678622
objective_val[8400000]:         6057.85 0.00678816
objective_val[8500000]:         6057.78 0.00678516
objective_val[8600000]:         6057.93 0.00678554
objective_val[8700000]:         6058.22 0.00678487
objective_val[8800000]:         6058.38 0.00678198
objective_val[8900000]:         6058.24 0.0067816
objective_val[9000000]:         6058.17 0.00677849
objective_val[9100000]:         6058.05 0.00678017
objective_val[9200000]:         6057.95 0.00678154
objective_val[9300000]:         6057.84 0.00678378
objective_val[9400000]:         6058.24 0.00677992
objective_val[9500000]:         6058.04 0.00677971
objective_val[9600000]:         6057.98 0.00678455
objective_val[9700000]:         6057.68 0.00678823
objective_val[9800000]:         6057.78 0.00678593
objective_val[9900000]:         6058.13 0.00678713
objective_val[10000000]:         6057.75 0.00678629
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6057.75 0.00678629 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=24 random + vector-between-2-classes orthogonalized
 start projection 24 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 24, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         58828.2 0.0630653
objective_val[200000]:         22115.6 0.0290027
objective_val[300000]:         10664.4 0.0156541
objective_val[400000]:         7484.89 0.00954332
objective_val[500000]:          6558.9 0.00733339
objective_val[600000]:         6448.37 0.00676008
objective_val[700000]:         6425.66 0.00645236
objective_val[800000]:         6417.32 0.00631291
objective_val[900000]:          6413.6 0.00619571
objective_val[1000000]:         6411.43 0.0061594
objective_val[1100000]:         6410.51 0.00610195
objective_val[1200000]:         6409.99 0.00606166
objective_val[1300000]:         6410.62 0.00603321
objective_val[1400000]:         6408.54 0.00602873
objective_val[1500000]:         6409.56 0.00598592
objective_val[1600000]:         6408.64 0.00599912
objective_val[1700000]:         6409.58 0.00598468
objective_val[1800000]:          6410.2 0.00598729
objective_val[1900000]:         6407.53 0.00597463
objective_val[2000000]:         6407.53 0.00597525
objective_val[2100000]:         6408.81 0.00595952
objective_val[2200000]:         6408.58 0.00594432
objective_val[2300000]:         6408.59 0.00593448
objective_val[2400000]:         6407.64 0.00594968
objective_val[2500000]:         6408.14 0.00594199
objective_val[2600000]:         6407.94 0.00593009
objective_val[2700000]:         6408.51 0.00592323
objective_val[2800000]:         6408.11 0.00592891
objective_val[2900000]:         6408.45 0.00591729
objective_val[3000000]:         6407.66 0.00593544
objective_val[3100000]:         6407.32 0.00594482
objective_val[3200000]:          6406.4 0.00596004
objective_val[3300000]:         6406.59 0.00595451
objective_val[3400000]:         6406.72 0.00594921
objective_val[3500000]:         6406.92 0.0059407
objective_val[3600000]:         6406.63 0.00594034
objective_val[3700000]:         6406.67 0.00593646
objective_val[3800000]:         6407.08 0.00592656
objective_val[3900000]:         6407.13 0.00592516
objective_val[4000000]:         6407.69 0.00593196
objective_val[4100000]:         6407.26 0.00592794
objective_val[4200000]:         6407.51 0.00591593
objective_val[4300000]:         6407.08 0.00592233
objective_val[4400000]:         6406.82 0.00592024
objective_val[4500000]:         6406.84 0.00592199
objective_val[4600000]:         6406.99 0.00592451
objective_val[4700000]:         6406.73 0.0059268
objective_val[4800000]:         6406.69 0.00592327
objective_val[4900000]:         6406.78 0.00592102
objective_val[5000000]:         6407.03 0.00592373
objective_val[5100000]:          6407.1 0.00591977
objective_val[5200000]:         6406.85 0.00592688
objective_val[5300000]:          6406.6 0.00592551
objective_val[5400000]:         6406.59 0.00592578
objective_val[5500000]:         6406.46 0.00592734
objective_val[5600000]:         6406.59 0.00592381
objective_val[5700000]:         6407.05 0.00591106
objective_val[5800000]:         6407.02 0.0059129
objective_val[5900000]:         6406.92 0.00591596
objective_val[6000000]:         6407.07 0.00591872
objective_val[6100000]:          6407.3 0.00591526
objective_val[6200000]:         6406.76 0.00592213
objective_val[6300000]:         6407.14 0.00591587
objective_val[6400000]:         6406.35 0.00592347
objective_val[6500000]:         6406.33 0.00593246
objective_val[6600000]:         6406.43 0.00592841
objective_val[6700000]:         6406.34 0.00592871
objective_val[6800000]:          6406.3 0.00592557
objective_val[6900000]:         6406.49 0.00592086
objective_val[7000000]:         6406.32 0.0059221
objective_val[7100000]:         6406.41 0.00592121
objective_val[7200000]:         6406.96 0.00591183
objective_val[7300000]:         6406.63 0.00591589
objective_val[7400000]:         6406.54 0.00592177
objective_val[7500000]:         6406.52 0.00591767
objective_val[7600000]:         6406.37 0.00592369
objective_val[7700000]:         6407.21 0.00592711
objective_val[7800000]:         6407.37 0.00592983
objective_val[7900000]:         6406.37 0.00592774
objective_val[8000000]:         6406.38 0.00592381
objective_val[8100000]:         6406.37 0.00592202
objective_val[8200000]:          6406.4 0.00592198
objective_val[8300000]:         6406.36 0.00592956
objective_val[8400000]:         6406.11 0.00593172
objective_val[8500000]:         6406.24 0.00593355
objective_val[8600000]:         6406.63 0.00593675
objective_val[8700000]:         6405.87 0.00593296
objective_val[8800000]:         6406.08 0.00592662
objective_val[8900000]:         6405.97 0.0059281
objective_val[9000000]:         6406.24 0.00592769
objective_val[9100000]:         6406.27 0.00592559
objective_val[9200000]:         6406.28 0.00592751
objective_val[9300000]:         6406.22 0.00592592
objective_val[9400000]:          6406.5 0.00592457
objective_val[9500000]:         6406.22 0.0059243
objective_val[9600000]:         6406.57 0.00591979
objective_val[9700000]:         6406.18 0.00592341
objective_val[9800000]:         6406.32 0.0059201
objective_val[9900000]:         6406.33 0.00592001
objective_val[10000000]:         6406.44 0.00591575
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6406.44 0.00591575 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=25 random + vector-between-2-classes orthogonalized
 start projection 25 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 25, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         38917.9 0.0490952
objective_val[200000]:         15618.8 0.0235968
objective_val[300000]:         8967.44 0.0136291
objective_val[400000]:         6693.11 0.00927392
objective_val[500000]:         6268.94 0.00734948
objective_val[600000]:         6080.57 0.00691911
objective_val[700000]:         6051.67 0.00664177
objective_val[800000]:         6030.64 0.0065115
objective_val[900000]:         6022.95 0.00644289
objective_val[1000000]:         6021.43 0.00635
objective_val[1100000]:         6018.69 0.00631331
objective_val[1200000]:         6015.37 0.006309
objective_val[1300000]:         6016.86 0.00627292
objective_val[1400000]:         6015.42 0.00624259
objective_val[1500000]:         6014.33 0.00625782
objective_val[1600000]:         6013.45 0.00625857
objective_val[1700000]:         6012.49 0.00624429
objective_val[1800000]:         6012.45 0.00623593
objective_val[1900000]:          6012.6 0.00623049
objective_val[2000000]:         6011.79 0.00624712
objective_val[2100000]:         6011.68 0.00621998
objective_val[2200000]:         6011.88 0.00620882
objective_val[2300000]:         6012.54 0.00619563
objective_val[2400000]:         6011.64 0.00620409
objective_val[2500000]:         6011.57 0.00620763
objective_val[2600000]:         6011.08 0.00621908
objective_val[2700000]:         6011.04 0.00620169
objective_val[2800000]:         6010.97 0.00621139
objective_val[2900000]:         6012.01 0.00621572
objective_val[3000000]:         6011.06 0.00622539
objective_val[3100000]:          6010.9 0.00622952
objective_val[3200000]:         6010.26 0.00621626
objective_val[3300000]:         6010.45 0.00621949
objective_val[3400000]:         6010.36 0.00623267
objective_val[3500000]:         6010.23 0.00621619
objective_val[3600000]:         6010.17 0.00620725
objective_val[3700000]:         6009.63 0.00622111
objective_val[3800000]:         6009.86 0.00621318
objective_val[3900000]:         6010.18 0.00620451
objective_val[4000000]:         6010.06 0.00620715
objective_val[4100000]:         6009.96 0.0062083
objective_val[4200000]:         6011.28 0.00619116
objective_val[4300000]:         6009.97 0.00620341
objective_val[4400000]:         6010.18 0.00620748
objective_val[4500000]:         6009.82 0.00620275
objective_val[4600000]:         6009.45 0.00620822
objective_val[4700000]:         6009.57 0.00620571
objective_val[4800000]:         6009.82 0.00620491
objective_val[4900000]:         6010.12 0.00620722
objective_val[5000000]:         6009.44 0.00621359
objective_val[5100000]:         6009.35 0.00621644
objective_val[5200000]:          6009.3 0.00621391
objective_val[5300000]:         6009.64 0.00621142
objective_val[5400000]:         6009.48 0.00620778
objective_val[5500000]:          6009.7 0.006202
objective_val[5600000]:         6009.66 0.00620067
objective_val[5700000]:         6009.86 0.00619491
objective_val[5800000]:         6009.58 0.00619908
objective_val[5900000]:         6009.64 0.00619991
objective_val[6000000]:         6009.28 0.0062064
objective_val[6100000]:         6009.35 0.00620505
objective_val[6200000]:         6010.09 0.00619665
objective_val[6300000]:         6009.68 0.00620132
objective_val[6400000]:         6009.64 0.00620409
objective_val[6500000]:         6009.35 0.00620414
objective_val[6600000]:         6009.47 0.00620218
objective_val[6700000]:         6009.38 0.00620283
objective_val[6800000]:         6009.39 0.00620248
objective_val[6900000]:         6009.17 0.00620622
objective_val[7000000]:         6009.26 0.00620955
objective_val[7100000]:         6009.35 0.00620561
objective_val[7200000]:          6009.6 0.0062146
objective_val[7300000]:         6009.52 0.0062079
objective_val[7400000]:          6009.9 0.00620443
objective_val[7500000]:         6009.22 0.00620669
objective_val[7600000]:         6009.39 0.0062036
objective_val[7700000]:          6009.5 0.00620354
objective_val[7800000]:         6009.46 0.0062003
objective_val[7900000]:         6009.29 0.00620604
objective_val[8000000]:         6009.25 0.0062038
objective_val[8100000]:         6009.36 0.006205
objective_val[8200000]:         6009.28 0.0062032
objective_val[8300000]:         6009.32 0.0061993
objective_val[8400000]:         6009.66 0.00619966
objective_val[8500000]:         6009.47 0.00619881
objective_val[8600000]:          6009.6 0.00619138
objective_val[8700000]:         6009.71 0.00618985
objective_val[8800000]:         6009.57 0.0061912
objective_val[8900000]:         6009.78 0.00618787
objective_val[9000000]:         6009.71 0.00618645
objective_val[9100000]:         6009.83 0.00618611
objective_val[9200000]:         6009.62 0.00619099
objective_val[9300000]:          6009.6 0.00619222
objective_val[9400000]:         6010.05 0.00619056
objective_val[9500000]:         6009.66 0.0061928
objective_val[9600000]:         6009.68 0.00618979
objective_val[9700000]:         6009.38 0.00619531
objective_val[9800000]:         6009.32 0.0061982
objective_val[9900000]:         6009.68 0.00619498
objective_val[10000000]:          6009.5 0.00619589
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6009.5 0.00619589 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=26 random + vector-between-2-classes orthogonalized
 start projection 26 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 26, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         6570.93 0.0078787
objective_val[200000]:         6485.01 0.00719436
objective_val[300000]:          6476.6 0.00714633
objective_val[400000]:         6476.45 0.00705713
objective_val[500000]:         6472.69 0.00707792
objective_val[600000]:         6469.31 0.00705788
objective_val[700000]:         6469.06 0.00705529
objective_val[800000]:         6468.09 0.00706211
objective_val[900000]:         6467.33 0.00706287
objective_val[1000000]:         6467.27 0.00706985
objective_val[1100000]:         6469.91 0.00703307
objective_val[1200000]:         6467.88 0.00703528
objective_val[1300000]:         6469.65 0.00703681
objective_val[1400000]:         6468.24 0.00703294
objective_val[1500000]:         6467.22 0.00703772
objective_val[1600000]:         6466.84 0.00704656
objective_val[1700000]:         6466.35 0.00705034
objective_val[1800000]:         6465.99 0.00706525
objective_val[1900000]:          6469.3 0.0070551
objective_val[2000000]:         6465.74 0.00705576
objective_val[2100000]:         6465.99 0.00704842
objective_val[2200000]:         6466.58 0.00705661
objective_val[2300000]:         6466.07 0.00705461
objective_val[2400000]:         6465.81 0.00706864
objective_val[2500000]:         6465.26 0.00706384
objective_val[2600000]:         6466.12 0.00706051
objective_val[2700000]:         6466.03 0.00704727
objective_val[2800000]:         6466.98 0.00703128
objective_val[2900000]:          6466.9 0.00702526
objective_val[3000000]:         6466.05 0.00703992
objective_val[3100000]:         6465.89 0.00704503
objective_val[3200000]:         6466.24 0.00704441
objective_val[3300000]:         6466.95 0.00704751
objective_val[3400000]:         6467.51 0.00703203
objective_val[3500000]:         6466.36 0.00703662
objective_val[3600000]:         6466.03 0.00704292
objective_val[3700000]:         6466.74 0.00704802
objective_val[3800000]:          6466.2 0.00703997
objective_val[3900000]:          6466.7 0.00702385
objective_val[4000000]:          6467.1 0.00702804
objective_val[4100000]:          6467.5 0.0070379
objective_val[4200000]:         6467.33 0.00703416
objective_val[4300000]:         6466.62 0.00702484
objective_val[4400000]:         6466.46 0.00702492
objective_val[4500000]:         6465.58 0.00704371
objective_val[4600000]:         6466.36 0.00703593
objective_val[4700000]:         6466.24 0.00703485
objective_val[4800000]:         6465.78 0.00704505
objective_val[4900000]:         6465.53 0.00704777
objective_val[5000000]:         6465.55 0.00704855
objective_val[5100000]:         6465.92 0.00704374
objective_val[5200000]:         6465.75 0.00703937
objective_val[5300000]:         6466.14 0.00703367
objective_val[5400000]:         6466.26 0.00703783
objective_val[5500000]:          6465.8 0.00703941
objective_val[5600000]:         6465.65 0.00704166
objective_val[5700000]:         6465.59 0.00704159
objective_val[5800000]:         6465.61 0.00704249
objective_val[5900000]:          6465.7 0.00704706
objective_val[6000000]:         6465.51 0.00704965
objective_val[6100000]:          6465.7 0.00704821
objective_val[6200000]:         6465.42 0.00705598
objective_val[6300000]:         6465.77 0.00705836
objective_val[6400000]:         6465.12 0.00705913
objective_val[6500000]:         6465.48 0.0070491
objective_val[6600000]:         6465.26 0.00705243
objective_val[6700000]:         6465.43 0.00705016
objective_val[6800000]:         6465.52 0.00704401
objective_val[6900000]:          6465.2 0.00705124
objective_val[7000000]:         6465.83 0.00704893
objective_val[7100000]:         6465.93 0.00704705
objective_val[7200000]:         6465.49 0.00704632
objective_val[7300000]:         6465.44 0.00704466
objective_val[7400000]:         6465.61 0.00704168
objective_val[7500000]:         6465.51 0.00704466
objective_val[7600000]:         6465.59 0.00704083
objective_val[7700000]:         6465.67 0.00704096
objective_val[7800000]:         6465.86 0.00704077
objective_val[7900000]:         6465.98 0.00703989
objective_val[8000000]:         6465.74 0.00703983
objective_val[8100000]:         6465.61 0.00703963
objective_val[8200000]:         6465.73 0.00704029
objective_val[8300000]:         6465.52 0.00704004
objective_val[8400000]:          6465.8 0.00703509
objective_val[8500000]:         6465.92 0.00703623
objective_val[8600000]:         6465.77 0.00703843
objective_val[8700000]:         6465.94 0.00704169
objective_val[8800000]:         6465.52 0.0070442
objective_val[8900000]:         6465.52 0.00704214
objective_val[9000000]:         6465.54 0.00703991
objective_val[9100000]:          6465.6 0.00703706
objective_val[9200000]:         6465.57 0.00704059
objective_val[9300000]:         6465.66 0.00703778
objective_val[9400000]:         6465.84 0.00703876
objective_val[9500000]:         6465.72 0.00703982
objective_val[9600000]:         6465.96 0.00703935
objective_val[9700000]:          6465.5 0.00704209
objective_val[9800000]:         6465.59 0.00703879
objective_val[9900000]:         6465.73 0.00703661
objective_val[10000000]:         6465.71 0.00703613
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6465.71 0.00703613 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=27 random + vector-between-2-classes orthogonalized
 start projection 27 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 27, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         31657.1 0.039238
objective_val[200000]:         14071.3 0.0201443
objective_val[300000]:         8278.02 0.0117663
objective_val[400000]:         6642.19 0.00844658
objective_val[500000]:         6264.49 0.00689802
objective_val[600000]:         6157.92 0.00660832
objective_val[700000]:         6121.96 0.00645288
objective_val[800000]:         6109.95 0.00640852
objective_val[900000]:         6106.08 0.0063646
objective_val[1000000]:         6065.07 0.00639115
objective_val[1100000]:         6055.51 0.00640192
objective_val[1200000]:         6050.63 0.00639943
objective_val[1300000]:         6045.02 0.00642716
objective_val[1400000]:            6043 0.00647186
objective_val[1500000]:         6040.86 0.00648595
objective_val[1600000]:         6037.59 0.00649819
objective_val[1700000]:         6034.28 0.00653198
objective_val[1800000]:         6032.43 0.00653536
objective_val[1900000]:         6032.82 0.00655124
objective_val[2000000]:         6030.11 0.00655428
objective_val[2100000]:         6029.67 0.00656253
objective_val[2200000]:         6028.95 0.0065632
objective_val[2300000]:          6028.6 0.00657294
objective_val[2400000]:          6027.1 0.00657641
objective_val[2500000]:         6025.96 0.00658494
objective_val[2600000]:         6026.27 0.00658474
objective_val[2700000]:         6023.86 0.0066145
objective_val[2800000]:         6023.27 0.00662078
objective_val[2900000]:         6022.76 0.00662998
objective_val[3000000]:         6022.32 0.00663024
objective_val[3100000]:         6022.01 0.0066466
objective_val[3200000]:         6021.26 0.00665029
objective_val[3300000]:            6021 0.00665572
objective_val[3400000]:         6020.61 0.00668193
objective_val[3500000]:         6019.18 0.00668855
objective_val[3600000]:         6019.83 0.00668329
objective_val[3700000]:         6019.67 0.00668073
objective_val[3800000]:         6019.65 0.00668331
objective_val[3900000]:          6018.6 0.00670025
objective_val[4000000]:         6018.38 0.00670376
objective_val[4100000]:         6017.67 0.00670905
objective_val[4200000]:         6018.56 0.00670322
objective_val[4300000]:         6017.34 0.00671349
objective_val[4400000]:         6017.21 0.00671709
objective_val[4500000]:         6017.57 0.00671536
objective_val[4600000]:         6016.53 0.0067275
objective_val[4700000]:         6016.32 0.00673156
objective_val[4800000]:         6016.07 0.00673898
objective_val[4900000]:         6015.75 0.00674664
objective_val[5000000]:         6015.67 0.00674385
objective_val[5100000]:         6015.74 0.00674556
objective_val[5200000]:         6015.55 0.00674554
objective_val[5300000]:         6016.25 0.00674944
objective_val[5400000]:         6015.25 0.00675479
objective_val[5500000]:         6014.53 0.00676662
objective_val[5600000]:          6014.5 0.00676673
objective_val[5700000]:         6014.07 0.00677262
objective_val[5800000]:            6014 0.00677343
objective_val[5900000]:         6013.91 0.00677738
objective_val[6000000]:         6013.95 0.00677493
objective_val[6100000]:         6014.07 0.00677777
objective_val[6200000]:         6014.22 0.00676632
objective_val[6300000]:         6014.49 0.00676849
objective_val[6400000]:         6013.95 0.0067722
objective_val[6500000]:         6014.29 0.00677534
objective_val[6600000]:         6013.84 0.00678664
objective_val[6700000]:         6013.93 0.00677408
objective_val[6800000]:         6013.56 0.00677922
objective_val[6900000]:          6013.4 0.00678111
objective_val[7000000]:          6013.6 0.0067813
objective_val[7100000]:         6013.34 0.00678227
objective_val[7200000]:         6013.43 0.00678445
objective_val[7300000]:         6013.79 0.00678985
objective_val[7400000]:         6013.33 0.00678979
objective_val[7500000]:         6013.05 0.00679246
objective_val[7600000]:         6013.06 0.00679088
objective_val[7700000]:         6012.92 0.00679636
objective_val[7800000]:         6012.88 0.00679545
objective_val[7900000]:         6013.43 0.00678917
objective_val[8000000]:         6013.12 0.00679011
objective_val[8100000]:         6012.91 0.00679301
objective_val[8200000]:         6013.25 0.00678809
objective_val[8300000]:         6013.55 0.00679095
objective_val[8400000]:          6014.1 0.00679324
objective_val[8500000]:         6013.11 0.00679422
objective_val[8600000]:         6012.48 0.00679759
objective_val[8700000]:         6012.61 0.00679522
objective_val[8800000]:         6012.74 0.00679844
objective_val[8900000]:         6012.21 0.00680205
objective_val[9000000]:         6012.61 0.00679824
objective_val[9100000]:         6012.35 0.00680057
objective_val[9200000]:         6012.18 0.00680175
objective_val[9300000]:         6012.12 0.00680427
objective_val[9400000]:         6012.09 0.0068079
objective_val[9500000]:         6011.92 0.0068081
objective_val[9600000]:         6011.66 0.00681383
objective_val[9700000]:         6011.57 0.00681588
objective_val[9800000]:         6011.78 0.00681115
objective_val[9900000]:         6011.73 0.0068114
objective_val[10000000]:         6011.68 0.00681328
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6011.68 0.00681328 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=28 random + vector-between-2-classes orthogonalized
 start projection 28 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 28, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         64030.9 0.067638
objective_val[200000]:         23405.7 0.0312879
objective_val[300000]:         12045.1 0.0172532
objective_val[400000]:         7911.37 0.0105724
objective_val[500000]:         6745.26 0.00765324
objective_val[600000]:         6483.33 0.006968
objective_val[700000]:         6435.55 0.00656916
objective_val[800000]:         6421.92 0.00638568
objective_val[900000]:         6421.44 0.00623646
objective_val[1000000]:         6419.28 0.00616796
objective_val[1100000]:         6415.73 0.00611758
objective_val[1200000]:            6413 0.00607415
objective_val[1300000]:         6411.54 0.00606418
objective_val[1400000]:         6411.38 0.00606017
objective_val[1500000]:         6410.57 0.00601149
objective_val[1600000]:         6410.66 0.00598566
objective_val[1700000]:         6409.72 0.00597522
objective_val[1800000]:         6409.91 0.00597046
objective_val[1900000]:         6410.22 0.00594675
objective_val[2000000]:         6408.97 0.00595905
objective_val[2100000]:         6410.61 0.00593053
objective_val[2200000]:          6410.1 0.00592996
objective_val[2300000]:         6409.36 0.00593613
objective_val[2400000]:         6408.25 0.00594374
objective_val[2500000]:         6407.27 0.00595771
objective_val[2600000]:         6407.51 0.00594488
objective_val[2700000]:         6407.94 0.005937
objective_val[2800000]:         6408.02 0.0059399
objective_val[2900000]:          6408.3 0.00593057
objective_val[3000000]:         6408.03 0.0059288
objective_val[3100000]:         6408.03 0.00593801
objective_val[3200000]:         6407.27 0.00594179
objective_val[3300000]:         6407.66 0.00593307
objective_val[3400000]:         6407.13 0.00594078
objective_val[3500000]:         6406.97 0.00594702
objective_val[3600000]:         6406.91 0.00593148
objective_val[3700000]:         6407.07 0.00594392
objective_val[3800000]:         6407.05 0.00593517
objective_val[3900000]:         6407.22 0.0059322
objective_val[4000000]:         6406.49 0.00594224
objective_val[4100000]:         6406.79 0.00593885
objective_val[4200000]:         6406.82 0.00594076
objective_val[4300000]:         6406.28 0.00594516
objective_val[4400000]:         6406.72 0.00593408
objective_val[4500000]:         6406.91 0.00593089
objective_val[4600000]:         6406.69 0.00593229
objective_val[4700000]:         6406.95 0.00592582
objective_val[4800000]:         6406.85 0.00592341
objective_val[4900000]:         6408.27 0.0059239
objective_val[5000000]:         6407.83 0.00591125
objective_val[5100000]:         6406.91 0.00591922
objective_val[5200000]:         6408.93 0.00590587
objective_val[5300000]:         6407.57 0.00591258
objective_val[5400000]:         6407.35 0.00591337
objective_val[5500000]:         6407.31 0.00590742
objective_val[5600000]:         6407.52 0.00590065
objective_val[5700000]:         6407.73 0.00590061
objective_val[5800000]:         6407.77 0.00589958
objective_val[5900000]:         6407.82 0.00590092
objective_val[6000000]:          6407.5 0.00590596
objective_val[6100000]:         6407.23 0.0059056
objective_val[6200000]:         6407.29 0.00590966
objective_val[6300000]:         6406.59 0.00591753
objective_val[6400000]:         6406.71 0.00591753
objective_val[6500000]:         6407.22 0.00591615
objective_val[6600000]:         6406.73 0.0059157
objective_val[6700000]:         6406.77 0.00591395
objective_val[6800000]:         6406.77 0.00592117
objective_val[6900000]:         6406.47 0.00591981
objective_val[7000000]:          6406.8 0.00591658
objective_val[7100000]:         6406.99 0.00592533
objective_val[7200000]:         6406.71 0.00591821
objective_val[7300000]:         6406.85 0.00591263
objective_val[7400000]:          6407.4 0.00591395
objective_val[7500000]:          6406.9 0.0059107
objective_val[7600000]:         6406.74 0.0059106
objective_val[7700000]:          6406.7 0.00591274
objective_val[7800000]:         6406.73 0.00591328
objective_val[7900000]:         6406.66 0.00591694
objective_val[8000000]:         6406.77 0.0059197
objective_val[8100000]:         6406.32 0.00592207
objective_val[8200000]:         6406.41 0.00591826
objective_val[8300000]:         6406.61 0.0059158
objective_val[8400000]:         6406.43 0.00592073
objective_val[8500000]:         6406.39 0.00591927
objective_val[8600000]:         6406.42 0.00591956
objective_val[8700000]:         6406.26 0.00592301
objective_val[8800000]:         6406.13 0.0059246
objective_val[8900000]:         6406.04 0.00592755
objective_val[9000000]:         6406.25 0.00592911
objective_val[9100000]:         6406.22 0.0059225
objective_val[9200000]:         6406.28 0.00592163
objective_val[9300000]:         6406.31 0.00592276
objective_val[9400000]:          6406.3 0.00592078
objective_val[9500000]:         6406.59 0.005915
objective_val[9600000]:         6406.62 0.00591589
objective_val[9700000]:         6406.37 0.00591781
objective_val[9800000]:         6406.52 0.00591862
objective_val[9900000]:         6406.92 0.00591415
objective_val[10000000]:         6406.42 0.0059164
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6406.42 0.0059164 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=30 projection_dim=29 random + vector-between-2-classes orthogonalized
 start projection 29 w.norm=1
 MCpermState::init(projection[60000],y,nc) nClasses=10)
projection_dim: 29, batch_size: 1, noClasses: 10, C1: 0.1, C2: 0.01, lambda: 100, size w: 784
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:         66772.4 0.0664069
objective_val[200000]:         26007.6 0.0311004
objective_val[300000]:         13245.1 0.0171656
objective_val[400000]:         8542.24 0.0109072
objective_val[500000]:         7030.05 0.00802449
objective_val[600000]:         6657.65 0.00720537
objective_val[700000]:         6587.84 0.00676015
objective_val[800000]:         6565.16 0.00656528
objective_val[900000]:         6557.36 0.0064005
objective_val[1000000]:         6550.86 0.00634322
objective_val[1100000]:          6547.1 0.00630019
objective_val[1200000]:         6546.27 0.00622313
objective_val[1300000]:         6543.76 0.00621391
objective_val[1400000]:         6542.01 0.00621402
objective_val[1500000]:         6540.42 0.00618745
objective_val[1600000]:         6539.48 0.00617474
objective_val[1700000]:         6539.06 0.00616368
objective_val[1800000]:         6540.11 0.00611943
objective_val[1900000]:         6540.45 0.00612446
objective_val[2000000]:         6538.12 0.00612274
objective_val[2100000]:         6537.76 0.00613749
objective_val[2200000]:         6537.26 0.00614326
objective_val[2300000]:         6538.13 0.00614525
objective_val[2400000]:         6536.54 0.00613538
objective_val[2500000]:         6536.58 0.00612637
objective_val[2600000]:         6536.26 0.00612749
objective_val[2700000]:         6535.86 0.00612656
objective_val[2800000]:         6535.91 0.00612065
objective_val[2900000]:         6536.52 0.0061181
objective_val[3000000]:         6536.03 0.00611524
objective_val[3100000]:         6535.88 0.00612615
objective_val[3200000]:         6535.81 0.00613608
objective_val[3300000]:         6534.31 0.00614485
objective_val[3400000]:         6535.13 0.00613082
objective_val[3500000]:         6535.35 0.00612557
objective_val[3600000]:         6534.74 0.00613186
objective_val[3700000]:         6534.48 0.00612705
objective_val[3800000]:         6534.99 0.006127
objective_val[3900000]:            6535 0.00612184
objective_val[4000000]:         6535.28 0.00612815
objective_val[4100000]:         6534.87 0.0061211
objective_val[4200000]:         6535.24 0.00613206
objective_val[4300000]:          6533.8 0.00613799
objective_val[4400000]:         6533.93 0.00613386
objective_val[4500000]:         6533.85 0.00613567
objective_val[4600000]:         6533.48 0.00614825
objective_val[4700000]:         6533.24 0.00615634
objective_val[4800000]:         6534.23 0.00614493
objective_val[4900000]:         6533.35 0.00614536
objective_val[5000000]:         6534.47 0.00614108
objective_val[5100000]:         6533.88 0.00614036
objective_val[5200000]:         6533.97 0.0061258
objective_val[5300000]:         6533.67 0.00613613
objective_val[5400000]:         6534.27 0.00612636
objective_val[5500000]:         6533.71 0.0061256
objective_val[5600000]:         6533.96 0.006129
objective_val[5700000]:         6533.29 0.00614112
objective_val[5800000]:         6533.46 0.00613972
objective_val[5900000]:         6533.28 0.00613562
objective_val[6000000]:         6533.11 0.00613911
objective_val[6100000]:         6533.23 0.00613987
objective_val[6200000]:         6534.04 0.00614519
objective_val[6300000]:         6533.09 0.00614024
objective_val[6400000]:         6532.89 0.00614537
objective_val[6500000]:          6533.3 0.00613651
objective_val[6600000]:            6533 0.00613946
objective_val[6700000]:         6532.44 0.00615242
objective_val[6800000]:         6532.96 0.00614677
objective_val[6900000]:         6532.68 0.00614618
objective_val[7000000]:         6532.61 0.00615011
objective_val[7100000]:         6532.47 0.00615219
objective_val[7200000]:         6532.94 0.00615597
objective_val[7300000]:         6533.01 0.00615537
objective_val[7400000]:         6532.97 0.00614759
objective_val[7500000]:         6532.95 0.00613771
objective_val[7600000]:         6532.91 0.00613718
objective_val[7700000]:         6532.79 0.00613874
objective_val[7800000]:         6532.64 0.00614156
objective_val[7900000]:         6532.79 0.00614001
objective_val[8000000]:          6533.1 0.00613705
objective_val[8100000]:         6533.12 0.0061329
objective_val[8200000]:         6532.92 0.00613538
objective_val[8300000]:         6532.97 0.00614522
objective_val[8400000]:         6532.66 0.00614286
objective_val[8500000]:         6532.78 0.00614274
objective_val[8600000]:          6532.4 0.00614936
objective_val[8700000]:         6532.48 0.00614927
objective_val[8800000]:         6532.38 0.0061503
objective_val[8900000]:         6532.04 0.00615533
objective_val[9000000]:         6532.11 0.00615258
objective_val[9100000]:         6532.34 0.00615127
objective_val[9200000]:         6532.14 0.00615307
objective_val[9300000]:         6532.24 0.0061495
objective_val[9400000]:         6532.17 0.00615473
objective_val[9500000]:          6532.2 0.00614922
objective_val[9600000]:          6532.5 0.00614373
objective_val[9700000]:          6532.5 0.00614215
objective_val[9800000]:         6532.73 0.00613745
objective_val[9900000]:         6533.08 0.00613437
objective_val[10000000]:         6532.64 0.00614017
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[10000000]: 6532.64 0.00614017 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 ~Proj{ngetSTD=363180,ngetAVG=0,nReuse=63150,nSwitch=0,nDemote=0}
MCsolveProgram::trySave()
 Writing MCsoln initially from .soln to mnist2d.soln
	mcdumpsoln -p < mnist2d.soln | less    # to prettyprint the soln
MCsolveProgram::tryRead()
 weights     norms:  0.00635438 0.00614413 0.00692908 0.00764694 0.00786053 0.00594881 0.00668356 0.00732655 0.00641293 0.00728549 0.0068045 0.00641985 0.00678832 0.00647877 0.00698997 0.00691115 0.00690328 0.00629814 0.00667129 0.00732035 0.00677421 0.00677758 0.00642094 0.00678629 0.00591575 0.00619589 0.00703613 0.00681328 0.0059164 0.00614017
 weights_avg norms:  0.00635438 0.00614413 0.00692908 0.00764694 0.00786053 0.00594881 0.00668356 0.00732655 0.00641293 0.00728549 0.0068045 0.00641985 0.00678832 0.00647877 0.00698997 0.00691115 0.00690328 0.00629814 0.00667129 0.00732035 0.00677421 0.00677758 0.00642094 0.00678629 0.00591575 0.00619589 0.00703613 0.00681328 0.0059164 0.00614017
normalized     weights[784x30]:
normalized weights_avg[784x30]:
      lower_bounds_avg[10x30]:
   2.06477   -9.43047  -0.237953   -1.69681   0.996908   -9.85055   -7.57235   -5.59042    2.13521   -5.58929    1.97633    2.17282    2.10711   -9.02825    1.03454   -4.87433   -4.92328   -9.55898   -9.91276    1.52771    2.09285    -10.034   -9.43897    2.06719   -10.3317     2.1333   -0.75761   -8.85574    2.41189   -9.53493
  -1.30179   0.289582   -5.17193   -6.24159   -1.03932   0.210687    1.72645   0.414237   -3.53451   0.430691   -1.21786   -3.48878   -4.30818   -1.27419   -5.05961   -1.03391   -1.05432   0.278169   0.538434   -4.29189   -4.32668   0.496752   0.226221   -4.33883  0.0367472   -1.16552   -4.99659   -1.02056   -3.14026   0.247089
   1.05127   -2.38515   -2.47582   -3.13511    1.22403   -2.52119   0.239026   -3.84454  -0.295028   -3.83676   0.578987  -0.256245   0.401078   -2.96802   -3.85716   -9.33976   -9.35244   -2.35189   -3.75087   0.464131   0.385689   -3.73755    -2.3786   0.355266   -4.07162   0.725881   -3.12325   -3.84531   0.732299    -2.4397
  0.498485    -4.6903    -3.9291   -0.80854  -0.387412   -4.68337     0.6378   -1.81883   0.248394   -1.80924   0.973102   0.289945  -0.736639   -4.57326   -3.04612   -3.51396   -3.55988   -3.71481   -2.21324  -0.776108  -0.749682   -1.84922    -3.1871  -0.780787   -4.78422    1.06184   -3.95918   -4.51683    1.09053   -4.75871
  -5.24368  -0.523088   0.233736   0.272837   -2.23677  -0.235207    -2.3429   -1.04585   -1.47311   -1.03905   -2.60515   -1.43449   -1.53437   0.605599  -0.237487 -0.0631128  -0.106592  -0.900723  -0.844053   -1.38892   -1.55869  -0.907808  -0.965931   -1.57027  -0.243746   -5.20678   0.443754 -0.0457732    -2.5482  -0.574051
 -0.282604   -3.76076   -1.75164  -0.944095  -0.525388   -3.77663   -1.69401   -2.74833    1.06384   -2.74428   0.262025     1.0969   0.154908    -3.3573  -0.504859   -2.10761   -2.14616   -4.43286   -2.75433   0.110473   0.136638   -2.79608   -4.46167   0.117232   -3.21745   0.255555   -1.66661   -2.89503   0.486806   -3.83179
  0.137667 -0.0453117   0.935857   -3.65605    1.95885  -0.658241   -3.97997   -9.00126  -0.491982   -8.96089  -0.734168  -0.452186    1.42703  -0.941143   0.529766   -2.83507   -2.86568   -1.99107   -5.09773    1.90536     1.4076   -5.19778   -2.07093    1.37541   -2.06787  -0.821092   0.933709   -1.73045  -0.384216  -0.100181
   -2.6301   -1.68331   -1.17296    1.31447   -5.63771   -1.68606  -0.746994  0.0658901   -2.70943   0.077929   -5.85311   -2.67728   -2.96346  0.0186828   -2.17961   0.724272   0.680608  -0.123258  0.0764553   -2.94426   -2.98355  0.0662019  -0.158965   -2.99831    -1.1504   -2.49534   -1.08473   0.669125   -1.28422    -1.7301
 -0.777834   -2.84017   -2.90733   -1.51494 -0.0506984    -2.8861  -0.290835   -2.30008    0.38493   -2.29271  -0.392803   0.430144  -0.411561   -2.10231     -1.744   -1.68155   -1.73043   -2.97599   -1.83312  -0.397857  -0.432955   -2.28045   -3.62331  -0.460993   -2.51227  -0.373315   -2.34029   -2.19894  -0.111963   -2.91614
  -3.37164  -0.944146  -0.631032   0.798378   -3.16755  -0.901737   -1.20209  -0.399369   -2.09388   -0.38922   -3.42358   -2.05576   -2.26411   0.801444   -1.13756   0.405129   0.359128  -0.438693  -0.256256   -2.17136   -2.28386  -0.322458  -0.508682   -2.30309  -0.525766   -3.34112  -0.188468   0.392209   -1.95849   -1.00314
      upper_bounds_avg[10x30]:
   9.64034   -2.07043    2.75841   0.715982    4.13638   -2.18475  -0.886871   -1.53886    9.34133   -1.53248    8.90835    9.40205    10.0837   -1.81385    7.60019   -1.32328   -1.36514    -2.1627   -2.05997    5.58206    10.0653   -2.07783    -2.1949    10.0249   -2.39138    9.79208    1.93157   -1.95399    10.3816   -2.12526
  0.989561    3.77827   -1.63195  -0.936446    1.28435     3.5269    5.17604    4.28163  -0.259063    4.30089     1.0405  -0.226892  -0.468826    0.99348   -1.56416    1.20819    1.18639    3.49798     4.2062  -0.425135  -0.478278    4.32674    3.50068  -0.508591    3.15197    1.13188   -1.50682    1.23873 -0.0284407    3.72593
   5.32365   0.338187   0.293471  0.0467554    5.52708   0.189886    3.18508  -0.477795    2.31309  -0.466842    3.86511    2.35608     3.7723   0.121181  -0.470917   -1.42728   -1.46479   0.276791  -0.340066    3.82977    3.75705  -0.376288     0.2411    3.70809  -0.726085      4.161 -0.0506734   -0.56114    4.08243   0.291696
   3.69737  -0.971403  -0.715041    1.78663    2.23367   -1.01859    3.81137    0.76689    3.13191   0.775832    4.55086    3.16996    1.88259  -0.924465  -0.147604  -0.441276  -0.488178  -0.574007   0.473707    1.80625    1.86693   0.764871  -0.302095    1.83195   -1.07865    4.83897  -0.678607  -0.945337    4.80356    -1.0274
 -0.694561    1.99418    3.19311    3.06822   0.387594    2.54325   0.352993    1.38383   0.912471    1.39074  0.0685989   0.951011   0.937332    3.61977    2.47132    2.70185    2.64505    1.50012    1.63138    1.03561   0.915941    1.56104    1.42101   0.898364    2.54911  -0.675851    3.42095    2.62448   0.248193    1.94301
   2.30411  -0.710666   0.703072    1.47878    1.95091  -0.727168   0.822772  -0.122212    4.39548   -0.11541    2.92696    4.43717    2.83204    -0.4515    1.99085   0.392321   0.349358   -1.06401  -0.089717    2.73627    2.81181  -0.129115   -1.11444    2.78746  -0.477705    3.00637   0.776523  -0.231993    3.23464  -0.761353
   2.87767    3.08879    6.81154  -0.607631    8.25275    2.13826  -0.454295   -1.91625    2.00639   -1.90478    1.76422    2.05219    5.24883     1.5992    3.98331   -0.24885  -0.276312   0.505279   -1.31933     9.0167    5.21748   -1.39311   0.435138     5.1842   0.403106    1.71424    6.47051   0.766105    2.09363    3.02729
  0.225699   0.683298     1.2912    7.77447  -0.652366   0.679019    1.81497    2.92869   0.112301    2.94908  -0.644533   0.144049 -0.0508585    2.83684    0.42456    6.12455    6.04897    2.72195    3.01924 -0.0784122 -0.0658088    2.97829    2.66168 -0.0834792    1.28066   0.297676    1.43676    5.89088    1.15145   0.641845
   1.73881  0.0189707  -0.132385      1.008    2.50512  -0.046907      2.345   0.385133    3.55404   0.395194    2.22286    3.59895    2.32085   0.546455   0.839579      0.797   0.751606 -0.0661903   0.789854    2.28027    2.29487    0.44399  -0.444479    2.26135   0.123232    2.28544   0.276709   0.416849     2.5291 -0.0399285
 -0.434351    1.41431    1.84102    3.99154  -0.182998    1.48458    1.19935    2.16185   0.449392    2.17637  -0.363437   0.488975   0.348243    5.72494    1.25203    3.45183    3.39304    2.12194    2.34426   0.390835   0.327157    2.28573    2.03133   0.307596    1.95864  -0.369885    2.32856    3.45254   0.531061    1.35411
 Projection 0 weights[ 784] 
 {l,u}:   0 {   2.06477,  9.64034} {  -1.30179, 0.989561} {   1.05127,  5.32365} {  0.498485,  3.69737} {  -5.24368,-0.694561} { -0.282604,  2.30411} {  0.137667,  2.87767} {   -2.6301, 0.225699}
 {l,u}:   8 { -0.777834,  1.73881} {  -3.37164,-0.434351} ...
 Some narrow non-zero intervals were:
 class      1 {  -1.30179, 0.989561 } width 2.29135
 class      8 { -0.777834, 1.73881 } width 2.51665
 class      5 { -0.282604, 2.30411 } width 2.58672
 class      6 {  0.137667, 2.87767 } width 2.74
 class      7 {   -2.6301, 0.225699 } width 2.8558
 class      9 {  -3.37164, -0.434351 } width 2.93729
 class      3 {  0.498485, 3.69737 } width 3.19889
 class      2 {   1.05127, 5.32365 } width 4.27238
 class      4 {  -5.24368, -0.694561 } width 4.54912
 class      0 {   2.06477, 9.64034 } width 7.57556
 Some wide non-zero intervals were:
 class      3 {  0.498485, 3.69737 } width 3.19889
 class      2 {   1.05127, 5.32365 } width 4.27238
 class      4 {  -5.24368, -0.694561 } width 4.54912
 class      0 {   2.06477, 9.64034 } width 7.57556
 0 classes had vanishing intervals, with lower > upper.
 Projection 1 weights[ 784] 
 {l,u}:   0 {  -9.43047, -2.07043} {  0.289582,  3.77827} {  -2.38515, 0.338187} {   -4.6903,-0.971403} { -0.523088,  1.99418} {  -3.76076,-0.710666} { -0.0453117,  3.08879} {  -1.68331, 0.683298}
 {l,u}:   8 {  -2.84017,0.0189707} { -0.944146,  1.41431} ...
 Some narrow non-zero intervals were:
 class      9 { -0.944146, 1.41431 } width 2.35846
 class      7 {  -1.68331, 0.683298 } width 2.36661
 class      4 { -0.523088, 1.99418 } width 2.51727
 class      2 {  -2.38515, 0.338187 } width 2.72334
 class      8 {  -2.84017, 0.0189707 } width 2.85914
 class      5 {  -3.76076, -0.710666 } width 3.0501
 class      6 {-0.0453117, 3.08879 } width 3.1341
 class      1 {  0.289582, 3.77827 } width 3.48869
 class      3 {   -4.6903, -0.971403 } width 3.7189
 class      0 {  -9.43047, -2.07043 } width 7.36004
 Some wide non-zero intervals were:
 class      6 {-0.0453117, 3.08879 } width 3.1341
 class      1 {  0.289582, 3.77827 } width 3.48869
 class      3 {   -4.6903, -0.971403 } width 3.7189
 class      0 {  -9.43047, -2.07043 } width 7.36004
 0 classes had vanishing intervals, with lower > upper.
 Projection 2 weights[ 784] 
 {l,u}:   0 { -0.237953,  2.75841} {  -5.17193, -1.63195} {  -2.47582, 0.293471} {   -3.9291,-0.715041} {  0.233736,  3.19311} {  -1.75164, 0.703072} {  0.935857,  6.81154} {  -1.17296,   1.2912}
 {l,u}:   8 {  -2.90733,-0.132385} { -0.631032,  1.84102} ...
 Some narrow non-zero intervals were:
 class      5 {  -1.75164, 0.703072 } width 2.45471
 class      7 {  -1.17296, 1.2912 } width 2.46416
 class      9 { -0.631032, 1.84102 } width 2.47205
 class      2 {  -2.47582, 0.293471 } width 2.76929
 class      8 {  -2.90733, -0.132385 } width 2.77494
 class      4 {  0.233736, 3.19311 } width 2.95938
 class      0 { -0.237953, 2.75841 } width 2.99636
 class      3 {   -3.9291, -0.715041 } width 3.21405
 class      1 {  -5.17193, -1.63195 } width 3.53998
 class      6 {  0.935857, 6.81154 } width 5.87568
 Some wide non-zero intervals were:
 class      0 { -0.237953, 2.75841 } width 2.99636
 class      3 {   -3.9291, -0.715041 } width 3.21405
 class      1 {  -5.17193, -1.63195 } width 3.53998
 class      6 {  0.935857, 6.81154 } width 5.87568
 0 classes had vanishing intervals, with lower > upper.
 Projection 3 weights[ 784] 
 {l,u}:   0 {  -1.69681, 0.715982} {  -6.24159,-0.936446} {  -3.13511,0.0467554} {  -0.80854,  1.78663} {  0.272837,  3.06822} { -0.944095,  1.47878} {  -3.65605,-0.607631} {   1.31447,  7.77447}
 {l,u}:   8 {  -1.51494,    1.008} {  0.798378,  3.99154} ...
 Some narrow non-zero intervals were:
 class      0 {  -1.69681, 0.715982 } width 2.4128
 class      5 { -0.944095, 1.47878 } width 2.42288
 class      8 {  -1.51494, 1.008 } width 2.52293
 class      3 {  -0.80854, 1.78663 } width 2.59517
 class      4 {  0.272837, 3.06822 } width 2.79538
 class      6 {  -3.65605, -0.607631 } width 3.04842
 class      2 {  -3.13511, 0.0467554 } width 3.18187
 class      9 {  0.798378, 3.99154 } width 3.19316
 class      1 {  -6.24159, -0.936446 } width 5.30514
 class      7 {   1.31447, 7.77447 } width 6.46
 Some wide non-zero intervals were:
 class      2 {  -3.13511, 0.0467554 } width 3.18187
 class      9 {  0.798378, 3.99154 } width 3.19316
 class      1 {  -6.24159, -0.936446 } width 5.30514
 class      7 {   1.31447, 7.77447 } width 6.46
 0 classes had vanishing intervals, with lower > upper.
 Projection 4 weights[ 784] 
 {l,u}:   0 {  0.996908,  4.13638} {  -1.03932,  1.28435} {   1.22403,  5.52708} { -0.387412,  2.23367} {  -2.23677, 0.387594} { -0.525388,  1.95091} {   1.95885,  8.25275} {  -5.63771,-0.652366}
 {l,u}:   8 { -0.0506984,  2.50512} {  -3.16755,-0.182998} ...
 Some narrow non-zero intervals were:
 class      1 {  -1.03932, 1.28435 } width 2.32367
 class      5 { -0.525388, 1.95091 } width 2.4763
 class      8 {-0.0506984, 2.50512 } width 2.55582
 class      3 { -0.387412, 2.23367 } width 2.62108
 class      4 {  -2.23677, 0.387594 } width 2.62437
 class      9 {  -3.16755, -0.182998 } width 2.98455
 class      0 {  0.996908, 4.13638 } width 3.13947
 class      2 {   1.22403, 5.52708 } width 4.30305
 class      7 {  -5.63771, -0.652366 } width 4.98534
 class      6 {   1.95885, 8.25275 } width 6.2939
 Some wide non-zero intervals were:
 class      0 {  0.996908, 4.13638 } width 3.13947
 class      2 {   1.22403, 5.52708 } width 4.30305
 class      7 {  -5.63771, -0.652366 } width 4.98534
 class      6 {   1.95885, 8.25275 } width 6.2939
 0 classes had vanishing intervals, with lower > upper.
 Projection 5 weights[ 784] 
 {l,u}:   0 {  -9.85055, -2.18475} {  0.210687,   3.5269} {  -2.52119, 0.189886} {  -4.68337, -1.01859} { -0.235207,  2.54325} {  -3.77663,-0.727168} { -0.658241,  2.13826} {  -1.68606, 0.679019}
 {l,u}:   8 {   -2.8861,-0.046907} { -0.901737,  1.48458} ...
 Some narrow non-zero intervals were:
 class      7 {  -1.68606, 0.679019 } width 2.36508
 class      9 { -0.901737, 1.48458 } width 2.38631
 class      2 {  -2.52119, 0.189886 } width 2.71108
 class      4 { -0.235207, 2.54325 } width 2.77846
 class      6 { -0.658241, 2.13826 } width 2.7965
 class      8 {   -2.8861, -0.046907 } width 2.83919
 class      5 {  -3.77663, -0.727168 } width 3.04946
 class      1 {  0.210687, 3.5269 } width 3.31621
 class      3 {  -4.68337, -1.01859 } width 3.66477
 class      0 {  -9.85055, -2.18475 } width 7.6658
 Some wide non-zero intervals were:
 class      5 {  -3.77663, -0.727168 } width 3.04946
 class      1 {  0.210687, 3.5269 } width 3.31621
 class      3 {  -4.68337, -1.01859 } width 3.66477
 class      0 {  -9.85055, -2.18475 } width 7.6658
 0 classes had vanishing intervals, with lower > upper.
 Projection 6 weights[ 784] 
 {l,u}:   0 {  -7.57235,-0.886871} {   1.72645,  5.17604} {  0.239026,  3.18508} {    0.6378,  3.81137} {   -2.3429, 0.352993} {  -1.69401, 0.822772} {  -3.97997,-0.454295} { -0.746994,  1.81497}
 {l,u}:   8 { -0.290835,    2.345} {  -1.20209,  1.19935} ...
 Some narrow non-zero intervals were:
 class      9 {  -1.20209, 1.19935 } width 2.40144
 class      5 {  -1.69401, 0.822772 } width 2.51678
 class      7 { -0.746994, 1.81497 } width 2.56196
 class      8 { -0.290835, 2.345 } width 2.63583
 class      4 {   -2.3429, 0.352993 } width 2.69589
 class      2 {  0.239026, 3.18508 } width 2.94605
 class      3 {    0.6378, 3.81137 } width 3.17357
 class      1 {   1.72645, 5.17604 } width 3.4496
 class      6 {  -3.97997, -0.454295 } width 3.52568
 class      0 {  -7.57235, -0.886871 } width 6.68547
 Some wide non-zero intervals were:
 class      3 {    0.6378, 3.81137 } width 3.17357
 class      1 {   1.72645, 5.17604 } width 3.4496
 class      6 {  -3.97997, -0.454295 } width 3.52568
 class      0 {  -7.57235, -0.886871 } width 6.68547
 0 classes had vanishing intervals, with lower > upper.
 Projection 7 weights[ 784] 
 {l,u}:   0 {  -5.59042, -1.53886} {  0.414237,  4.28163} {  -3.84454,-0.477795} {  -1.81883,  0.76689} {  -1.04585,  1.38383} {  -2.74833,-0.122212} {  -9.00126, -1.91625} { 0.0658901,  2.92869}
 {l,u}:   8 {  -2.30008, 0.385133} { -0.399369,  2.16185} ...
 Some narrow non-zero intervals were:
 class      4 {  -1.04585, 1.38383 } width 2.42967
 class      9 { -0.399369, 2.16185 } width 2.56122
 class      3 {  -1.81883, 0.76689 } width 2.58572
 class      5 {  -2.74833, -0.122212 } width 2.62612
 class      8 {  -2.30008, 0.385133 } width 2.68521
 class      7 { 0.0658901, 2.92869 } width 2.8628
 class      2 {  -3.84454, -0.477795 } width 3.36674
 class      1 {  0.414237, 4.28163 } width 3.86739
 class      0 {  -5.59042, -1.53886 } width 4.05155
 class      6 {  -9.00126, -1.91625 } width 7.08501
 Some wide non-zero intervals were:
 class      2 {  -3.84454, -0.477795 } width 3.36674
 class      1 {  0.414237, 4.28163 } width 3.86739
 class      0 {  -5.59042, -1.53886 } width 4.05155
 class      6 {  -9.00126, -1.91625 } width 7.08501
 0 classes had vanishing intervals, with lower > upper.
 Projection 8 weights[ 784] 
 {l,u}:   0 {   2.13521,  9.34133} {  -3.53451,-0.259063} { -0.295028,  2.31309} {  0.248394,  3.13191} {  -1.47311, 0.912471} {   1.06384,  4.39548} { -0.491982,  2.00639} {  -2.70943, 0.112301}
 {l,u}:   8 {   0.38493,  3.55404} {  -2.09388, 0.449392} ...
 Some narrow non-zero intervals were:
 class      4 {  -1.47311, 0.912471 } width 2.38558
 class      6 { -0.491982, 2.00639 } width 2.49837
 class      9 {  -2.09388, 0.449392 } width 2.54327
 class      2 { -0.295028, 2.31309 } width 2.60812
 class      7 {  -2.70943, 0.112301 } width 2.82173
 class      3 {  0.248394, 3.13191 } width 2.88352
 class      8 {   0.38493, 3.55404 } width 3.16911
 class      1 {  -3.53451, -0.259063 } width 3.27545
 class      5 {   1.06384, 4.39548 } width 3.33164
 class      0 {   2.13521, 9.34133 } width 7.20612
 Some wide non-zero intervals were:
 class      8 {   0.38493, 3.55404 } width 3.16911
 class      1 {  -3.53451, -0.259063 } width 3.27545
 class      5 {   1.06384, 4.39548 } width 3.33164
 class      0 {   2.13521, 9.34133 } width 7.20612
 0 classes had vanishing intervals, with lower > upper.
 Projection 9 weights[ 784] 
 {l,u}:   0 {  -5.58929, -1.53248} {  0.430691,  4.30089} {  -3.83676,-0.466842} {  -1.80924, 0.775832} {  -1.03905,  1.39074} {  -2.74428, -0.11541} {  -8.96089, -1.90478} {  0.077929,  2.94908}
 {l,u}:   8 {  -2.29271, 0.395194} {  -0.38922,  2.17637} ...
 Some narrow non-zero intervals were:
 class      4 {  -1.03905, 1.39074 } width 2.42979
 class      9 {  -0.38922, 2.17637 } width 2.56559
 class      3 {  -1.80924, 0.775832 } width 2.58507
 class      5 {  -2.74428, -0.11541 } width 2.62887
 class      8 {  -2.29271, 0.395194 } width 2.6879
 class      7 {  0.077929, 2.94908 } width 2.87115
 class      2 {  -3.83676, -0.466842 } width 3.36991
 class      1 {  0.430691, 4.30089 } width 3.87019
 class      0 {  -5.58929, -1.53248 } width 4.05682
 class      6 {  -8.96089, -1.90478 } width 7.05611
 Some wide non-zero intervals were:
 class      2 {  -3.83676, -0.466842 } width 3.36991
 class      1 {  0.430691, 4.30089 } width 3.87019
 class      0 {  -5.58929, -1.53248 } width 4.05682
 class      6 {  -8.96089, -1.90478 } width 7.05611
 0 classes had vanishing intervals, with lower > upper.
 Projection 10 weights[ 784] 
 {l,u}:   0 {   1.97633,  8.90835} {  -1.21786,   1.0405} {  0.578987,  3.86511} {  0.973102,  4.55086} {  -2.60515,0.0685989} {  0.262025,  2.92696} { -0.734168,  1.76422} {  -5.85311,-0.644533}
 {l,u}:   8 { -0.392803,  2.22286} {  -3.42358,-0.363437} ...
 Some narrow non-zero intervals were:
 class      1 {  -1.21786, 1.0405 } width 2.25836
 class      6 { -0.734168, 1.76422 } width 2.49839
 class      8 { -0.392803, 2.22286 } width 2.61566
 class      5 {  0.262025, 2.92696 } width 2.66494
 class      4 {  -2.60515, 0.0685989 } width 2.67375
 class      9 {  -3.42358, -0.363437 } width 3.06014
 class      2 {  0.578987, 3.86511 } width 3.28612
 class      3 {  0.973102, 4.55086 } width 3.57776
 class      7 {  -5.85311, -0.644533 } width 5.20858
 class      0 {   1.97633, 8.90835 } width 6.93202
 Some wide non-zero intervals were:
 class      2 {  0.578987, 3.86511 } width 3.28612
 class      3 {  0.973102, 4.55086 } width 3.57776
 class      7 {  -5.85311, -0.644533 } width 5.20858
 class      0 {   1.97633, 8.90835 } width 6.93202
 0 classes had vanishing intervals, with lower > upper.
 Projection 11 weights[ 784] 
 {l,u}:   0 {   2.17282,  9.40205} {  -3.48878,-0.226892} { -0.256245,  2.35608} {  0.289945,  3.16996} {  -1.43449, 0.951011} {    1.0969,  4.43717} { -0.452186,  2.05219} {  -2.67728, 0.144049}
 {l,u}:   8 {  0.430144,  3.59895} {  -2.05576, 0.488975} ...
 Some narrow non-zero intervals were:
 class      4 {  -1.43449, 0.951011 } width 2.3855
 class      6 { -0.452186, 2.05219 } width 2.50438
 class      9 {  -2.05576, 0.488975 } width 2.54473
 class      2 { -0.256245, 2.35608 } width 2.61232
 class      7 {  -2.67728, 0.144049 } width 2.82132
 class      3 {  0.289945, 3.16996 } width 2.88002
 class      8 {  0.430144, 3.59895 } width 3.16881
 class      1 {  -3.48878, -0.226892 } width 3.26189
 class      5 {    1.0969, 4.43717 } width 3.34027
 class      0 {   2.17282, 9.40205 } width 7.22923
 Some wide non-zero intervals were:
 class      8 {  0.430144, 3.59895 } width 3.16881
 class      1 {  -3.48878, -0.226892 } width 3.26189
 class      5 {    1.0969, 4.43717 } width 3.34027
 class      0 {   2.17282, 9.40205 } width 7.22923
 0 classes had vanishing intervals, with lower > upper.
 Projection 12 weights[ 784] 
 {l,u}:   0 {   2.10711,  10.0837} {  -4.30818,-0.468826} {  0.401078,   3.7723} { -0.736639,  1.88259} {  -1.53437, 0.937332} {  0.154908,  2.83204} {   1.42703,  5.24883} {  -2.96346,-0.0508585}
 {l,u}:   8 { -0.411561,  2.32085} {  -2.26411, 0.348243} ...
 Some narrow non-zero intervals were:
 class      4 {  -1.53437, 0.937332 } width 2.4717
 class      9 {  -2.26411, 0.348243 } width 2.61236
 class      3 { -0.736639, 1.88259 } width 2.61923
 class      5 {  0.154908, 2.83204 } width 2.67713
 class      8 { -0.411561, 2.32085 } width 2.73241
 class      7 {  -2.96346, -0.0508585 } width 2.91261
 class      2 {  0.401078, 3.7723 } width 3.37122
 class      6 {   1.42703, 5.24883 } width 3.8218
 class      1 {  -4.30818, -0.468826 } width 3.83935
 class      0 {   2.10711, 10.0837 } width 7.97662
 Some wide non-zero intervals were:
 class      2 {  0.401078, 3.7723 } width 3.37122
 class      6 {   1.42703, 5.24883 } width 3.8218
 class      1 {  -4.30818, -0.468826 } width 3.83935
 class      0 {   2.10711, 10.0837 } width 7.97662
 0 classes had vanishing intervals, with lower > upper.
 Projection 13 weights[ 784] 
 {l,u}:   0 {  -9.02825, -1.81385} {  -1.27419,  0.99348} {  -2.96802, 0.121181} {  -4.57326,-0.924465} {  0.605599,  3.61977} {   -3.3573,  -0.4515} { -0.941143,   1.5992} { 0.0186828,  2.83684}
 {l,u}:   8 {  -2.10231, 0.546455} {  0.801444,  5.72494} ...
 Some narrow non-zero intervals were:
 class      1 {  -1.27419, 0.99348 } width 2.26767
 class      6 { -0.941143, 1.5992 } width 2.54034
 class      8 {  -2.10231, 0.546455 } width 2.64877
 class      7 { 0.0186828, 2.83684 } width 2.81815
 class      5 {   -3.3573, -0.4515 } width 2.9058
 class      4 {  0.605599, 3.61977 } width 3.01417
 class      2 {  -2.96802, 0.121181 } width 3.0892
 class      3 {  -4.57326, -0.924465 } width 3.6488
 class      9 {  0.801444, 5.72494 } width 4.9235
 class      0 {  -9.02825, -1.81385 } width 7.2144
 Some wide non-zero intervals were:
 class      2 {  -2.96802, 0.121181 } width 3.0892
 class      3 {  -4.57326, -0.924465 } width 3.6488
 class      9 {  0.801444, 5.72494 } width 4.9235
 class      0 {  -9.02825, -1.81385 } width 7.2144
 0 classes had vanishing intervals, with lower > upper.
 Projection 14 weights[ 784] 
 {l,u}:   0 {   1.03454,  7.60019} {  -5.05961, -1.56416} {  -3.85716,-0.470917} {  -3.04612,-0.147604} { -0.237487,  2.47132} { -0.504859,  1.99085} {  0.529766,  3.98331} {  -2.17961,  0.42456}
 {l,u}:   8 {    -1.744, 0.839579} {  -1.13756,  1.25203} ...
 Some narrow non-zero intervals were:
 class      9 {  -1.13756, 1.25203 } width 2.3896
 class      5 { -0.504859, 1.99085 } width 2.4957
 class      8 {    -1.744, 0.839579 } width 2.58358
 class      7 {  -2.17961, 0.42456 } width 2.60417
 class      4 { -0.237487, 2.47132 } width 2.70881
 class      3 {  -3.04612, -0.147604 } width 2.89851
 class      2 {  -3.85716, -0.470917 } width 3.38625
 class      6 {  0.529766, 3.98331 } width 3.45354
 class      1 {  -5.05961, -1.56416 } width 3.49545
 class      0 {   1.03454, 7.60019 } width 6.56565
 Some wide non-zero intervals were:
 class      2 {  -3.85716, -0.470917 } width 3.38625
 class      6 {  0.529766, 3.98331 } width 3.45354
 class      1 {  -5.05961, -1.56416 } width 3.49545
 class      0 {   1.03454, 7.60019 } width 6.56565
 0 classes had vanishing intervals, with lower > upper.
 Projection 15 weights[ 784] 
 {l,u}:   0 {  -4.87433, -1.32328} {  -1.03391,  1.20819} {  -9.33976, -1.42728} {  -3.51396,-0.441276} { -0.0631128,  2.70185} {  -2.10761, 0.392321} {  -2.83507, -0.24885} {  0.724272,  6.12455}
 {l,u}:   8 {  -1.68155,    0.797} {  0.405129,  3.45183} ...
 Some narrow non-zero intervals were:
 class      1 {  -1.03391, 1.20819 } width 2.2421
 class      8 {  -1.68155, 0.797 } width 2.47855
 class      5 {  -2.10761, 0.392321 } width 2.49993
 class      6 {  -2.83507, -0.24885 } width 2.58622
 class      4 {-0.0631128, 2.70185 } width 2.76496
 class      9 {  0.405129, 3.45183 } width 3.0467
 class      3 {  -3.51396, -0.441276 } width 3.07269
 class      0 {  -4.87433, -1.32328 } width 3.55105
 class      7 {  0.724272, 6.12455 } width 5.40028
 class      2 {  -9.33976, -1.42728 } width 7.91248
 Some wide non-zero intervals were:
 class      3 {  -3.51396, -0.441276 } width 3.07269
 class      0 {  -4.87433, -1.32328 } width 3.55105
 class      7 {  0.724272, 6.12455 } width 5.40028
 class      2 {  -9.33976, -1.42728 } width 7.91248
 0 classes had vanishing intervals, with lower > upper.
 Projection 16 weights[ 784] 
 {l,u}:   0 {  -4.92328, -1.36514} {  -1.05432,  1.18639} {  -9.35244, -1.46479} {  -3.55988,-0.488178} { -0.106592,  2.64505} {  -2.14616, 0.349358} {  -2.86568,-0.276312} {  0.680608,  6.04897}
 {l,u}:   8 {  -1.73043, 0.751606} {  0.359128,  3.39304} ...
 Some narrow non-zero intervals were:
 class      1 {  -1.05432, 1.18639 } width 2.24071
 class      8 {  -1.73043, 0.751606 } width 2.48204
 class      5 {  -2.14616, 0.349358 } width 2.49552
 class      6 {  -2.86568, -0.276312 } width 2.58937
 class      4 { -0.106592, 2.64505 } width 2.75165
 class      9 {  0.359128, 3.39304 } width 3.03392
 class      3 {  -3.55988, -0.488178 } width 3.07171
 class      0 {  -4.92328, -1.36514 } width 3.55814
 class      7 {  0.680608, 6.04897 } width 5.36837
 class      2 {  -9.35244, -1.46479 } width 7.88764
 Some wide non-zero intervals were:
 class      3 {  -3.55988, -0.488178 } width 3.07171
 class      0 {  -4.92328, -1.36514 } width 3.55814
 class      7 {  0.680608, 6.04897 } width 5.36837
 class      2 {  -9.35244, -1.46479 } width 7.88764
 0 classes had vanishing intervals, with lower > upper.
 Projection 17 weights[ 784] 
 {l,u}:   0 {  -9.55898,  -2.1627} {  0.278169,  3.49798} {  -2.35189, 0.276791} {  -3.71481,-0.574007} { -0.900723,  1.50012} {  -4.43286, -1.06401} {  -1.99107, 0.505279} { -0.123258,  2.72195}
 {l,u}:   8 {  -2.97599,-0.0661903} { -0.438693,  2.12194} ...
 Some narrow non-zero intervals were:
 class      4 { -0.900723, 1.50012 } width 2.40084
 class      6 {  -1.99107, 0.505279 } width 2.49635
 class      9 { -0.438693, 2.12194 } width 2.56063
 class      2 {  -2.35189, 0.276791 } width 2.62868
 class      7 { -0.123258, 2.72195 } width 2.84521
 class      8 {  -2.97599, -0.0661903 } width 2.9098
 class      3 {  -3.71481, -0.574007 } width 3.1408
 class      1 {  0.278169, 3.49798 } width 3.21981
 class      5 {  -4.43286, -1.06401 } width 3.36885
 class      0 {  -9.55898, -2.1627 } width 7.39628
 Some wide non-zero intervals were:
 class      3 {  -3.71481, -0.574007 } width 3.1408
 class      1 {  0.278169, 3.49798 } width 3.21981
 class      5 {  -4.43286, -1.06401 } width 3.36885
 class      0 {  -9.55898, -2.1627 } width 7.39628
 0 classes had vanishing intervals, with lower > upper.
 Projection 18 weights[ 784] 
 {l,u}:   0 {  -9.91276, -2.05997} {  0.538434,   4.2062} {  -3.75087,-0.340066} {  -2.21324, 0.473707} { -0.844053,  1.63138} {  -2.75433,-0.089717} {  -5.09773, -1.31933} { 0.0764553,  3.01924}
 {l,u}:   8 {  -1.83312, 0.789854} { -0.256256,  2.34426} ...
 Some narrow non-zero intervals were:
 class      4 { -0.844053, 1.63138 } width 2.47543
 class      9 { -0.256256, 2.34426 } width 2.60052
 class      8 {  -1.83312, 0.789854 } width 2.62297
 class      5 {  -2.75433, -0.089717 } width 2.66461
 class      3 {  -2.21324, 0.473707 } width 2.68695
 class      7 { 0.0764553, 3.01924 } width 2.94279
 class      2 {  -3.75087, -0.340066 } width 3.41081
 class      1 {  0.538434, 4.2062 } width 3.66777
 class      6 {  -5.09773, -1.31933 } width 3.7784
 class      0 {  -9.91276, -2.05997 } width 7.85279
 Some wide non-zero intervals were:
 class      2 {  -3.75087, -0.340066 } width 3.41081
 class      1 {  0.538434, 4.2062 } width 3.66777
 class      6 {  -5.09773, -1.31933 } width 3.7784
 class      0 {  -9.91276, -2.05997 } width 7.85279
 0 classes had vanishing intervals, with lower > upper.
 Projection 19 weights[ 784] 
 {l,u}:   0 {   1.52771,  5.58206} {  -4.29189,-0.425135} {  0.464131,  3.82977} { -0.776108,  1.80625} {  -1.38892,  1.03561} {  0.110473,  2.73627} {   1.90536,   9.0167} {  -2.94426,-0.0784122}
 {l,u}:   8 { -0.397857,  2.28027} {  -2.17136, 0.390835} ...
 Some narrow non-zero intervals were:
 class      4 {  -1.38892, 1.03561 } width 2.42453
 class      9 {  -2.17136, 0.390835 } width 2.56219
 class      3 { -0.776108, 1.80625 } width 2.58235
 class      5 {  0.110473, 2.73627 } width 2.62579
 class      8 { -0.397857, 2.28027 } width 2.67813
 class      7 {  -2.94426, -0.0784122 } width 2.86585
 class      2 {  0.464131, 3.82977 } width 3.36563
 class      1 {  -4.29189, -0.425135 } width 3.86675
 class      0 {   1.52771, 5.58206 } width 4.05435
 class      6 {   1.90536, 9.0167 } width 7.11134
 Some wide non-zero intervals were:
 class      2 {  0.464131, 3.82977 } width 3.36563
 class      1 {  -4.29189, -0.425135 } width 3.86675
 class      0 {   1.52771, 5.58206 } width 4.05435
 class      6 {   1.90536, 9.0167 } width 7.11134
 0 classes had vanishing intervals, with lower > upper.
 Projection 20 weights[ 784] 
 {l,u}:   0 {   2.09285,  10.0653} {  -4.32668,-0.478278} {  0.385689,  3.75705} { -0.749682,  1.86693} {  -1.55869, 0.915941} {  0.136638,  2.81181} {    1.4076,  5.21748} {  -2.98355,-0.0658088}
 {l,u}:   8 { -0.432955,  2.29487} {  -2.28386, 0.327157} ...
 Some narrow non-zero intervals were:
 class      4 {  -1.55869, 0.915941 } width 2.47463
 class      9 {  -2.28386, 0.327157 } width 2.61102
 class      3 { -0.749682, 1.86693 } width 2.61661
 class      5 {  0.136638, 2.81181 } width 2.67517
 class      8 { -0.432955, 2.29487 } width 2.72782
 class      7 {  -2.98355, -0.0658088 } width 2.91774
 class      2 {  0.385689, 3.75705 } width 3.37136
 class      6 {    1.4076, 5.21748 } width 3.80989
 class      1 {  -4.32668, -0.478278 } width 3.8484
 class      0 {   2.09285, 10.0653 } width 7.97242
 Some wide non-zero intervals were:
 class      2 {  0.385689, 3.75705 } width 3.37136
 class      6 {    1.4076, 5.21748 } width 3.80989
 class      1 {  -4.32668, -0.478278 } width 3.8484
 class      0 {   2.09285, 10.0653 } width 7.97242
 0 classes had vanishing intervals, with lower > upper.
 Projection 21 weights[ 784] 
 {l,u}:   0 {   -10.034, -2.07783} {  0.496752,  4.32674} {  -3.73755,-0.376288} {  -1.84922, 0.764871} { -0.907808,  1.56104} {  -2.79608,-0.129115} {  -5.19778, -1.39311} { 0.0662019,  2.97829}
 {l,u}:   8 {  -2.28045,  0.44399} { -0.322458,  2.28573} ...
 Some narrow non-zero intervals were:
 class      4 { -0.907808, 1.56104 } width 2.46884
 class      9 { -0.322458, 2.28573 } width 2.60819
 class      3 {  -1.84922, 0.764871 } width 2.61409
 class      5 {  -2.79608, -0.129115 } width 2.66697
 class      8 {  -2.28045, 0.44399 } width 2.72444
 class      7 { 0.0662019, 2.97829 } width 2.91209
 class      2 {  -3.73755, -0.376288 } width 3.36126
 class      6 {  -5.19778, -1.39311 } width 3.80466
 class      1 {  0.496752, 4.32674 } width 3.82999
 class      0 {   -10.034, -2.07783 } width 7.95621
 Some wide non-zero intervals were:
 class      2 {  -3.73755, -0.376288 } width 3.36126
 class      6 {  -5.19778, -1.39311 } width 3.80466
 class      1 {  0.496752, 4.32674 } width 3.82999
 class      0 {   -10.034, -2.07783 } width 7.95621
 0 classes had vanishing intervals, with lower > upper.
 Projection 22 weights[ 784] 
 {l,u}:   0 {  -9.43897,  -2.1949} {  0.226221,  3.50068} {   -2.3786,   0.2411} {   -3.1871,-0.302095} { -0.965931,  1.42101} {  -4.46167, -1.11444} {  -2.07093, 0.435138} { -0.158965,  2.66168}
 {l,u}:   8 {  -3.62331,-0.444479} { -0.508682,  2.03133} ...
 Some narrow non-zero intervals were:
 class      4 { -0.965931, 1.42101 } width 2.38694
 class      6 {  -2.07093, 0.435138 } width 2.50607
 class      9 { -0.508682, 2.03133 } width 2.54001
 class      2 {   -2.3786, 0.2411 } width 2.6197
 class      7 { -0.158965, 2.66168 } width 2.82065
 class      3 {   -3.1871, -0.302095 } width 2.885
 class      8 {  -3.62331, -0.444479 } width 3.17883
 class      1 {  0.226221, 3.50068 } width 3.27446
 class      5 {  -4.46167, -1.11444 } width 3.34723
 class      0 {  -9.43897, -2.1949 } width 7.24407
 Some wide non-zero intervals were:
 class      8 {  -3.62331, -0.444479 } width 3.17883
 class      1 {  0.226221, 3.50068 } width 3.27446
 class      5 {  -4.46167, -1.11444 } width 3.34723
 class      0 {  -9.43897, -2.1949 } width 7.24407
 0 classes had vanishing intervals, with lower > upper.
 Projection 23 weights[ 784] 
 {l,u}:   0 {   2.06719,  10.0249} {  -4.33883,-0.508591} {  0.355266,  3.70809} { -0.780787,  1.83195} {  -1.57027, 0.898364} {  0.117232,  2.78746} {   1.37541,   5.1842} {  -2.99831,-0.0834792}
 {l,u}:   8 { -0.460993,  2.26135} {  -2.30309, 0.307596} ...
 Some narrow non-zero intervals were:
 class      4 {  -1.57027, 0.898364 } width 2.46864
 class      9 {  -2.30309, 0.307596 } width 2.61069
 class      3 { -0.780787, 1.83195 } width 2.61274
 class      5 {  0.117232, 2.78746 } width 2.67023
 class      8 { -0.460993, 2.26135 } width 2.72234
 class      7 {  -2.99831, -0.0834792 } width 2.91483
 class      2 {  0.355266, 3.70809 } width 3.35283
 class      6 {   1.37541, 5.1842 } width 3.80879
 class      1 {  -4.33883, -0.508591 } width 3.83024
 class      0 {   2.06719, 10.0249 } width 7.95768
 Some wide non-zero intervals were:
 class      2 {  0.355266, 3.70809 } width 3.35283
 class      6 {   1.37541, 5.1842 } width 3.80879
 class      1 {  -4.33883, -0.508591 } width 3.83024
 class      0 {   2.06719, 10.0249 } width 7.95768
 0 classes had vanishing intervals, with lower > upper.
 Projection 24 weights[ 784] 
 {l,u}:   0 {  -10.3317, -2.39138} { 0.0367472,  3.15197} {  -4.07162,-0.726085} {  -4.78422, -1.07865} { -0.243746,  2.54911} {  -3.21745,-0.477705} {  -2.06787, 0.403106} {   -1.1504,  1.28066}
 {l,u}:   8 {  -2.51227, 0.123232} { -0.525766,  1.95864} ...
 Some narrow non-zero intervals were:
 class      7 {   -1.1504, 1.28066 } width 2.43107
 class      6 {  -2.06787, 0.403106 } width 2.47098
 class      9 { -0.525766, 1.95864 } width 2.4844
 class      8 {  -2.51227, 0.123232 } width 2.6355
 class      5 {  -3.21745, -0.477705 } width 2.73975
 class      4 { -0.243746, 2.54911 } width 2.79285
 class      1 { 0.0367472, 3.15197 } width 3.11522
 class      2 {  -4.07162, -0.726085 } width 3.34554
 class      3 {  -4.78422, -1.07865 } width 3.70557
 class      0 {  -10.3317, -2.39138 } width 7.94031
 Some wide non-zero intervals were:
 class      1 { 0.0367472, 3.15197 } width 3.11522
 class      2 {  -4.07162, -0.726085 } width 3.34554
 class      3 {  -4.78422, -1.07865 } width 3.70557
 class      0 {  -10.3317, -2.39138 } width 7.94031
 0 classes had vanishing intervals, with lower > upper.
 Projection 25 weights[ 784] 
 {l,u}:   0 {    2.1333,  9.79208} {  -1.16552,  1.13188} {  0.725881,    4.161} {   1.06184,  4.83897} {  -5.20678,-0.675851} {  0.255555,  3.00637} { -0.821092,  1.71424} {  -2.49534, 0.297676}
 {l,u}:   8 { -0.373315,  2.28544} {  -3.34112,-0.369885} ...
 Some narrow non-zero intervals were:
 class      1 {  -1.16552, 1.13188 } width 2.2974
 class      6 { -0.821092, 1.71424 } width 2.53534
 class      8 { -0.373315, 2.28544 } width 2.65876
 class      5 {  0.255555, 3.00637 } width 2.75082
 class      7 {  -2.49534, 0.297676 } width 2.79301
 class      9 {  -3.34112, -0.369885 } width 2.97124
 class      2 {  0.725881, 4.161 } width 3.43512
 class      3 {   1.06184, 4.83897 } width 3.77713
 class      4 {  -5.20678, -0.675851 } width 4.53093
 class      0 {    2.1333, 9.79208 } width 7.65878
 Some wide non-zero intervals were:
 class      2 {  0.725881, 4.161 } width 3.43512
 class      3 {   1.06184, 4.83897 } width 3.77713
 class      4 {  -5.20678, -0.675851 } width 4.53093
 class      0 {    2.1333, 9.79208 } width 7.65878
 0 classes had vanishing intervals, with lower > upper.
 Projection 26 weights[ 784] 
 {l,u}:   0 {  -0.75761,  1.93157} {  -4.99659, -1.50682} {  -3.12325,-0.0506734} {  -3.95918,-0.678607} {  0.443754,  3.42095} {  -1.66661, 0.776523} {  0.933709,  6.47051} {  -1.08473,  1.43676}
 {l,u}:   8 {  -2.34029, 0.276709} { -0.188468,  2.32856} ...
 Some narrow non-zero intervals were:
 class      5 {  -1.66661, 0.776523 } width 2.44313
 class      9 { -0.188468, 2.32856 } width 2.51703
 class      7 {  -1.08473, 1.43676 } width 2.52149
 class      8 {  -2.34029, 0.276709 } width 2.617
 class      0 {  -0.75761, 1.93157 } width 2.68918
 class      4 {  0.443754, 3.42095 } width 2.9772
 class      2 {  -3.12325, -0.0506734 } width 3.07257
 class      3 {  -3.95918, -0.678607 } width 3.28057
 class      1 {  -4.99659, -1.50682 } width 3.48977
 class      6 {  0.933709, 6.47051 } width 5.5368
 Some wide non-zero intervals were:
 class      2 {  -3.12325, -0.0506734 } width 3.07257
 class      3 {  -3.95918, -0.678607 } width 3.28057
 class      1 {  -4.99659, -1.50682 } width 3.48977
 class      6 {  0.933709, 6.47051 } width 5.5368
 0 classes had vanishing intervals, with lower > upper.
 Projection 27 weights[ 784] 
 {l,u}:   0 {  -8.85574, -1.95399} {  -1.02056,  1.23873} {  -3.84531, -0.56114} {  -4.51683,-0.945337} { -0.0457732,  2.62448} {  -2.89503,-0.231993} {  -1.73045, 0.766105} {  0.669125,  5.89088}
 {l,u}:   8 {  -2.19894, 0.416849} {  0.392209,  3.45254} ...
 Some narrow non-zero intervals were:
 class      1 {  -1.02056, 1.23873 } width 2.25929
 class      6 {  -1.73045, 0.766105 } width 2.49656
 class      8 {  -2.19894, 0.416849 } width 2.61578
 class      5 {  -2.89503, -0.231993 } width 2.66304
 class      4 {-0.0457732, 2.62448 } width 2.67026
 class      9 {  0.392209, 3.45254 } width 3.06033
 class      2 {  -3.84531, -0.56114 } width 3.28417
 class      3 {  -4.51683, -0.945337 } width 3.57149
 class      7 {  0.669125, 5.89088 } width 5.22175
 class      0 {  -8.85574, -1.95399 } width 6.90175
 Some wide non-zero intervals were:
 class      2 {  -3.84531, -0.56114 } width 3.28417
 class      3 {  -4.51683, -0.945337 } width 3.57149
 class      7 {  0.669125, 5.89088 } width 5.22175
 class      0 {  -8.85574, -1.95399 } width 6.90175
 0 classes had vanishing intervals, with lower > upper.
 Projection 28 weights[ 784] 
 {l,u}:   0 {   2.41189,  10.3816} {  -3.14026,-0.0284407} {  0.732299,  4.08243} {   1.09053,  4.80356} {   -2.5482, 0.248193} {  0.486806,  3.23464} { -0.384216,  2.09363} {  -1.28422,  1.15145}
 {l,u}:   8 { -0.111963,   2.5291} {  -1.95849, 0.531061} ...
 Some narrow non-zero intervals were:
 class      7 {  -1.28422, 1.15145 } width 2.43567
 class      6 { -0.384216, 2.09363 } width 2.47784
 class      9 {  -1.95849, 0.531061 } width 2.48955
 class      8 { -0.111963, 2.5291 } width 2.64106
 class      5 {  0.486806, 3.23464 } width 2.74784
 class      4 {   -2.5482, 0.248193 } width 2.79639
 class      1 {  -3.14026, -0.0284407 } width 3.11182
 class      2 {  0.732299, 4.08243 } width 3.35013
 class      3 {   1.09053, 4.80356 } width 3.71302
 class      0 {   2.41189, 10.3816 } width 7.9697
 Some wide non-zero intervals were:
 class      1 {  -3.14026, -0.0284407 } width 3.11182
 class      2 {  0.732299, 4.08243 } width 3.35013
 class      3 {   1.09053, 4.80356 } width 3.71302
 class      0 {   2.41189, 10.3816 } width 7.9697
 0 classes had vanishing intervals, with lower > upper.
 Projection 29 weights[ 784] 
 {l,u}:   0 {  -9.53493, -2.12526} {  0.247089,  3.72593} {   -2.4397, 0.291696} {  -4.75871,  -1.0274} { -0.574051,  1.94301} {  -3.83179,-0.761353} { -0.100181,  3.02729} {   -1.7301, 0.641845}
 {l,u}:   8 {  -2.91614,-0.0399285} {  -1.00314,  1.35411} ...
 Some narrow non-zero intervals were:
 class      9 {  -1.00314, 1.35411 } width 2.35725
 class      7 {   -1.7301, 0.641845 } width 2.37194
 class      4 { -0.574051, 1.94301 } width 2.51706
 class      2 {   -2.4397, 0.291696 } width 2.7314
 class      8 {  -2.91614, -0.0399285 } width 2.87621
 class      5 {  -3.83179, -0.761353 } width 3.07043
 class      6 { -0.100181, 3.02729 } width 3.12747
 class      1 {  0.247089, 3.72593 } width 3.47884
 class      3 {  -4.75871, -1.0274 } width 3.73131
 class      0 {  -9.53493, -2.12526 } width 7.40967
 Some wide non-zero intervals were:
 class      6 { -0.100181, 3.02729 } width 3.12747
 class      1 {  0.247089, 3.72593 } width 3.47884
 class      3 {  -4.75871, -1.0274 } width 3.73131
 class      0 {  -9.53493, -2.12526 } width 7.40967
 0 classes had vanishing intervals, with lower > upper.

 long run solved in 2146.4724514484 seconds


	
 parse( argc=5, argv, ... )
    argv[0] = foo
    argv[1] = --xfile=x-mnist.bin
    argv[2] = --yfile=y-mnist.bin
    argv[3] = --solnfile=mnist2d.soln
    argv[4] = --output=mnist2d.proj
mcproj args...
 +MCprojProgram --xfile=x-mnist.bin --yFile=y-mnist.bin --solnFile=mnist2d.soln --output=mnist2d.proj -T -D --yfile=y-mnist.bin
 MCprojProgram::tryRead() oops ... io_bin: read failure
Retrying xFile as SparseM...
 r,c,nData = 60000, 10, 60000
 u16 oindex... u8 iindex... setting 'true' eigen_io_binbool input OK
 MCprojProgram::tryProj() sparse
project(x,MCsoln): x[60000x784], w[784x30], projections[30x60000]
 feasible[60000] classes, after project(xDense,soln)
 MCprojProgram::trySave()	Saving feasible Text Dense classes --> mnist2d.proj
Overall confusion matrix: tp=   39240 fp=   17445
                          tn=  522555 fn=   20760
 Average true predictions per example = 0
 McprojProgram::tryValidate() against --yfile y-mnist.bin
Overall confusion matrix: tp=   39240 fp=   17445
                          tn=  522555 fn=   20760
 Average labels      per example = 1
 Average predictions per example = 0.94475
 Precision (tp/tp+fp)            = 0.692247
 Recall    (tp/tp+fn)            = 1.02709
 F1        (2tp/(2tp+fp+fn)      = 0.67258

 long run projected in 0.18434190750122 seconds


	
	Command being timed: "lua_cpp solve-mnist2.lua"
	User time (seconds): 17022.33
	System time (seconds): 151.40
	Percent of CPU this job got: 798%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 35:50.19
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 638108
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 12061
	Voluntary context switches: 10813
	Involuntary context switches: 17455
	Swaps: 0
	File system inputs: 0
	File system outputs: 5680
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
