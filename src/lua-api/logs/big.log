|| loaded module 'milde_gui'	
 Function:   Solve for Multi-Class Label Filter projection lines
    Determine a number (--proj) of projection lines. Any example whose projection
    lies outside bounds for that class is 'filtered'.  With many projecting lines,
    potentially many classes can be filtered out, leaving few potential classes
    for each example.
 Usage:
    <mcsolve> --xfile=... --yfile=... [--solnfile=...] [--output=...] [ARGS...]
 where ARGS guide the optimization procedure
 - Without solnfile[.soln], use random initial conditions.
 - xfile is a plain eigen DenseM or SparseM (always stored as float)
 - yfile is an Eigen SparseMb matrix of bool storing only the true values,
         read/written via 'eigen_io_binbool'
Allowed options:

mcsolve options:
  -x [ --xfile ] arg                    x data (row-wise nExamples x dim)
  -y [ --yfile ] arg                    y data (if absent, try reading as 
                                        libsvm format)
  -s [ --solnfile ] arg                 solnfile[.soln] starting solver state
  -o [ --output ] arg (=mc)             output[.soln] file base name
  -B [=arg(=1)] (=1)                    B|T output BINARY
  -T [=arg(=1)] (=0)                    B|T output TEXT
  -S [=arg(=1)] (=1)                    S|L output SHORT
  -L [=arg(=1)] (=0)                    S|L output LONG
  --xnorm [=arg(=1)] (=0)               col-normalize x dimensions 
                                        (mean=stdev=1)
                                        (forces Dense x)
  -t [ --threads ] arg (=1)             TBD: threads
  -v [ --verbose ] [=arg(=1)] (=0)      --verbosity=-1 may reduce output

solver args:
  -h [ --help ] 
  --proj arg (=5)                       # of projections
  --C1 arg (=10)                        ~ label in correct [l,u]
  --C2 arg (=1)                         ~ label outside other [l,u]
  --maxiter arg (=1000000)              max iterations per projection
  -b [ --batchsize ] arg (=100)         batch size
  -u [ --update ] arg (=MINIBATCH_SGD)  BATCH | SAFE : gradient update type
  --eps arg (=0.0001)                   unused cvgce threshold
  --eta0 arg (=0.10000000000000001)     initial learning rate
  --etatype arg (=ETA_LIN)              CONST | SQRT | LIN | 3_4 : learning 
                                        rate schedule
  --etamin arg (=0)                     learning rate limit
  --optlu arg (=10000)                  expensive exact {l,u} soln period
  --treorder arg (=1000)                reorder iteration period
  --reorder arg (=REORDER_AVG_PROJ_MEANS)
                                        Permutation re-ordering: AVG projected 
                                        means | PROJ projected means | MID 
                                        range midpoints. If --avg=0, default is
                                        PROJ
  --treport arg (=1000)                 period for reports about latest iter
  --avg arg (=0)                        averaging start iteration
  --tavg arg (=0)                       period for reports about avg, expensive
  --reweight arg (=REWEIGHT_LAMBDA)     NONE | LAMBDA | ALL lambda reweighting 
                                        method
  --wt_by_nclasses [=arg(=1)] (=0)      ?
  --wt_class_by_nclasses [=arg(=1)] (=0)
                                        ?
  --negclass arg (=0)                   # of negative classes used at each 
                                        iter, 0 ~ all classes
  --remove_constraints [=arg(=1)] (=0)  after each projection, remove 
                                        constraints
  --remove_class [=arg(=1)] (=0)        after each projection, remove 
                                        already-separated classes(?)
  --threads arg (=0)                    # threads, 0 ~ use OMP_NUM_THREADS
  --seed arg (=0)                       random number seed
  --tgrad arg (=0)                      iter period for finite difference 
                                        gradient test
  --ngrad arg (=1000)                   directions per gradient test
  --grad arg (=0.0001)                  step size per gradient test
  --resume [=arg(=1)] (=0)              resume an existing soln?
  --reoptlu [=arg(=1)] (=0)             reoptimize {l,u} bounds of existing 
                                        soln?

lua constructors:
    - <mcsolve>.new("--xfile=... --yfile=... --solnfile=... etc")
    - <mcsolve>.new(<explicit_defaults:xparm>, "--xfile=... etc")
 Program default settings can come from:
   1. lua <explicit_defaults:xparm>
   2. else parameters from previous --solnfile
   3. else library default settings
	
 parse( argc=10, argv, ... )
    argv[0] = foo
    argv[1] = --xfile=../../data/temp.test.svm
    argv[2] = --output=big.soln
    argv[3] = -B
    argv[4] = -S
    argv[5] = --C1=-1
    argv[6] = --maxiter=1000000
    argv[7] = --optlu=5000
    argv[8] = --treport=100000
    argv[9] = --proj=5
msolve args...
opt::extract...
 reparse cmdline, this time with .soln parms as defaults
 parse( argc=10, argv, ... )
    argv[0] = foo
    argv[1] = --xfile=../../data/temp.test.svm
    argv[2] = --output=big.soln
    argv[3] = -B
    argv[4] = -S
    argv[5] = --C1=-1
    argv[6] = --maxiter=1000000
    argv[7] = --optlu=5000
    argv[8] = --treport=100000
    argv[9] = --proj=5
msolve args...
opt::extract...
MCsolveProgram::tryRead()
 GOOD libsvm-like text input, minClass=1 maxClass=451 minXidx=1 maxXidx=1123493 row=28926
Assuming 1-based y classes, subtracting minClass=1 from all class labels
	 y.setFromTriplets...
Assuming 1-based x indices: minXidx = 1, not zero
	 x.setFromTriplets...
MCsolveProgram::trySolve() sparse
 mcsolver.hh, solve_optimization: nProj: 5
size x: 28926 rows and 1123493 columns.
size y: 28926 rows and 451 columns.
 +MCpermState(nClass=451)
 solve_ with _OPENMP and params.num_threads set to 1, nThreads is 1, and omp_max_threads is now 1
 batch_size = 100
 idx_chunks=1
 initial lambda=170.076 C1=170.076 C2=0.00587971
  ... begin with weights[0x0] weights_avg[0x0] lower_bounds_avg[0x0] upper_bounds_avg[0x0]
  ... starting with     weights[0x0]:

  ... starting with weights_avg[0x0]:

  ... beginning at projection_dim=0 reuse_dim=0
 init_w : weights.cols()=5 projection_dim=0 random + vector-between-2-classes orthogonalized
 start projection 0 w.norm=1
 MCpermState::init(projection[28926],y,nc) nClasses=451)
projection_dim: 0, batch_size: 100, noClasses: 451, C1: 170.076, C2: 0.00587971, lambda: 170.076, size w: 1123493
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:          153079 0.00303905
objective_val[200000]:          153077 0.00275563
objective_val[300000]:          153076 0.00262055
objective_val[400000]:          153076 0.00253051
objective_val[500000]:          153075 0.00246471
objective_val[600000]:          153075 0.0024178
objective_val[700000]:          153075 0.00238339
objective_val[800000]:          153074 0.0023546
objective_val[900000]:          153074 0.00233185
objective_val[1000000]:          153074 0.00231285
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[1000000]: 153074 0.00231285 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=5 projection_dim=1 random + vector-between-2-classes orthogonalized
 start projection 1 w.norm=1
 MCpermState::init(projection[28926],y,nc) nClasses=451)
projection_dim: 1, batch_size: 100, noClasses: 451, C1: 170.076, C2: 0.00587971, lambda: 170.076, size w: 1123493
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:          153094 0.00473128
objective_val[200000]:          153085 0.00373793
objective_val[300000]:          153081 0.00332458
objective_val[400000]:          153079 0.00310855
objective_val[500000]:          153078 0.00298098
objective_val[600000]:          153077 0.00289748
objective_val[700000]:          153077 0.00283851
objective_val[800000]:          153077 0.00279061
objective_val[900000]:          153076 0.00275476
objective_val[1000000]:          153076 0.00272414
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[1000000]: 153076 0.00272414 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=5 projection_dim=2 random + vector-between-2-classes orthogonalized
 start projection 2 w.norm=1
 MCpermState::init(projection[28926],y,nc) nClasses=451)
projection_dim: 2, batch_size: 100, noClasses: 451, C1: 170.076, C2: 0.00587971, lambda: 170.076, size w: 1123493
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:          153069 0.00141325
objective_val[200000]:          153068 0.0013803
objective_val[300000]:          153068 0.00136777
objective_val[400000]:          153068 0.00136087
objective_val[500000]:          153068 0.0013562
objective_val[600000]:          153067 0.00135253
objective_val[700000]:          153067 0.00134975
objective_val[800000]:          153067 0.00134741
objective_val[900000]:          153067 0.00134547
objective_val[1000000]:          153067 0.00134375
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[1000000]: 153067 0.00134375 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=5 projection_dim=3 random + vector-between-2-classes orthogonalized
 start projection 3 w.norm=1
 MCpermState::init(projection[28926],y,nc) nClasses=451)
projection_dim: 3, batch_size: 100, noClasses: 451, C1: 170.076, C2: 0.00587971, lambda: 170.076, size w: 1123493
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:          153085 0.00356467
objective_val[200000]:          153082 0.00323596
objective_val[300000]:          153080 0.00303414
objective_val[400000]:          153079 0.00290665
objective_val[500000]:          153078 0.00281603
objective_val[600000]:          153077 0.00274703
objective_val[700000]:          153077 0.0026961
objective_val[800000]:          153076 0.00265668
objective_val[900000]:          153076 0.00262542
objective_val[1000000]:          153076 0.00260031
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[1000000]: 153076 0.00260031 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 init_w : weights.cols()=5 projection_dim=4 random + vector-between-2-classes orthogonalized
 start projection 4 w.norm=1
 MCpermState::init(projection[28926],y,nc) nClasses=451)
projection_dim: 4, batch_size: 100, noClasses: 451, C1: 170.076, C2: 0.00587971, lambda: 170.076, size w: 1123493
-----------------------------
objective_val[  t   ]: value    w.norm (initially 1)
--------------------- -------  -------
objective_val[100000]:          153074 0.00238271
objective_val[200000]:          153073 0.00232924
objective_val[300000]:          153073 0.00230069
objective_val[400000]:          153073 0.00228137
objective_val[500000]:          153073 0.00226709
objective_val[600000]:          153073 0.00225578
objective_val[700000]:          153073 0.00224634
objective_val[800000]:          153073 0.00223834
objective_val[900000]:          153073 0.00223145
objective_val[1000000]:          153072 0.00222535
 * end iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0
objective_val[1000000]: 153072 0.00222535 * END iterations: * luPerm ok_lu=1 ok_sortlu=1 * luPerm ok_lu_avg=0 ok_sortlu_avg=1 nAccSortlu_avg=0

 ~Proj{ngetSTD=6080,ngetAVG=0,nReuse=1075,nSwitch=0,nDemote=0}
MCsolveProgram::trySave()
 Writing MCsoln initially from .soln to big.soln
	mcdumpsoln -p < big.soln | less    # to prettyprint the soln
MCsolveProgram::tryRead()
 weights     norms:  0.00231285 0.00272414 0.00134375 0.00260031 0.00222535
 weights_avg norms:  0.00231285 0.00272414 0.00134375 0.00260031 0.00222535
normalized     weights[1123493x5]:
normalized weights_avg[1123493x5]:
      lower_bounds_avg[451x5]:
      upper_bounds_avg[451x5]:
 Projection 0 weights[ 1123493] 
 {l,u}:   0 {  -1.00079, 0.999662} {   -1.0011, 0.999483} {  -1.00107, 0.999523} {  -1.00086, 0.999527} {  -1.00088, 0.999447} {  -1.00093, 0.999433} {  -1.00106, 0.999491} {  -1.00101, 0.999498}
 {l,u}:   8 {  -1.00087, 0.999417} {  -1.00099, 0.999466} {  -1.00094,  0.99946} {  -1.00088, 0.999592} {  -1.00092, 0.999393} {  -1.00103, 0.999625} {  -1.00106, 0.999575} {  -1.00101, 0.999619}
 {l,u}:  16 {  -1.00088, 0.999491} {  -1.00092, 0.999539} {  -1.00101, 0.999533} {  -1.00083, 0.999439} {  -1.00084, 0.999504} {   -1.0007, 0.999678} {  -1.00082, 0.999781} {   -1.0006, 0.999717}
 {l,u}:  24 {  -1.00079, 0.999736} {  -1.00069, 0.999731} {  -1.00096, 0.999828} {  -1.00077, 0.999646} {   -1.0009, 0.999678} {  -1.00083,  0.99969} {  -1.00093, 0.999598} {  -1.00089, 0.999639}
 {l,u}:  32 {  -1.00098, 0.999621} {  -1.00081, 0.999642} {  -1.00096, 0.999596} {   -1.0009,  0.99947} {  -1.00104,  0.99981} {  -1.00105,  0.99992} {  -1.00092, 0.999634} {  -1.00087, 0.999502}
 {l,u}:  40 {  -1.00084, 0.999616} {  -1.00088, 0.999667} {  -1.00103, 0.999595} {  -1.00082, 0.999478} {  -1.00078, 0.999568} {  -1.00091, 0.999791} {  -1.00085, 0.999859} {  -1.00072, 0.999769}
 {l,u}:  48 {  -1.00092, 0.999699} {  -1.00074, 0.999718} {  -1.00089, 0.999807} {  -1.00102, 0.999775} {  -1.00091, 0.999714} {   -1.0009, 0.999821} {  -1.00068, 0.999756} {  -1.00101, 0.999579}
 {l,u}:  56 {  -1.00077, 0.999665} {  -1.00061, 0.999698} {  -1.00104, 0.999345} {  -1.00083, 0.999525} {  -1.00089, 0.999535} {  -1.00086, 0.999504} {  -1.00088, 0.999482} {  -1.00091, 0.999814} ...
 Some narrow non-zero intervals were:
 class    114 {  -1.00082, 0.999328 } width 2.00015
 class    230 {   -1.0008, 0.999359 } width 2.00016
 class    298 {  -1.00055, 0.999615 } width 2.00017
 class     98 {   -1.0008, 0.999378 } width 2.00018
 class    362 {  -1.00082, 0.999362 } width 2.00019
 class    356 {  -1.00075, 0.999449 } width 2.0002
 class    406 {   -1.0007, 0.999495 } width 2.0002
 class    214 {   -1.0008, 0.999411 } width 2.00021
 class     85 {  -1.00081, 0.999406 } width 2.00021
 class    294 {  -1.00056, 0.999661 } width 2.00022
 Some wide non-zero intervals were:
 class    336 {  -1.00106, 0.999839 } width 2.0009
 class    321 {  -1.00099, 0.999937 } width 2.00093
 class    274 {  -1.00104, 0.999926 } width 2.00096
 class     37 {  -1.00105, 0.99992 } width 2.00097
 0 classes had vanishing intervals, with lower > upper.
 Projection 1 weights[ 1123493] 
 {l,u}:   0 {  -1.00099, 0.999581} {  -1.00134, 0.999381} {  -1.00125, 0.999444} {  -1.00104, 0.999352} {  -1.00108, 0.999328} {  -1.00112, 0.999318} {  -1.00126, 0.999403} {  -1.00117, 0.999414}
 {l,u}:   8 {  -1.00103,  0.99932} {   -1.0012,  0.99936} {  -1.00113, 0.999363} {  -1.00102, 0.999516} {   -1.0011,  0.99927} {  -1.00119, 0.999582} {  -1.00126, 0.999533} {   -1.0012, 0.999563}
 {l,u}:  16 {  -1.00101, 0.999416} {   -1.0011, 0.999459} {  -1.00122, 0.999441} {  -1.00099, 0.999331} {  -1.00099, 0.999413} {  -1.00089, 0.999557} {  -1.00101, 0.999682} {  -1.00083, 0.999597}
 {l,u}:  24 {  -1.00102, 0.999609} {  -1.00085, 0.999585} {  -1.00114, 0.999709} {  -1.00095, 0.999544} {  -1.00111,  0.99957} {  -1.00106, 0.999557} {   -1.0012, 0.999465} {  -1.00106, 0.999509}
 {l,u}:  32 {  -1.00123, 0.999509} {    -1.001, 0.999514} {  -1.00116, 0.999475} {   -1.0011, 0.999303} {  -1.00127, 0.999707} {  -1.00127,  0.99986} {  -1.00118, 0.999537} {  -1.00111,  0.99934}
 {l,u}:  40 {  -1.00105, 0.999501} {  -1.00108, 0.999516} {  -1.00124,  0.99952} {  -1.00105, 0.999313} {  -1.00098, 0.999406} {  -1.00115, 0.999658} {  -1.00106, 0.999755} {  -1.00095,  0.99966}
 {l,u}:  48 {  -1.00111, 0.999543} {  -1.00097, 0.999632} {  -1.00112, 0.999641} {  -1.00125, 0.999658} {  -1.00113, 0.999636} {  -1.00116, 0.999736} {  -1.00087, 0.999652} {  -1.00125, 0.999422}
 {l,u}:  56 {  -1.00095, 0.999557} {   -1.0008, 0.999583} {  -1.00125, 0.999207} {  -1.00102, 0.999395} {  -1.00106, 0.999485} {  -1.00102, 0.999402} {  -1.00104, 0.999299} {  -1.00114, 0.999668} ...
 Some narrow non-zero intervals were:
 class    298 {  -1.00074, 0.999426 } width 2.00017
 class     98 {  -1.00098, 0.999189 } width 2.00017
 class    114 {  -1.00099, 0.999208 } width 2.0002
 class    406 {  -1.00082, 0.9994 } width 2.00022
 class    230 {  -1.00097, 0.999254 } width 2.00022
 class    214 {  -1.00094, 0.999296 } width 2.00024
 class    441 {  -1.00083, 0.999423 } width 2.00025
 class    143 {  -1.00101, 0.999239 } width 2.00025
 class    429 {  -1.00073, 0.99953 } width 2.00026
 class    356 {  -1.00092, 0.999345 } width 2.00026
 Some wide non-zero intervals were:
 class    321 {  -1.00116, 0.99993 } width 2.00109
 class    274 {  -1.00129, 0.999823 } width 2.00111
 class    232 {  -1.00191, 0.999228 } width 2.00113
 class     37 {  -1.00127, 0.99986 } width 2.00113
 0 classes had vanishing intervals, with lower > upper.
 Projection 2 weights[ 1123493] 
 {l,u}:   0 { -0.999926,  1.00019} { -0.999913,  1.00072} { -0.999935,  1.00062} { -0.999916,  1.00017} { -0.999914,  1.00029} { -0.999949,  1.00013} {  -0.99996,  1.00035} { -0.999909,   1.0003}
 {l,u}:   8 { -0.999879,  1.00035} { -0.999922,  1.00016} { -0.999847,  1.00046} { -0.999948,  1.00031} { -0.999956,  1.00013} { -0.999814,  1.00064} { -0.999854,  1.00063} { -0.999865,  1.00042}
 {l,u}:  16 { -0.999624,  1.00054} { -0.999874,  1.00043} { -0.999941,  1.00018} {  -0.99995,  1.00011} {  -0.99993,  1.00025} { -0.999979,  1.00017} {  -0.99998,  1.00037} { -0.999965,  1.00015}
 {l,u}:  24 { -0.999984,  1.00023} { -0.999978,  1.00011} { -0.999999,  1.00027} { -0.999917,  1.00025} { -0.999972,  1.00025} {  -0.99997,  1.00023} { -0.999941,  1.00027} { -0.999894,  1.00025}
 {l,u}:  32 { -0.999933,  1.00028} { -0.999966,  1.00022} { -0.999936,  1.00034} { -0.999868,  1.00042} { -0.999958,  1.00048} { -0.999945,  1.00078} {  -0.99989,  1.00036} { -0.999943,  1.00028}
 {l,u}:  40 { -0.999934,  1.00029} { -0.999928,  1.00024} { -0.999915,  1.00026} { -0.999921,  1.00021} { -0.999882,  1.00038} { -0.999977,  1.00027} {        -1,  1.00042} { -0.999942,  1.00037}
 {l,u}:  48 { -0.999896,  1.00036} { -0.999901,  1.00045} { -0.999971,  1.00038} { -0.999955,  1.00032} {  -0.99997,  1.00032} { -0.999982,  1.00053} { -0.999958,   1.0004} { -0.999936,  1.00033}
 {l,u}:  56 { -0.999949,  1.00048} { -0.999924,  1.00022} { -0.999969,  1.00017} {  -0.99992,  1.00025} { -0.999897,  1.00052} { -0.999855,  1.00058} { -0.999943,  1.00022} { -0.999978,  1.00023} ...
 Some narrow non-zero intervals were:
 class    178 { -0.999939, 1.0001 } width 2.00003
 class    130 { -0.999966, 1.00009 } width 2.00006
 class     19 {  -0.99995, 1.00011 } width 2.00006
 class     85 { -0.999975, 1.00008 } width 2.00006
 class    378 {  -0.99995, 1.00011 } width 2.00006
 class    148 { -0.999968, 1.00009 } width 2.00006
 class    351 { -0.999962, 1.0001 } width 2.00006
 class     87 { -0.999916, 1.00015 } width 2.00006
 class    294 { -0.999911, 1.00016 } width 2.00007
 class    204 { -0.999967, 1.0001 } width 2.00007
 Some wide non-zero intervals were:
 class    412 { -0.999901, 1.00087 } width 2.00077
 class    393 { -0.999912, 1.00087 } width 2.00078
 class    392 { -0.999907, 1.00088 } width 2.00079
 class     99 {  -1.36882, 1.00008 } width 2.36889
 0 classes had vanishing intervals, with lower > upper.
 Projection 3 weights[ 1123493] 
 {l,u}:   0 { -0.999587,  1.00097} { -0.999433,  1.00125} { -0.999497,  1.00116} { -0.999374,    1.001} { -0.999349,  1.00103} { -0.999339,  1.00109} { -0.999447,  1.00121} { -0.999441,  1.00109}
 {l,u}:   8 { -0.999397,  1.00098} { -0.999394,  1.00115} { -0.999436,  1.00108} { -0.999549,  1.00094} { -0.999292,  1.00106} { -0.999623,   1.0011} { -0.999592,  1.00119} { -0.999598,  1.00114}
 {l,u}:  16 { -0.999472,  1.00093} { -0.999505,  1.00103} { -0.999466,  1.00116} { -0.999344,  1.00096} { -0.999431,  1.00094} { -0.999561,  1.00088} { -0.999674,  1.00098} { -0.999592,  1.00083}
 {l,u}:  24 { -0.999593,  1.00101} { -0.999579,  1.00084} { -0.999695,  1.00111} { -0.999553,  1.00093} { -0.999564,  1.00108} { -0.999555,  1.00104} { -0.999468,  1.00117} { -0.999516,  1.00102}
 {l,u}:  32 { -0.999513,   1.0012} { -0.999522,  1.00098} { -0.999496,  1.00113} { -0.999359,  1.00103} { -0.999697,  1.00123} { -0.999895,   1.0012} { -0.999537,  1.00115} { -0.999353,  1.00109}
 {l,u}:  40 { -0.999526,  1.00104} { -0.999518,  1.00104} { -0.999541,  1.00119} { -0.999331,  1.00102} { -0.999448,  1.00094} { -0.999656,   1.0011} { -0.999753,  1.00102} { -0.999655,  1.00095}
 {l,u}:  48 { -0.999556,  1.00104} { -0.999637,  1.00095} { -0.999639,  1.00111} {  -0.99966,   1.0012} { -0.999636,   1.0011} { -0.999733,  1.00114} { -0.999675,  1.00087} { -0.999455,  1.00119}
 {l,u}:  56 { -0.999583,   1.0009} { -0.999583,  1.00078} { -0.999223,  1.00183} {  -0.99941,  1.00098} { -0.999539,    1.001} { -0.999457,  1.00097} { -0.999323,  1.00098} {  -0.99965,   1.0011} ...
 Some narrow non-zero intervals were:
 class    298 { -0.999441, 1.00073 } width 2.00017
 class     98 { -0.999217, 1.00096 } width 2.00018
 class    406 {  -0.99944, 1.00076 } width 2.0002
 class    441 { -0.999481, 1.00075 } width 2.00023
 class    114 { -0.999293, 1.00094 } width 2.00024
 class    214 { -0.999346, 1.00089 } width 2.00024
 class    143 { -0.999287, 1.00096 } width 2.00024
 class    152 { -0.999261, 1.00099 } width 2.00025
 class    230 { -0.999323, 1.00093 } width 2.00025
 class    199 { -0.999279, 1.00098 } width 2.00025
 Some wide non-zero intervals were:
 class    314 { -0.999734, 1.0013 } width 2.00104
 class     58 { -0.999223, 1.00183 } width 2.00105
 class    274 { -0.999798, 1.00127 } width 2.00107
 class     37 { -0.999895, 1.0012 } width 2.0011
 0 classes had vanishing intervals, with lower > upper.
 Projection 4 weights[ 1123493] 
 {l,u}:   0 {  -0.99971,  1.00067} { -0.999789,  1.00125} {  -0.99976,  1.00102} { -0.999759,  1.00062} { -0.999649,  1.00069} { -0.999775,  1.00057} { -0.999778,  1.00068} { -0.999749,  1.00051}
 {l,u}:   8 { -0.999674,  1.00057} { -0.999829,  1.00059} { -0.999678,  1.00079} { -0.999765,  1.00051} { -0.999779,  1.00041} { -0.999613,  1.00105} { -0.999712,  1.00115} {  -0.99961,  1.00079}
 {l,u}:  16 { -0.999373,   1.0009} {  -0.99971,  1.00087} { -0.999788,  1.00057} { -0.999744,   1.0004} { -0.999765,  1.00053} { -0.999639,  1.00075} { -0.999757,  1.00086} { -0.999689,  1.00081}
 {l,u}:  24 { -0.999638,  1.00098} { -0.999676,  1.00068} { -0.999789,   1.0008} { -0.999598,  1.00078} { -0.999849,  1.00077} { -0.999675,  1.00095} { -0.999733,  1.00083} { -0.999626,  1.00079}
 {l,u}:  32 { -0.999749,  1.00077} { -0.999814,  1.00088} { -0.999721,  1.00081} {  -0.99973,  1.00102} { -0.999733,  1.00094} {  -0.99974,  1.00149} { -0.999678,  1.00091} { -0.999778,  1.00088}
 {l,u}:  40 { -0.999681,  1.00088} { -0.999707,  1.00073} { -0.999738,  1.00052} { -0.999657,  1.00073} { -0.999518,  1.00072} { -0.999736,  1.00085} { -0.999756,  1.00119} { -0.999666,  1.00093}
 {l,u}:  48 { -0.999604,  1.00083} { -0.999554,  1.00102} { -0.999757,  1.00096} { -0.999711,  1.00093} { -0.999789,  1.00079} { -0.999804,  1.00107} { -0.999619,  1.00088} { -0.999697,  1.00065}
 {l,u}:  56 { -0.999619,  1.00104} { -0.999715,  1.00069} {  -0.99979,   1.0006} { -0.999655,  1.00062} { -0.999774,  1.00081} {  -0.99968,    1.001} { -0.999691,  1.00084} { -0.999664,   1.0008} ...
 Some narrow non-zero intervals were:
 class     19 { -0.999744, 1.0004 } width 2.00015
 class    207 {  -0.99966, 1.0005 } width 2.00016
 class    172 {  -0.99974, 1.00043 } width 2.00017
 class    121 { -0.999699, 1.00048 } width 2.00018
 class     91 { -0.999747, 1.00043 } width 2.00018
 class    331 { -0.999731, 1.00045 } width 2.00018
 class    378 { -0.999748, 1.00044 } width 2.00018
 class    145 { -0.999624, 1.00056 } width 2.00019
 class     12 { -0.999779, 1.00041 } width 2.00019
 class    242 { -0.999533, 1.00066 } width 2.00019
 Some wide non-zero intervals were:
 class    393 { -0.999882, 1.0014 } width 2.00128
 class    415 { -0.999776, 1.00151 } width 2.00129
 class    394 {  -0.99981, 1.00149 } width 2.0013
 class    392 { -0.999853, 1.00151 } width 2.00136
 0 classes had vanishing intervals, with lower > upper.

 Solved xxx.bin and yyy.txt for slc demo	
 Used s=libmclua.mcsolve.new("--xfile=../../data/temp.test.svm --output=big.soln -B -S --C1=-1 --maxiter=1000000 --optlu=5000 --treport=100000 --proj=5")	
 And then s:read():solve():save():display()	
 mcdumpsoln < big.soln    to dump .soln file as text	
